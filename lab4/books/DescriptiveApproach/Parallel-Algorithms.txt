Lecture Notes Series on Computing - Vol. 16




      Parallel
     Algorithms
                 LECTURE NOTES SERIES ON COMPUTING
                 Editor-in-Chief: D T Lee (Academia Sinica, Taiwan)



                 Published

                 Vol. 16:         Parallel Algorithms
                                  M H Alsuwaiyel
                 Vol. 15:         Algorithms: Design Techniques and Analysis (Second Edition)
                                  M H Alsuwaiyel
                 Vol. 14:         Algorithms: Design Techniques and Analysis (Revised Edition)
                                  M H Alsuwaiyel
                 Vol. 13:         Computational Aspects of Algebraic Curves
                                  Ed. T Shaska
                 Vol. 12:         Planar Graph Drawing
                                  T Nishizeki & S Rahman
                 Vol. 11:         Geometric Computation
                                  Eds. F Chen & D Wang
                 Vol. 10:         Computer Mathematics
                                  Proceedings of the Sixth Asian Symposium (ASCM 2003)
                                  Eds. Z Li & W Sit
                 Vol. 9:          Computer Mathematics
                                  Proceedings of the Fifth Asian Symposium (ASCM 2001)
                                  Eds. K Yokoyama & K Shirayanagi
                 Vol. 8:          Computer Mathematics
                                  Proceedings of the Fourth Asian Symposium (ASCM 2000)
                                  Eds. X-S Gao & D Wang
                 Vol. 7:          Algorithms: Design Techniques and Analysis
                                  Ed. M H Alsuwaiyel
                 Vol. 6:          VLSI Physical Design Automation: Theory and Practice
                                  S M Sait & H Youssef
                 Vol. 5:          Proceedings of the Conference on
                                  Parallel Symbolic Computation — PASCO ’94
                                  Ed. H Hong
                 Vol. 4:          Computing in Euclidean Geometry (Second Edition)
                                  Eds. D-Z Du & F Hwang
                 Vol. 3:          String Searching Algorithms
                                  G A Stephen
                 Vol. 2:          Algorithmic Aspects of VLSI Layout
                                  Eds. D T Lee & M Sarrafzadeh
                 Vol. 1:          Computing in Euclidean Geometry
                                  Eds. D-Z Du & F Hwang




Balasubramanian - 12744 - Parallel Algorithms.indd 1                                             4/5/2022 9:54:08 am
                   Lecture Notes Series on Computing - Vol. 16




                                Parallel
                               Algorithms

                                  M. H. Alsuwaiyel
                 King Fahd University of Petroleum & Minerals (KFUPM), Saudi Arabia




                                               World Scientific
NEW JERSEY   •   LONDON   •   SINGAPORE   •   BEIJING   •   SHANGHAI   •   HONG KONG   •   TA I P E I   •   CHENNAI   •   TOKYO
                 Published by
                 World Scientific Publishing Co. Pte. Ltd.
                 5 Toh Tuck Link, Singapore 596224
                 USA office: 27 Warren Street, Suite 401-402, Hackensack, NJ 07601
                 UK office: 57 Shelton Street, Covent Garden, London WC2H 9HE




                 Library of Congress Cataloging-in-Publication Data
                 Names: Alsuwaiyel, M. H., author.
                 Title: Parallel algorithms / M. H. Alsuwaiyel, King Fahd University of
                     Petroleum & Minerals (KFUPM), Saudi Arabia.
                 Description: [Hackensack] New Jersey : World Scientific, [2022] |
                     Series: Lecture notes series on computing, 1793-1223 ; vol. 16 |
                     Includes bibliographical references and index.
                 Identifiers: LCCN 2022008953 | ISBN 9789811252976 (hardcover) |
                     ISBN 9789811252983 (ebook for institutions) | ISBN 9789811252990 (ebook for individuals)
                 Subjects: LCSH: Parallel algorithms. | Parallel processing (Electronic computers)
                 Classification: LCC QA76.642 .A47 2022 | DDC 005.2/75--dc23/eng/20220429
                 LC record available at https://lccn.loc.gov/2022008953



                 British Library Cataloguing-in-Publication Data
                 A catalogue record for this book is available from the British Library.




                 Copyright © 2022 by World Scientific Publishing Co. Pte. Ltd.
                 All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means,
                 electronic or mechanical, including photocopying, recording or any information storage and retrieval
                 system now known or to be invented, without written permission from the publisher.




                 For photocopying of material in this volume, please pay a copying fee through the Copyright Clearance
                 Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to photocopy
                 is not required from the publisher.




                 For any available supplementary material, please visit
                 https://www.worldscientific.com/worldscibooks/10.1142/12744#t=suppl


                 Desk Editors: Balasubramanian Shanmugam/Amanda Yun

                 Typeset by Stallion Press
                 Email: enquiries@stallionpress.com

                 Printed in Singapore




Balasubramanian - 12744 - Parallel Algorithms.indd 2                                                               4/5/2022 9:54:08 am
May 7, 2022   11:44   Parallel Algorithms       9in x 6in   b4591-fm   page v




                      To my wife Noura and my daughter Sara




                                            v
                             B1948   Governing Asia




                      This page intentionally left blank




B1948_1-Aoki.indd 6                                        9/22/2014 4:24:57 PM
May 7, 2022   11:44     Parallel Algorithms       9in x 6in    b4591-fm                   page vii




                                              Preface



        In the last few decades, there has been an explosion of interest in the ﬁeld
        of parallel computation. From the computer scientist’s point of view, this
        has provided a challenging range of problems with new ground rules for the
        design and analysis of parallel algorithms.
            This text is meant to be an introduction to the ﬁeld of parallel algo-
        rithms and to techniques for eﬃcient parallelization. The emphasis is upon
        designing algorithms within the timeless and abstract context of a high-
        level programming language, rather than depending upon highly detailed
        machine architectures.
            Although the main theme of the book is algorithm design using diﬀerent
        models of computation, it also emphasizes the other major component in
        algorithmic design: the analysis of parallel algorithms. It covers the analysis
        of most of the algorithms presented in detail. The focus of the presentation
        is on practical applications of algorithm design using diﬀerent models of
        parallel computation. Each model is illustrated by providing an adequate
        number of algorithms to solve some problems that quite often arise in many
        applications in science and engineering.
            The style of presentation of algorithms is straightforward, and uses
        pseudocode that is similar to the syntax of structured programming
        languages, e.g., if-then-else, for and while constructs. The pseudocode is
        sometimes intermixed with English whenever necessary. Describing a por-
        tion of an algorithm in English is indeed instructive; it conveys the idea
        with minimum eﬀort on the part of the reader. However, sometimes it is
        both easier and more formal to use a pseudocode statement.


                                                vii
May 7, 2022    11:44    Parallel Algorithms           9in x 6in     b4591-fm               page viii




        viii                                  Parallel Algorithms

            The book is largely self-contained, presuming no special knowledge of
        parallel computers or particular mathematics. However, the reader familiar
        with elementary ideas from the areas of discrete mathematics, data struc-
        tures and sequential algorithms will be at an advantage. Most chapters
        include examples and illustrations. In addition, the solutions to all exer-
        cises are included at the end of each chapter.
            The book is intended as a text in the ﬁeld of the design and analysis
        of parallel algorithms. It includes adequate material for a course in parallel
        algorithms in the undergraduate or graduate levels.
            The author would like to thank those who have critically oﬀered sugges-
        tions, including the students of the parallel algorithms course at KFUPM.
        Special thanks go to Wasﬁ Al-Khatib and Sultan Almuhammadi for their
        valuable discussions and comments.
                                                                       M. H. Alsuwaiyel
                                                                    Khobar, Saudi Arabia
May 7, 2022   11:44    Parallel Algorithms        9in x 6in   b4591-fm              page ix




                           About the Author


        Muhammad Hamad (M. H.) Alsuwaiyel is a retired Professor of the
        Information and Computer Science Department at King Fahd University
        of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia. He was a
        member of KFUPM’s faculty from 1991 to 2014. Dr Alsuwaiyel taught both
        undergraduate and graduate courses on design and analysis of algorithms,
        computer networks, discrete structures, theory of computing, automata and
        formal languages, foundations of computer science, mathematical logic,
        design and theory of algorithms, combinatorial algorithms optimization,
        parallel algorithms, theory of automata and formal languages, computa-
        tional complexity, and computational geometry. He has publications in the
        design and analysis of algorithms. He holds a B.S. in Systems Engineering
        from KFUPM, an M.S. in Computer Science from University of Colorado
        at Boulder, USA and a Ph.D. in Computer Science from Northwestern
        University, Illinois, USA. M. H. Alsuwaiyel is the author of Algorithms:
        Design Techniques and Analysis.




                                             ix
                             B1948   Governing Asia




                      This page intentionally left blank




B1948_1-Aoki.indd 6                                        9/22/2014 4:24:57 PM
May 7, 2022   11:44         Parallel Algorithms        9in x 6in       b4591-fm                                      page xi




                                             Contents



        Preface                                                                                                vii
        About the Author                                                                                       ix

        1. Introduction                                                                                         1
              1.1     Classiﬁcations of Parallel Architectures     .   .   .   .   .   .   .   .   .   .   .    1
              1.2     Shared-Memory Computers . . . . . .          .   .   .   .   .   .   .   .   .   .   .    2
              1.3     Interconnection-Network Computers .          .   .   .   .   .   .   .   .   .   .   .    3
              1.4     Two Simple Examples . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   .    5
        2. Shared-memory Computers (PRAM)                                                                       7
              2.1     Introduction . . . . . . . . . . . . . . . . . . . . . . .                   .   .   .    7
              2.2     The Balanced Tree Method . . . . . . . . . . . . . .                         .   .   .    8
              2.3     Brent Theorem . . . . . . . . . . . . . . . . . . . . .                      .   .   .   10
              2.4     Sorting in Θ(1) Time on the CRCW PRAM Model .                                .   .   .   11
                      2.4.1 Implementation on the CREW PRAM model                                  .   .   .   12
                      2.4.2 Implementation on the EREW PRAM model                                  .   .   .   13
              2.5     Parallel Preﬁx . . . . . . . . . . . . . . . . . . . . . .                   .   .   .   14
                      2.5.1 Array packing . . . . . . . . . . . . . . . . .                        .   .   .   16
                      2.5.2 Parallel quicksort . . . . . . . . . . . . . . . .                     .   .   .   18
              2.6     Parallel Search . . . . . . . . . . . . . . . . . . . . .                    .   .   .   18
              2.7     Pointer Jumping . . . . . . . . . . . . . . . . . . . .                      .   .   .   21
              2.8     Euler Tour . . . . . . . . . . . . . . . . . . . . . . . .                   .   .   .   22
                      2.8.1 Directing a tree . . . . . . . . . . . . . . . . .                     .   .   .   25
                      2.8.2 Computing vertex levels in a tree . . . . . . .                        .   .   .   26

                                                  xi
May 7, 2022   11:44         Parallel Algorithms            9in x 6in    b4591-fm                                 page xii




        xii                                       Parallel Algorithms

              2.9     Merging by Ranking . . . . . . . . . . . . . .          .    .   .   .   .   .   .   27
                      2.9.1 Computing ranks . . . . . . . . . . . .           .    .   .   .   .   .   .   27
                      2.9.2 Merging . . . . . . . . . . . . . . . . .         .    .   .   .   .   .   .   30
                      2.9.3 Parallel bottom-up merge sorting . . .            .    .   .   .   .   .   .   31
              2.10    The Zero-one Principle . . . . . . . . . . . . .        .    .   .   .   .   .   .   32
              2.11    Odd–Even Merging . . . . . . . . . . . . . . .          .    .   .   .   .   .   .   33
              2.12    Bitonic Merging and Sorting . . . . . . . . . .         .    .   .   .   .   .   .   35
                      2.12.1 Bitonic sorting . . . . . . . . . . . . .        .    .   .   .   .   .   .   40
              2.13    Pipelined Mergesort . . . . . . . . . . . . . . .       .    .   .   .   .   .   .   43
                      2.13.1 The algorithm . . . . . . . . . . . . .          .    .   .   .   .   .   .   44
                      2.13.2 Computing and maintaining ranks . .              .    .   .   .   .   .   .   46
                      2.13.3 Analysis of the algorithm . . . . . . .          .    .   .   .   .   .   .   49
              2.14    Selection . . . . . . . . . . . . . . . . . . . . .     .    .   .   .   .   .   .   50
              2.15    Multiselection . . . . . . . . . . . . . . . . . .      .    .   .   .   .   .   .   52
              2.16    Matrix Multiplication . . . . . . . . . . . . . .       .    .   .   .   .   .   .   56
              2.17    Transitive Closure . . . . . . . . . . . . . . . .      .    .   .   .   .   .   .   58
              2.18    Shortest Paths . . . . . . . . . . . . . . . . . .      .    .   .   .   .   .   .   58
              2.19    Minimum Spanning Trees . . . . . . . . . . .            .    .   .   .   .   .   .   59
              2.20    Computing the Convex Hull of a Set of Points            .    .   .   .   .   .   .   63
              2.21    Bibliographic Notes . . . . . . . . . . . . . . .       .    .   .   .   .   .   .   68
              2.22    Exercises . . . . . . . . . . . . . . . . . . . . .     .    .   .   .   .   .   .   69
              2.23    Solutions . . . . . . . . . . . . . . . . . . . . .     .    .   .   .   .   .   .   77
        3. The Hypercube                                                                                   95
              3.1     Introduction . . . . . . . . . . . . . . . . . . . . .           .   .   .   .   . 95
              3.2     The Butterﬂy . . . . . . . . . . . . . . . . . . . .             .   .   .   .   . 96
              3.3     Embeddings of the Hypercube . . . . . . . . . . .                .   .   .   .   . 99
                      3.3.1 Gray codes . . . . . . . . . . . . . . . . .               .   .   .   .   . 100
                      3.3.2 Embedding of a linear array into the
                             hypercube . . . . . . . . . . . . . . . . . .             . . . . . 101
                      3.3.3 Embedding of a mesh into the hypercube                     . . . . . 102
                      3.3.4 Embedding of a binary tree into the
                             hypercube . . . . . . . . . . . . . . . . . .             .   .   .   .   .   103
              3.4     Broadcasting in the Hypercube . . . . . . . . . .                .   .   .   .   .   104
              3.5     Semigroup Operations . . . . . . . . . . . . . . .               .   .   .   .   .   105
              3.6     Permutation Routing on the Hypercube . . . . .                   .   .   .   .   .   105
                      3.6.1 The greedy algorithm . . . . . . . . . . .                 .   .   .   .   .   106
                      3.6.2 The randomized algorithm . . . . . . . .                   .   .   .   .   .   107
May 7, 2022   11:44         Parallel Algorithms         9in x 6in   b4591-fm                                  page xiii




                                                  Contents                                             xiii

              3.7     Permutation Routing on the Butterﬂy . . . . . .              .   .   .   .   .   110
              3.8     Computing Parallel Preﬁx on the Hypercube . . .              .   .   .   .   .   112
              3.9     Hyperquicksort . . . . . . . . . . . . . . . . . . .         .   .   .   .   .   113
              3.10    Sample Sort . . . . . . . . . . . . . . . . . . . . .        .   .   .   .   .   115
              3.11    Selection on the Hypercube . . . . . . . . . . . .           .   .   .   .   .   118
              3.12    Multiselection on the Hypercube . . . . . . . . .            .   .   .   .   .   119
              3.13    Load Balancing on the Hypercube . . . . . . . . .            .   .   .   .   .   122
              3.14    Computing Parallel Preﬁx on the Butterﬂy . . . .             .   .   .   .   .   126
              3.15    Odd–Even Merging and Sorting on the Butterﬂy                 .   .   .   .   .   127
              3.16    Matrix Multiplication on the Hypercube . . . . .             .   .   .   .   .   132
              3.17    Bibliographic Notes . . . . . . . . . . . . . . . . .        .   .   .   .   .   138
              3.18    Exercises . . . . . . . . . . . . . . . . . . . . . . .      .   .   .   .   .   138
              3.19    Solutions . . . . . . . . . . . . . . . . . . . . . . .      .   .   .   .   .   144
        4. The Linear Array and the Mesh                                                               159
              4.1     Introduction . . . . . . . . . . . . . . . . . . . .     .   .   .   .   .   .   159
              4.2     Embedding between a Mesh and a Linear Array              .   .   .   .   .   .   161
              4.3     Broadcasting in the Linear Array and the Mesh            .   .   .   .   .   .   162
              4.4     Computing Parallel Preﬁx on the Mesh . . . . .           .   .   .   .   .   .   163
              4.5     Odd–Even Transposition Sort . . . . . . . . . .          .   .   .   .   .   .   164
              4.6     Shearsort . . . . . . . . . . . . . . . . . . . . . .    .   .   .   .   .   .   165
                                   √
              4.7     A Simple Θ( n) Time Algorithm for Sorting
                      on the Mesh . . . . . . . . . . . . . . . . . . . .      .   .   .   .   .   .   167
              4.8     Odd–Even Merging and Sorting on the Mesh . .             .   .   .   .   .   .   169
              4.9     Routing on the Linear Array and the Mesh . . .           .   .   .   .   .   .   172
                      4.9.1 Routing in the linear array . . . . . . .          .   .   .   .   .   .   172
                      4.9.2 Deterministic routing in the mesh . . .            .   .   .   .   .   .   173
                      4.9.3 Randomized routing on the mesh . . . .             .   .   .   .   .   .   174
              4.10    Matrix Multiplication on the Mesh . . . . . . .          .   .   .   .   .   .   177
                      4.10.1 The ﬁrst algorithm . . . . . . . . . . . .        .   .   .   .   .   .   177
                      4.10.2 The second algorithm . . . . . . . . . .          .   .   .   .   .   .   178
              4.11    Computing the Transitive Closure on the Mesh             .   .   .   .   .   .   180
              4.12    Connected Components . . . . . . . . . . . . .           .   .   .   .   .   .   184
              4.13    Shortest Paths . . . . . . . . . . . . . . . . . . .     .   .   .   .   .   .   185
              4.14    Computing the Convex Hull of a Set of Points
                      on the Mesh . . . . . . . . . . . . . . . . . . . .      . . . . . . 185
                      4.14.1 The ﬁrst algorithm . . . . . . . . . . . .        . . . . . . 186
                      4.14.2 The second algorithm . . . . . . . . . .          . . . . . . 187
May 7, 2022   11:44         Parallel Algorithms           9in x 6in     b4591-fm                                      page xiv




        xiv                                       Parallel Algorithms

              4.15 Labeling Connected Components . . . .                .   .   .   .   .   .   .   .   .   .   191
                   4.15.1 The propagation algorithm . . .               .   .   .   .   .   .   .   .   .   .   191
                   4.15.2 The recursive algorithm . . . . .             .   .   .   .   .   .   .   .   .   .   192
              4.16 Columnsort . . . . . . . . . . . . . . . .           .   .   .   .   .   .   .   .   .   .   196
              4.17 3-dimensional Mesh . . . . . . . . . . . .           .   .   .   .   .   .   .   .   .   .   202
                   4.17.1 Sorting on 3-dimensional meshes               .   .   .   .   .   .   .   .   .   .   202
              4.18 Bibliographic Notes . . . . . . . . . . . .          .   .   .   .   .   .   .   .   .   .   206
              4.19 Exercises . . . . . . . . . . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   207
              4.20 Solutions . . . . . . . . . . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   212
        5. Fast Fourier Transform                                                                               227
              5.1     Introduction . . . . . . . . . . . . . . . . . . . .              .   .   .   .   .   .   227
              5.2     Implementation on the Butterﬂy . . . . . . . .                    .   .   .   .   .   .   231
              5.3     Iterative FFT on the Butterﬂy . . . . . . . . . .                 .   .   .   .   .   .   231
              5.4     The Inverse Fourier Transform . . . . . . . . . .                 .   .   .   .   .   .   234
              5.5     Product of Polynomials . . . . . . . . . . . . . .                .   .   .   .   .   .   235
              5.6     Computing the Convolution of Two Vectors . .                      .   .   .   .   .   .   238
              5.7     The Product of a Toeplitz Matrix and a Vectors                    .   .   .   .   .   .   239
              5.8     Using Modular Arithmetic . . . . . . . . . . . .                  .   .   .   .   .   .   241
              5.9     Bibliographic Notes . . . . . . . . . . . . . . . .               .   .   .   .   .   .   244
              5.10    Exercises . . . . . . . . . . . . . . . . . . . . . .             .   .   .   .   .   .   244
              5.11    Solutions . . . . . . . . . . . . . . . . . . . . . .             .   .   .   .   .   .   246
        6. Tree-based Networks                                                                                  253
              6.1     The Tree Network . . . . . . . . . . . . . . . . . .                  .   .   .   .   .   253
                      6.1.1 Semigroup operations . . . . . . . . . . .                      .   .   .   .   .   254
                      6.1.2 Sorting by minimum extraction . . . . . .                       .   .   .   .   .   254
                      6.1.3 Sorting by partitioning . . . . . . . . . . .                   .   .   .   .   .   255
                      6.1.4 Selection . . . . . . . . . . . . . . . . . .                   .   .   .   .   .   256
                      6.1.5 The one-dimensional pyramid . . . . . . .                       .   .   .   .   .   259
              6.2     The Pyramid . . . . . . . . . . . . . . . . . . . .                   .   .   .   .   .   260
                      6.2.1 Computing parallel preﬁx on the pyramid                         .   .   .   .   .   262
              6.3     Mesh of Trees . . . . . . . . . . . . . . . . . . . .                 .   .   .   .   .   264
                      6.3.1 Sorting on the mesh of trees . . . . . . . .                    .   .   .   .   .   266
                      6.3.2 Routing in the mesh of trees . . . . . . .                      .   .   .   .   .   269
              6.4     Computing Parallel Preﬁx on the Mesh of Trees .                       .   .   .   .   .   270
              6.5     Comparison Between the Mesh of Trees
                      and the Pyramid . . . . . . . . . . . . . . . . . .                   . . . . . 272
May 7, 2022   11:44         Parallel Algorithms        9in x 6in    b4591-fm                            page xv




                                                  Contents                                         xv

              6.6     Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . 273
              6.7     Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
              6.8     Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
        7. The Star Network                                                                       281
              7.1     Introduction . . . . . . . . . . . . . . . . .    . . . . . .   .   .   .   281
              7.2     Ranking of the Processors . . . . . . . . .       . . . . . .   .   .   .   283
              7.3     Routing between Substars . . . . . . . . .        . . . . . .   .   .   .   285
              7.4     Computing Parallel Preﬁx on the Star . .          . . . . . .   .   .   .   287
              7.5     Computing the Maximum . . . . . . . . .           . . . . . .   .   .   .   290
              7.6     Neighborhood Broadcasting and Recursive           Doubling      .   .   .   292
              7.7     Broadcasting in the Star . . . . . . . . . .      . . . . . .   .   .   .   294
              7.8     The Arrangement Graph . . . . . . . . . .         . . . . . .   .   .   .   296
              7.9     The (d, k)-Star Graph . . . . . . . . . . .       . . . . . .   .   .   .   297
              7.10    Sorting in the Sd,k Star . . . . . . . . . . .    . . . . . .   .   .   .   300
              7.11    Bibliographic Notes . . . . . . . . . . . . .     . . . . . .   .   .   .   303
              7.12    Exercises . . . . . . . . . . . . . . . . . . .   . . . . . .   .   .   .   303
              7.13    Solutions . . . . . . . . . . . . . . . . . . .   . . . . . .   .   .   .   305
        8. Optical Transpose Interconnection System (OTIS)                                        313
              8.1     Introduction . . . . . . . . . . . . . . . . . . . . . .    .   .   .   .   313
              8.2     The OTIS-Mesh . . . . . . . . . . . . . . . . . . . .       .   .   .   .   314
                      8.2.1 Data movements in the OTIS-Mesh . . . .               .   .   .   .   315
                      8.2.2 Broadcasting in the OTIS-Mesh . . . . . .             .   .   .   .   316
                      8.2.3 Semigroup operations on the OTIS-Mesh .               .   .   .   .   316
                      8.2.4 Parallel preﬁx in OTIS-Mesh . . . . . . . .           .   .   .   .   318
                      8.2.5 Shift operations on the OTIS-Mesh . . . . .           .   .   .   .   320
                      8.2.6 Permutation routing in OTIS-Mesh . . . . .            .   .   .   .   322
                      8.2.7 Sorting on OTIS-Mesh . . . . . . . . . . . .          .   .   .   .   324
              8.3     The OTIS-Hypercube . . . . . . . . . . . . . . . . .        .   .   .   .   324
                      8.3.1 Simulation of an n2 -processor hypercube . .          .   .   .   .   324
                      8.3.2 Broadcasting in the OTIS-Hypercube . . .              .   .   .   .   326
                      8.3.3 Semigroup operations on the
                             OTIS-Hypercube . . . . . . . . . . . . . . .         .   .   .   .   326
                      8.3.4 Sorting and routing in the OTIS-Hypercube             .   .   .   .   327
              8.4     Other OTIS Networks . . . . . . . . . . . . . . . .         .   .   .   .   328
                      8.4.1 The OTIS-Star . . . . . . . . . . . . . . . .         .   .   .   .   328
                      8.4.2 The OTIS-MOT . . . . . . . . . . . . . . .            .   .   .   .   328
May 7, 2022   11:44         Parallel Algorithms           9in x 6in          b4591-fm                                    page xvi




        xvi                                       Parallel Algorithms

              8.5     Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . 330
              8.6     Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
              8.7     Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
        9. Systolic Computation                                                                                    341
              9.1     Introduction . . . . . . . . . .         . . .   . . . . . .     .   .   .   .   .   .   .   341
              9.2     Matrix-vector Multiplication .           . . .   . . . . . .     .   .   .   .   .   .   .   342
              9.3     Computing the Convolution of             Two     Sequences       .   .   .   .   .   .   .   342
                      9.3.1 Semisystolic solution .            . . .   . . . . . .     .   .   .   .   .   .   .   343
                      9.3.2 Pure systolic solution             . . .   . . . . . .     .   .   .   .   .   .   .   344
              9.4     A Zero-time VLSI Sorter . . .            . . .   . . . . . .     .   .   .   .   .   .   .   345
              9.5     An On-chip Bubble Sorter . .             . . .   . . . . . .     .   .   .   .   .   .   .   347
              9.6     Bibliographic Notes . . . . . .          . . .   . . . . . .     .   .   .   .   .   .   .   351
              9.7     Exercises . . . . . . . . . . . .        . . .   . . . . . .     .   .   .   .   .   .   .   352
              9.8     Solutions . . . . . . . . . . . .        . . .   . . . . . .     .   .   .   .   .   .   .   353
        Appendix A Mathematical Preliminaries                                                                      357
              A.1     Asymptotic Notations . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   357
                      A.1.1 The O-notation . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   357
                      A.1.2 The Ω-notation . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   358
                      A.1.3 The Θ-notation . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   358
                      A.1.4 The o-notation . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   359
              A.2     Divide-and-conquer Recurrences . . . . . .               .   .   .   .   .   .   .   .   .   359
              A.3     Summations . . . . . . . . . . . . . . . . .             .   .   .   .   .   .   .   .   .   361
              A.4     Probability . . . . . . . . . . . . . . . . . .          .   .   .   .   .   .   .   .   .   362
                      A.4.1 Random variables and expectation                   .   .   .   .   .   .   .   .   .   362
                      A.4.2 Bernoulli distribution . . . . . . .               .   .   .   .   .   .   .   .   .   363
                      A.4.3 Binomial distribution . . . . . . .                .   .   .   .   .   .   .   .   .   364
                      A.4.4 Chernoﬀ bounds . . . . . . . . . .                 .   .   .   .   .   .   .   .   .   364

        Bibliography                                                                                               367
        Index                                                                                                      375
May 7, 2022   11:14     Parallel Algorithms           9in x 6in   b4591-ch01             page 1




                                              Chapter 1


                                   Introduction



        With the growing number of areas in which computers are being used,
        there is an ever-increasing demand for more computing power. A means
        to attain very high computational speeds is to use a parallel computer,
        meaning, a computer that possesses several processing units, or processors.
        In this case, the problem is broken down into smaller parts, which are solved
        simultaneously, each by a diﬀerent processor.
            A parallel algorithm, as opposed to a traditional sequential algorithm, is
        an algorithm which can do multiple operations in a given time. In sequential
        algorithms, an algorithm is described and analyzed using the random-access
        machine (RAM) as a model of computation. By contrast, in parallel algo-
        rithms, an algorithm is described and analyzed using diﬀerent models, one
        of which is the so-called parallel random-access machine (PRAM). The pur-
        pose of this chapter is to introduce parallel architectures and models, and
        illustrate parallel algorithms through simple examples.


        1.1     Classifications of Parallel Architectures

        There are four classiﬁcations of parallel architectures based upon the num-
        ber of concurrent instruction streams and data streams available in the
        architecture.
        (a) Single instruction stream, single data stream (SISD): Most conventional
        computers with one central processing unit (CPU) belong to this class.
        Examples of SISD architecture are the traditional uniprocessor machines


                                                  1
May 7, 2022   11:14     Parallel Algorithms        9in x 6in    b4591-ch01              page 2




        2                                 Parallel Algorithms

        like older personal computers and mainframe computers. By 2010, many
        personal computers had multiple cores.
        (b) Single instruction stream, multiple data streams (SIMD): This cate-
        gory includes machines with a single program and multiple CPUs. In this
        class, a parallel computer consists of p identical processors. All processors
        operate under the control of a single instruction stream issued by a central
        control unit. Processors communicate among themselves during computa-
        tion in order to exchange data or intermediate results in two ways, giving
        rise to two subclasses: SIMD computers where communication is eﬀected
        through a shared memory, and those where it is done via an interconnection
        network.
        (c) Multiple instruction streams, single data stream (MISD): This architec-
        ture is uncommon and unrealistic.
        (d) Multiple instruction streams, multiple data streams (MIMD): This class
        of computers is the most general and most powerful. In this class, there are
        p processors, p streams of instructions, and p streams of data. The machines
        that fall into this category are capable of executing several programs inde-
        pendently. They include multi-core superscalar processors, and distributed
        systems, using either one shared memory space or a distributed memory
        space. In MIMD, processors may have multiple processing cores that can
        execute diﬀerent instructions on diﬀerent data. Most parallel computers, as
        of 2013, are MIMD systems.


        1.2     Shared-Memory Computers

        This class is also known in the literature as the Parallel Random-Access
        Machine (PRAM) model. It assumes that there is a random-access shared
        memory, such that any processor can access any variable with unit cost.
        This assumption of unit-cost access (regardless of the size of the mem-
        ory) is unrealistic, but it makes the analysis of parallel algorithms easier.
        The programs written on these machines are, in general, of type SIMD.
        These kinds of algorithms are useful for understanding the exploitation of
        concurrency, for they divide the original problem into similar subproblems
        and then solve them in parallel. The introduction of the formal PRAM
        model had the aim of quantifying analysis of parallel algorithms in a way
        analogous to the RAM model. The structure of the PRAM is shown in
        Fig. 1.1. Here, multiple processors are attached to a single block of memory.
May 7, 2022   11:14         Parallel Algorithms          9in x 6in      b4591-ch01             page 3




                                                  Introduction                             3


                         Processor 1

                         Processor 2

                         Processor 3                 Memory             Shared
                                                     access             memory
                                                      unit




                         Processor n

                         Fig. 1.1.     Parallel random access machine (PRAM).



                      Memory 1          Memory 2       Memory 3              Memory n




                      Processor 1      Processor 2     Processor 3           Processor n




                                           Communication network

                                     Fig. 1.2.    Interconnection network.


        The processors can communicate among themselves through the shared
        memory only. A memory access unit connects the processors with the shared
        memory block.


        1.3     Interconnection-Network Computers

        Interconnection networks or distributed memory machines are constructed
        as processor-memory pairs and connected to each other in a well-deﬁned
        pattern. These processor-memory pairs are often referred to as processing
        elements or PEs, or sometimes just as processors. An interconnection net-
        work may be viewed as an undirected graph G = (V, E), where V is the
        set of nodes or processors, and E is the set of two-way links. Processors
        communicate between each other by sending messages. The structure of
        the interconnection network is shown in Fig. 1.2.
May 7, 2022   11:14     Parallel Algorithms        9in x 6in    b4591-ch01               page 4




        4                                 Parallel Algorithms

            The topology of a network refers to its general infrastructure — the
        pattern in which multiple processors are connected. This pattern could
        either be regular or irregular, though many multi-core architectures today
        use highly regular interconnection networks. On one extreme, there is the
        complete graph, which models an interconnection network where every pro-
        cessor is connected to every other processor. This kind of connection is pro-
        hibitive, as it is impractical. On the other extreme, the line graph, which
        models the linear array, connects each node to one or two other nodes. In
        between, there is a multitude of interconnection networks that have both
        advantages and disadvantages. For instance, there is the hypercube, the
        mesh, the tree and the pyramid, to mention a few.
            The degree of a network is the maximum degree of any vertex in the
        underlying graph. The degree of processor P corresponds to the number
        of processors directly connected to P . Naturally, networks of high degree
        become very diﬃcult to manufacture. Therefore, it is desirable to use net-
        works of low degree, especially if the network is to be scaled to an extremely
        large number of processors. In a network with n processors, a constant
        degree is preferable to one that is a function of n. For example, the degree
        of the mesh network is 4, while that of the hypercube is log n.
            The network diameter is deﬁned as the maximum shortest path dis-
        tance between any two processors. A low communication diameter is highly
        desirable, because it allows for eﬃcient communication between arbitrary
        processors. For instance, the diameter of the hypercube with n processors
        is log n, while the diameter of a mesh with the same number of processors
            √
        is 2 n − 2.
            The bisection width of an interconnection network is the minimum num-
        ber of links that have to be removed in order to disconnect the network
        into two approximately equal-sized subnetworks. In general, machines with
        a high bisection width are diﬃcult to build, but they provide users with the
        possibility of moving large amounts of data eﬃciently. The bisection width
        implies a lower bound on the computations in an interconnection network,
        especially in algorithms that require massive data movements. For instance,
        in the problem of sorting n elements, Ω(n) data items may have to be moved
        from one half of the network to the other. For example, the bisection width
        of the hypercube is Θ(n), and it admits sorting algorithms in the order of
        Θ(log2 n) and Θ(log n log log n), while the bisection width of the mesh is
            √                                                     √
        Θ( n), which explains why sorting on the mesh is Ω( n).
May 7, 2022   11:14     Parallel Algorithms          9in x 6in   b4591-ch01               page 5




                                              Introduction                           5

        1.4     Two Simple Examples

        Now, we present two simple examples of parallel algorithms, and deﬁne and
        illustrate some of the performance measures that are used in the analysis
        of parallel algorithms.

        Example 1.1 Consider the problem of adding n numbers s = a1 +
        a2 + · · · + an , where n = 2k for some nonnegative integer k. Sequentially,
        the expression can be computed by scanning the input from left to right
        in the obvious way using n − 1 additions. In parallel, b1 = a1 + a2 , b2 =
        a3 + a4 , . . . , bn/2 = an−1 + an are computed in one parallel step using n/2
        processors to produce a new expression b1 + b2 + · · · + bn/2 consisting of
        n/2 operands. Then c1 = b1 + b2 , c2 = b3 + b4 , . . . , cn/4 = bn/2−1 + bn/2
        are computed in one parallel step using n/4 processors to produce a new
        expression c1 + c2 + · · · + cn/4 consisting of n/4 operands. This process
        continues until there is only one value left. The total number of parallel
        steps is k = log n using n/2 processors.                                    

        Example 1.2 Recall the search problem: Given a set X = {x1 , x2 , . . . ,
        xn } of n unordered and distinct elements, and an element y, determine j
        such that y = xj if y ∈ X and j = 0 otherwise. n comparisons are needed
        in the worst case to solve this problem sequentially. In parallel, assume
        there are n processors P1 , P2 , . . . , Pn , and that xi is stored in Pi , 1 ≤
        i ≤ n. Initially, P1 sets j = 0. Then all processors Pi compare y with xi
        simultaneously. If y ∈ X, only one processor Pk will succeed in setting
        j = k. It follows that the problem can be solved in two parallel steps
        using n processors. Notice that concurrent read capability is required, as
        all processors need to read y at the same time.                               

            Unlike in sequential algorithms, the performance measures include the
        number of processors and communication cost. Let n be the input size,
        and p the number of processors. Then, T (n, p), or simply T (n) if p is
        known from the context, denotes the running time of the algorithm using p
        processors. If the algorithm has two parameters, n and m, then we write
        T (n, m, p). We may also write T (n, p) or T (n, m) if m or p are known from
        the context. In Example 1.1, T (n, n/2) = Θ(log n), while in Example 1.2,
        T (n, n) = Θ(1). The cost of an algorithm is the product of the running
        time and number of processors, e.g., C(n, p) = pT (n, p). In Example 1.1,
May 7, 2022   11:14     Parallel Algorithms        9in x 6in    b4591-ch01              page 6




        6                                 Parallel Algorithms


        C(n, n/2) = Θ(n log n), while in Example 1.2, C(n, n) = Θ(n). The work
        done by an algorithm is the total number of operations done by individ-
        ual processors. It is less than or equal to the cost of the algorithm. In
        Example 1.1, W (n, n/2) = n/2 + n/4 + · · · + 1 = n − 1 = Θ(n), while in
        Example 1.2, W (n, n) = Θ(n), since there are n comparisons.
            The ratio S(p) = T (n, 1)/T (n, p) is called the speedup of the algo-
        rithm, where T (n, 1) should be taken from the best sequential algorithm.
        An algorithm achieves a perfect speedup if S(p) = p. In Example 1.1,
        S(n/2) = Θ(n/ log n), while in Example 1.2, S(n) = Θ(n). A useful measure
        of the utilization of the processors is the eﬃciency of a parallel algorithm,
        which is deﬁned as E(n, p) = S(p)/p = T (n, 1)/pT (n, p). The eﬃciency is
        the ratio of the time used by one processor with a sequential algorithm
        and the total time used by p processors, which is the cost of the algo-
        rithm. The eﬃciency indicates the percentage of the processors’ time that
        is not wasted, compared to the sequential algorithm. If E(n, p) = 1, then
        the amount of work done by all processors throughout the execution of
        the algorithm is equal to the amount of work required by the sequential
        algorithm. In this case, we get optimal usage of the processors. All in all,
        the goal is to maximize eﬃciency. In Example 1.1, E(n, n/2) = Θ(1/ log n),
        while in Example 1.2, E(n, n) = Θ(n)/nΘ(1) = Θ(1).
May 7, 2022   11:14     Parallel Algorithms           9in x 6in   b4591-ch02            page 7




                                              Chapter 2


         Shared-memory Computers (PRAM)



        2.1     Introduction

        The parallel random-access machine (PRAM) was intended as the parallel-
        computing analogy to the random-access machine (RAM). It is used to
        model parallel algorithmic performance such as time complexity, where the
        number of processors assumed is typically also stated. As in the RAM,
        the PRAM model neglects issues such as synchronization and communica-
        tion, but includes the number of processors. Algorithm cost, for instance, is
        estimated using two parameters: time × number of processors. Read/write
        conﬂicts are resolved by one of the following models:
        • Exclusive read exclusive write (EREW): In this strategy, every processor
          can read or write to a memory cell at a time.
        • Concurrent read exclusive write (CREW): Here, multiple processors can
          read a memory cell but only one can write to it at a time.
        • Exclusive read concurrent write (ERCW): This is never considered.
        • Concurrent read concurrent write (CRCW): In this strategy, multiple
          processors can read from or write to the same memory cell at the same
          time.
           In the CRCW model, the writes cause some discrepancies, and hence
        the write is further deﬁned as:
        • COMMON: If all processors write the same value, it is successful; other-
          wise it is illegal.


                                                  7
May 7, 2022   11:14      Parallel Algorithms         9in x 6in       b4591-ch02               page 8




        8                                  Parallel Algorithms

        • ARBITRARY: Only one arbitrary attempt by an arbitrary processor is
          successful.
        • PRIORITY: Processors are ranked, and the processor with the maximum
          rank can write.

            Array reduction uses associative binary operations (e.g., SUM, Logical
        AND or MAX) of processor contents. Only either the maximum of proces-
        sors’ contents, or the sum of all contents in all processors can be written.
            In the PRAM, there is no limit on the number of processors in the
        machine. Any memory location is accessible from any processor, and there
        is no limit on the amount of shared memory in the system.


        2.2     The Balanced Tree Method

        The balanced tree method is one of the parallel algorithmic design tech-
        niques usually implemented either as the main component or as a subtask
        of the parallel algorithm. Let ◦ be a binary associative operation (e.g.,
        +, ×, min, max), and consider computing the expression

                                        s = a1 ◦ a2 ◦ · · · ◦ an ,

        where n = 2k for some nonnegative integer k (see Example 1.1). Sequen-
        tially, the expression can be computed by scanning the input from left to
        right. In parallel, b1 = a1 ◦ a2 , b2 = a3 ◦ a4 , . . . , bn/2 = an−1 ◦ an are com-
        puted in one parallel step to produce a new expression s = b1 ◦ b2 ◦ · · · ◦ bn/2
        consisting of n/2 operands. This process continues until there is only one
        value to compute. This procedure deﬁnes a complete binary tree where
        the input is initially at its leaves, and each internal node corresponds to
        a subproblem, while the root corresponds to the overall problem. Each
        leaf node is assigned a processor Pi , 1 ≤ i ≤ n. The internal nodes at
        level j, 0 ≤ j ≤ k − 1, are assigned processors P1 , P2 , . . . , P2j . The compu-
        tations at the internal nodes of the same level are performed in one parallel
        step. Figure 2.1 depicts a typical complete binary tree for n = 8. It has
        2n − 1 nodes. Note that it is represented by the array B[1..2n − 1], where
        the children for B[j], 1 ≤ j ≤ n − 1, are stored at B[2j] and B[2j + 1]. For
        j, 1 ≤ j ≤ n − 1, if B[2j] = x and B[2j + 1] = y, then B[j] = x ◦ y.
             Algorithm paraddition performs the operation of addition on n
        numbers stored initially in array A[1..n]. The ﬁrst for loop copies the
May 7, 2022    11:14         Parallel Algorithms        9in x 6in        b4591-ch02        page 9




                                  Shared-memory Computers (PRAM)                       9




                          Fig. 2.1.    The computation of s = a1 ◦ a2 ◦ · · · ◦ an .




        numbers in A into B[n], B[n + 1], . . . , B[2n − 1], which correspond to the
        leaves of the binary tree. The for loop in Line 3 is repeated k = log n
        times, once for each internal level of the tree. The for loop at line 4 is for
        performing 2i additions in parallel, i = k − 1, k − 2, . . . , 0.


          Algorithm 2.1 paraddition
          Input: A[1..n], an array of n numbers, where n = 2k .
          Output: A[1] + A[2] + · · · + A[n].
              1.   for j ← 1 to n do in parallel
              2.       B[j + n − 1] ← A[j]
              3.   for i ← k − 1 downto 0 do
              4.       for j ← 2i to 2i+1 − 1 do in parallel
              5.           B[j] ← B[2j] + B[2j + 1]
              6.       end for
              7.   end for
              8.   return B[1]




            The running time of the algorithm is equal to the depth of the binary
        tree, which is Θ(log n). The work done by the algorithm is proportional to
        the number of additions performed in the internal nodes, which is n − 1.
        The cost of the algorithm is n × Θ(log n) = Θ(n log n).
May 7, 2022   11:14    Parallel Algorithms              9in x 6in        b4591-ch02    page 10




        10                                   Parallel Algorithms

        2.3     Brent Theorem

        Consider the algorithm for ﬁnding the maximum of n numbers on the
        EREW PRAM using the balanced tree method (see Fig. 2.1). The algorithm
        uses n/2 processors. Note that n/2 processors are only required by the ﬁrst
        step of the algorithm. In the second step, only n/4 processors are needed.
        In the third step, only n/8 processors are needed, and so on. Therefore, in
        a very short time, most of the processors will be idle. The running time of
        the algorithm is Θ(log n). We can reduce the number of processors signiﬁ-
        cantly without aﬀecting the time complexity as follows. Let the number of
        processors be n/ log n, and assign log n numbers to each processor. Now,
        each processor ﬁnds the maximum in its group sequentially using log n − 1
        comparisons, and the parallel algorithm continues to ﬁnd the maximum of
        the n/ log n group maxima. Thus, the running time is still Θ(log n), while
        the cost of the algorithm is reduced from Θ(n log n) to Θ(n). The following
        theorem, known as Brent’s theorem, generalizes the above discussion.


        Theorem 2.1 Suppose an algorithm Ap performs tp parallel steps using p
        processors on the PRAM such that the total number of operations over all
        processors is s, and let q = s/tp . Then, there exists an algorithm Aq that
        performs at most 2tp parallel steps using q processors. Moreover, if the
        sequential time complexity is O(s), then the cost of Aq is optimal.

        Proof.     Let si , 1 ≤ i ≤ tp , be the number of operations performed by
        all p processors in step i of Algorithm Ap . Let Algorithm Aq emulate Ap
        by replacing each parallel step i of Ap by si /q parallel steps. The total
        number of parallel steps performed by algorithm Aq is thus

                                              tp  
                                                 si
                                    tq =
                                              i=1
                                                    q
                                              tp           
                                                  s i × tp
                                        =
                                              i=1
                                                         s
                                              tp                    
                                                  s i × tp
                                        ≤                       +1
                                              i=1
                                                          s
May 7, 2022   11:14      Parallel Algorithms        9in x 6in    b4591-ch02                   page 11




                              Shared-memory Computers (PRAM)                            11


                                                 tp 
                                                    tp
                                          = tp +       si
                                                 s i=1
                                          = 2tp ,
             tp
        since i=1   si = s. The new cost of the algorithm is ≤ 2tp × tsp = 2s = O(s).
        Hence, if the sequential time complexity is O(s), then Algorithm Aq is cost-
        optimal.                                                                   

            Thus, in O-notation, if the original running time is O(tp ), and the work
        is O(s), then the number of processors can be reduced to O(s/tp ) without
        increasing the running time. Recall Algorithm paraddition in Section 2.2
        for the addition of n numbers using n processors. The running time of the
        algorithm is Θ(log n) and it performs a total of O(n) operations. Its cost is
        Θ(n log n). By Brent Theorem, the number of processors can be reduced to
        n/log n without changing the time complexity. The new cost is Θ(n), which
        is optimal.


        2.4     Sorting in Θ(1) Time on the CRCW PRAM Model

        Let A[1..n] be an array of n elements to be sorted on the CRCW PRAM
        model with n2 processors. We use the SUM criterion for resolving write
        conﬂicts. In other words, if k processors need to write x1 , x2 , . . . , xk simul-
        taneously in the same memory location, then the sum x1 + x2 + · · · + xk
        is written in that memory location. Assume for simplicity that the ele-
        ments are distinct. The rank of element A[i] is deﬁned to be the number
        of elements in A less than A[i]. Algorithm sortingcrcw performs the
        operation of sorting on A. There are concurrent writes in Line 3, as more
        than one processor may attempt to write to the same memory location. For
        instance, A[1] will be compared with A[1], A[2], A[3], . . . , A[n] simultane-
        ously, and many processors may attempt to execute the statement r[i]← 1
        at the same time. These concurrent writes are resolved using the sum oper-
        ation. Speciﬁcally, the sum of all 1’s will be assigned to r[i], which is the
        rank of A[i]. Note that there are no write conﬂicts in the assignment in
        Line 7. Clearly, the running time of the algorithm is Θ(1), and its cost is
        Θ(n2 ). The above algorithm is also sometimes referred to as enumeration
        sort.
May 7, 2022    11:14          Parallel Algorithms           9in x 6in     b4591-ch02     page 12




        12                                          Parallel Algorithms


          Algorithm 2.2 sortingcrcw
          Input: A[1..n], an array of n elements.
          Output: A[1..n] sorted in ascending order.
              1.   for i ← 1 to n do in parallel
              2.       for j ← 1 to n do in parallel
              3.           if A[i] > A[j] then r[i] ← 1 else r[i] ← 0
              4.       end for
              5.   end for
              6.   for i ← 1 to n do in parallel
              7.       A[r[i] + 1] ← A[i]
              8.   end for



        2.4.1          Implementation on the CREW PRAM model
        The above algorithm can be implemented to run on the CREW PRAM
        with n processors only, but the running time will increase substantially.
        The CREW algorithm is shown as Algorithm sortingcrew.

          Algorithm 2.3 sortingcrew
          Input: A[1..n], an array of n elements.
          Output: A[1..n] sorted in ascending order.
           1.      for i ← 1 to n do in parallel
           2.          r[i] ← 0
           3.      for i ← 1 to n do in parallel
           4.          for j ← 1 to n do
           5.               if A[i] > A[j] then r[i] ← r[i] + 1
           6.          end for
           7.      end for
           8.      for i ← 1 to n do in parallel
           9.          B[r[i] + 1] ← A[i]
          10.      end for
          11.      return B



            The diﬀerence between this algorithm and Algorithm sortingcrcw for
        the CRCW PRAM is that the for loop in Line 4 is now sequential. There
        are no concurrent writes, but there are concurrent reads. For instance,
        comparing A[1] with any pair of A[1], A[2], A[3], . . . , A[n] will not take
        place simultaneously, and hence the statement r[1]← r[1] + 1 will not be
        executed more than once at the same time. However, A[1], for example,
        will be fetched n times simultaneously when comparing A[1], A[2], . . . , A[n]
May 7, 2022   11:14        Parallel Algorithms   9in x 6in       b4591-ch02                 page 13




                                Shared-memory Computers (PRAM)                        13


        with A[1]. Clearly, the running time is Θ(n), and the cost is Θ(n2 ). We will
        see later in this chapter that sorting n elements on the CREW PRAM can
        be eﬀected in optimal Θ(log n) time using n processors.


        2.4.2         Implementation on the EREW PRAM model
        The above CREW algorithm can be implemented to run on the EREW
        PRAM with n processors without increasing the running time or cost; we
        only need to take care of concurrent reads. The EREW algorithm is given as
        Algorithm sortingerew. In this algorithm, A[j] is compared starting with

          Algorithm 2.4 sortingerew
          Input: A[1..n], an array of n elements.
          Output: A[1..n] sorted in ascending order.
           1.   for j ← 1 to n do in parallel
           2.       r[j] ← 0
           3.       C[j] ← A[j]
           4.   end for
           5.   for i ← 1 to n − 1 do
           6.       for j ← 1 to n do in parallel
           7.            k ← i + j (mod n) if k = 0 then k ← n
           8.            if A[j] > C[k] then r[j] ← r[j] + 1
           9.       end for
          10.   end for
          11.   for i ← 1 to n do in parallel
          12.       B[r[i] + 1] ← A[i]
          13.   end for
          14.   return B



        the element at distance i. Figure 2.2 depicts an example of the comparisons
        performed by the algorithm on 8 elements. In this ﬁgure, comparing x
        and y is shown by an arrow from x to y. As is evident from the ﬁgure,
        there are no concurrent reads or concurrent writes. In the ﬁrst iteration of
        the outer for loop, A[1] is compared with C[2] = A[2], A[2] is compared
        with C[3] = A[3], etc. (see Fig. 2.2(a)). In the second iteration, that is,
        when i = 2, A[1] is compared with C[3] = A[3], A[2] is compared with
        C[4] = A[4], etc. (see Fig. 2.2(b)). In the third iteration, that is, when i = 3,
        A[1] is compared with C[4] = A[4], A[2] is compared with C[5] = A[5], etc.
        (see Fig. 2.2(c)). Finally, in the last iteration, when i = n − 1, A[1] is
        compared with C[n] = A[n], A[2] is compared with C[1] = A[1], and so
May 7, 2022   11:14           Parallel Algorithms            9in x 6in           b4591-ch02               page 14




        14                                          Parallel Algorithms

                             (a)
                              1       2        3         4         5         6    7      8



                            (b)
                              1      2        3         4      5         6       7      8




                            (c)
                              1      2        3         4      5         6       7      8



                      Fig. 2.2.    Example of the action of Algorithm sortingerew.


        forth. Clearly, the running time is Θ(n), and the cost is Θ(n2 ). We will see
        later in this chapter that sorting n elements on the EREW PRAM can be
        achieved in optimal Θ(log n) time using n processors.


        2.5     Parallel Preﬁx

        Let X = x1 , x2 , . . . , xn  be a sequence of n numbers, where n = 2k for
        a nonnegative integer k. Let ◦ be a binary associative operation deﬁned
        on X. The preﬁx sums problem is to compute the n partial sums: s1 = x1 ,
        s2 = x1 ◦x2 , . . . , si = x1 ◦x2 ◦· · ·◦xi , . . . , sn = x1 ◦x2 ◦· · ·◦xn . It is also called
        the scan or the scan operation. We will call s1 , s2 , . . . , sn the preﬁx sums.
        Algorithm parprefix is a simple iterative procedure to compute the preﬁx
        sums. The algorithm uses n processors. There are k = log n iterations in the
        outer loop in Step 5. Since the time needed for the loop in Step 6 is Θ(1),
        the running time of the algorithm is Θ(log n). Its cost is n × Θ(log n) =
        Θ(n log n), which is not optimal in view of the Θ(n) time complexity for the
        sequential algorithm. The work can be computed as follows. The number
        of operations done by Step 6 in the ﬁrst iteration is n − 1, and in the
                                                                  
        jth iteration it is n − 2j−1 . Thus, W (n) = kj=1 (n − 2j−1 ) = Θ(n log n).
        The cost can be reduced to Θ(n) by reducing the number of processors to
        n/ log n, and making some simple modiﬁcations.
May 7, 2022    11:14          Parallel Algorithms     9in x 6in       b4591-ch02                     page 15




                                   Shared-memory Computers (PRAM)                              15


          Algorithm 2.5 parprefix
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers, where n = 2k .
          Output: S = s1 , s2 , . . . , sn , the preﬁx sums of X.
           1.      for i ← 1 to n do in parallel
           2.          si ← x i
           3.      end for
           4.      t ←1
           5.      for j ← 1 to k do
           6.          for i ← t + 1 to n do in parallel
           7.              si ← si−t ◦ si
           8.          end for
           9.          t ← 2t
          10.      end for
          11.      return S



            Another algorithm for computing the preﬁx sums is shown as Algorithm
        parprefixrec, which is recursive. First, it recursively computes the preﬁx
        sums s2 , s4 , s6 . . . , sn . It then computes s1 , s3 , s5 , . . . , sn−1 using the com-
        bined divide-and-conquer step. Except for the recursive call, the parallel
        time is Θ(1). Hence, T (n) = Θ(log n). We compute the work done by the
        algorithm as follows. There are n/2 and n/2 − 1 iterations in the loops
        in Steps 3 and 7, respectively. Therefore W (n) = W (n/2) + Θ(n) = Θ(n).
        The cost, however, is not optimal since the number of processors needed is
        n/2 for a total cost of Θ(n log n).


          Algorithm 2.6 parprefixrec
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers, where n = 2k .
          Output: S = s1 , s2 , . . . , sn , the preﬁx sums of X.
              1.   s1 ← x 1
              2.   if n = 1 then return S = x1 
              3.   for i ← 1 to n/2 do in parallel
              4.       x2i ← x2i−1 ◦ x2i
              5.   end for
              6.   Recursively compute the preﬁx sums of x2 , x4 , . . . , xn  and store them
                   in s2 , s4 , . . . , sn 
           7.      for i ← 2 to n/2 do in parallel
           8.          s2i−1 ← s2(i−1) ◦ x2i−1
           9.      end for
          10.      return S = s1 , s2 , . . . , sn 
May 7, 2022   11:14                Parallel Algorithms             9in x 6in           b4591-ch02                page 16




        16                                               Parallel Algorithms


              1           2    3        4      5          6    7       8       Input


                          3             7                11           15       Add odd to even

                      3                 10               21          36        Find prefix sums of even
                               3               5               7


                               6             15               28               Add computed prefix sums to odd

              1       3       6        10     15         21   28     36        Final prefix sums

              Fig. 2.3.       Example of recursive parallel preﬁx, Algorithm parprefixrec.

           There are no concurrent reads or writes in the above two algorithms,
        and hence they run on the EREW PRAM.

        Example 2.1 Figure 2.3 shows an example of the recursive parallel preﬁx
        algorithm, parprefixrec. We will use addition as the binary operation.
        The input is given in Line 1. In Line 2, the odd-indexed numbers are added
        to the even-indexed numbers. In Line 3, the preﬁx sums are computed
        recursively for the even-indexed numbers. These preﬁx sums are shown
        in boldface: 3, 10, 21, 36. These preﬁxes are added to the odd-indexed
        numbers, which results in the odd-indexed preﬁx sums. These preﬁx sums
        are shown in Line 5 in boldface: 6, 15, 28. The last line shows the ﬁnal
        preﬁx sums.                                                             


        2.5.1         Array packing
        Let A = a1 , a2 , . . . , an  be an array of n elements such that t of them are
        “marked” and the remaining n − t elements are “unmarked”. The array
        packing problem consists of creating another array D where all the marked
        elements are moved to the lower part of D and the unmarked ones to
        the upper part of the array D without changing their relative order. One
        method of packing consists of assigning a value of 1 to each of the marked
        elements and a value of 0 to each of the unmarked elements. A new array
        B = b1 , b2 , . . . , bn  is used to hold the 0–1 values, with bi = 1 if and only
        if ai is marked. Now, if we apply the preﬁx sums algorithm to the array B
        and store the preﬁx sums in C = c1 , c2 , . . . , cn , the ranks of the marked
May 7, 2022   11:14         Parallel Algorithms          9in x 6in      b4591-ch02        page 17




                                 Shared-memory Computers (PRAM)                      17

        elements will be computed in C. Speciﬁcally, if ai is marked, then it is stored
        in D at position ci . So, the marked elements are moved to the ﬁrst t cells
        of array D. Likewise, the ranks of the unmarked elements are computed
        by interchanging 0’s and 1’s in array B. Finally, the preﬁx computation is
        run again and the unmarked elements are moved to the last n − t cells of
        array D.


        Example 2.2 We now illustrate array packing explained above. Refer-
        ring to Fig 2.4, the problem requires us to pack the even elements to the
        left. The ﬁrst row, part (a), contains the input array A. The second row,
        part (b), contains the 0–1 values in array B. Array C in part (c) of the
        ﬁgure contains the result of applying parallel preﬁx on array B. Array D
        in part (d) contains the even numbers packed in their positions as given
        in array C. If we now interchange 0’s and 1’s in B, then we can pack the
        odd numbers using the same procedure to pack the even numbers. This is
        shown in Figs. 2.4(e)–(g).
                                                                               

                      (a)



                      (b)



                      (c)


                      (d)



                      (e)


                      (f)


                      (g)



                                  Fig. 2.4.       Example of array packing.
May 7, 2022    11:14         Parallel Algorithms           9in x 6in     b4591-ch02        page 18




        18                                         Parallel Algorithms

        2.5.2          Parallel quicksort
        A parallel version of the quicksort algorithm for the EREW PRAM with
        n processors is shown as Algorithm parquicksort. As in the sequential
        quicksort algorithm, the pivot v is chosen as A[1]. First, the pivot is copied
        n times to avoid concurrent reads. This can be done by a broadcasting
        procedure in time Θ(log n)(see Exercise 2.5). Next, array packing is used
        to partition the array A into two parts, one with elements less than v and
        one with elements greater than v. This takes Θ(log n) time as explained in
        Section 2.5.1. Next, these two parts are sorted recursively into A1 and A2 ,
        whose concatenation together with v is returned as the sorted array. In the
        worst case, the recursion depth can be as large as Θ(n), causing the running
        time to be Θ(n log n). However, the average recursion depth is Θ(log n), for
        a total running time of Θ(log2 n).


          Algorithm 2.7 parquicksort
          Input: An array A[1..n] of n distinct numbers.
          Output: A sorted in ascending order.
              1.   if n = 1 then return A
              2.   v ← A[1]
              3.   Let B[i] = v for 1 ≤ i ≤ n
              4.   for i ← 1 to n do in parallel
              5.       if A[i] < B[i] then C[i] ← 1
              6.       else if A[i] > B[i] then C[i] ← 0
              7.   end for
              8.   Pack the numbers in A marked 1 in C at the beginning of A followed by
                   v followed by the numbers in A marked 0 in C
           9.      Let w be the position of v in A
          10.      do in parallel
          11.          A1 ← parquicksort(A, 1, w − 1)
          12.          A2 ← parquicksort(A, w + 1, n)
          13.      A ← A1 ||v||A2 , the concatenation of A1 , v and A2
          14.      return A



        2.6        Parallel Search

        Consider the search problem: Given a sequence S = a1 , a2 , . . . , an  of n
        distinct elements drawn from a linearly ordered set such that a1 < a2 <
        · · · < an , and an element x, ﬁnd the index k, 1 ≤ k ≤ n, such that x = ak
May 7, 2022   11:14      Parallel Algorithms        9in x 6in        b4591-ch02               page 19




                              Shared-memory Computers (PRAM)                            19

        if x ∈ S and 0 otherwise. Assume that we have a CREW PRAM with p
        processors, 1 ≤ p < n. For convenience, let n = (p + 1)q. First, the sequence
        S is divided into p + 1 subsequences of q elements each, and x is compared
        to the elements at the p internal boundaries of these subsequences. That
        is, the algorithm compares x with p elements simultaneously; processor Pi
        compares x with aiq for 1 ≤ i ≤ p. We have the following cases:

        (1) If for some i, 1 ≤ i ≤ p, x = aiq , the algorithm returns k = iq and
            halts.
        (2) x < aq , and hence only the elements less than aq are kept for the next
            stage. This is shown as the shaded area in Fig. 2.5(a). In this case, the
            algorithm returns the index of x in a1 , a2 , . . . , aq−1 .
        (3) x > apq , and hence only the elements greater than apq are kept for the
            next stage. This is shown as the shaded area in Fig. 2.5(b). In this case,
            the algorithm returns pq plus the index of x in apq+1 , apq+2 , . . . , an .
        (4) There exists an i, 1 ≤ i < p, such that x > aiq and x < a(i+1)q . The
            next stage performs the search on aiq+1 , aiq+2 , . . . , a(i+1)q−1 . This is
            shown as the shaded area in Fig. 2.5(c). In this case, the algorithm
            returns iq plus the index of x in aiq+1 , aiq+2 , . . . , a(i+1)q−1 .


                 (a)




                 (b)




                 (c)




                                      Fig. 2.5.   Parallel search.
May 7, 2022   11:14       Parallel Algorithms           9in x 6in     b4591-ch02            page 20




        20                                      Parallel Algorithms

            The above discussion is summarized in Algorithm parsearch. In
        Step 1, all processors read x simultaneously in one step. Step 2 is the stop-
        ping condition for recursion, which happens when the number of remain-
        ing elements drops as or below the number of processors. In this case, n
        processors are allocated, and each processor tests one element for equality
        against x. If one processor ﬁnds element ai = x, it sets k = i. The remaining
        steps are as explained above.
                                                               n
            The size of the recursive call is approximately p+1  . Hence, the running
        time is given by the recurrence T (n) = T (n/(p + 1)) + Θ(1) whose solution
                                        log n
        is T (n) = Θ(logp+1 n) = Θ( log(p+1)  ). There are at most p element compar-
        isons in each stage for a total of Θ(p logp+1 n). Hence, the work done by the
        algorithm is W (n) = Θ(p logp+1 n). If p = n , 0 <  < 1, then T (n) = Θ(1)
        and W (n) = Θ(n ).



          Algorithm 2.8 parsearch
          Input: A sequence S = a1 , a2 , . . . , an  of n distinct elements such that
                 a1 < a2 < · · · < an , and an element x.
          Output: The index k, 1 ≤ k ≤ n, such that x = ak if x ∈ S and 0 otherwise.
           1.   Initialize: k ← 0, All processors read x
           2.   if n ≤ p use n processors to compare x with ai , 1 ≤ i ≤ n, and return k.
           3.   q ← n/(p + 1)
           4.   for i ← 1 to p do in parallel
           5.       Processor Pi compares x with aiq
           6.       if x = aiq return k = iq
           7.   if x < aq then
           8.       let S  = a1 , a2 , . . . , aq−1 
           9.       k ← parsearch(S  , x)
          10.       return k
          11.   else if x > apq then
          12.       let S  = apq+1 , apq+2 , . . . , an 
          13.       k ← parsearch(S  , x)
          14.       return k + pq
          15.   else let i be such that x > aiq and x < a(i+1)q . do
          16.       let S  = aiq+1 , aiq+2 , . . . , a(i+1)q−1 
          17.       k ← parsearch(S  , x)
          18.       return k + iq
          19.   end if
          20.   return k = 0
May 7, 2022   11:14     Parallel Algorithms          9in x 6in        b4591-ch02          page 21




                             Shared-memory Computers (PRAM)                         21

        Example 2.3 We apply Algorithm parsearch for parallel search using
        two processors on the sequence S = 1, 3, 4, 6, 9, 12, 14, 15, 20 and x = 8.
        Initially, the algorithm divides S into three subsequences
                              1, 3, 4,      6, 9, 12,   14, 15, 20.
        The two processors compare x with elements at the internal boundaries,
        that is, 4 and 12. Since 8 > 4 and 8 < 12, the search area is reduced to
        6, 9. Finally, the two processors perform two comparisons simultaneously
        and both of them return 0 indicating that x is not found. The number of
        parallel steps is 2.                                                    

        Example 2.4 We apply Algorithm parsearch for parallel search using
        two processors on the same sequence in Example 2.3 and x = 14. Initially,
        the algorithm divides S into three subsequences
                              1, 3, 4,      6, 9, 12,   14, 15, 20.
        The two processors compare x with elements at the internal boundaries,
        that is, 4 and 12. 14 > a6 = 12, so the search area is reduced to 14, 15, 20.
        Now, the number of remaining elements is greater than p, so the algorithm
        performs one more iteration and divides these elements into three subse-
        quences 14, 15 and 20. In this iteration, q = n/(p + 1) = 3/3 = 1.
        Since x = aq = a1 = 14, the algorithm returns 6 + 1 = 7. The number of
        parallel steps is 3.                                                         


        2.7     Pointer Jumping

        Let L denote a linked list of n elements, and let us associate a processor
        with each element in the list. Each element x has two ﬁelds: succ(x) and
        dist(x). succ(x) is a pointer that points to the next element in the list. The
        succ ﬁeld of the last element points to itself, that is, succ(L(n)) = L(n).
        The other ﬁeld dist is initially 1 if succ(x) = x and 0 if succ(x) = x. An
        algorithm is required to be developed to compute: for each element x —
        its distance from the end of the list and to store it in dist(x). Algorithm
        pjumping computes the distances from each node to the end of the list
        using a technique called pointer jumping or doubling.
May 7, 2022    11:14        Parallel Algorithms           9in x 6in     b4591-ch02        page 22




        22                                        Parallel Algorithms


          Algorithm 2.9 pjumping
          Input: A Linked list L = (dist(x), succ(x)), 1 ≤ x ≤ n.
          Output: dist(x), 1 ≤ x ≤ n, the distance of x from the end of the list.
              1. for x ← 1 to n do in parallel
              2.     s(x) ← succ(x)
              3.     while s(x) = s(s(x)) do
              4.         dist(x) ← dist(x) + dist(s(x))
              5.         s(x) ← s(s(x))
              6.     end while
              7. end for



            Pointer jumping consists of updating the successor of each node by
        that node successor’s successor. Thus, the distance between a node and its
        successor doubles unless it is its own successor. Hence, after k iterations,
        the distance between a node x and its successor is 2k unless succ(x) is the
        last element in the list. It follows that the while loop is executed log n
        times, which means the parallel time complexity of the algorithm is T (n) =
        Θ(log n). Its cost, however, is Θ(n log n) since there are n processors.

        Example 2.5 Figure 2.6 illustrates the algorithm for a list of seven
        elements. Each pointer s(x) is shown as an arc from one element to another,
        and the arc from element x is labelled with the current value of dist(x).
        The original list is shown on the top of the ﬁgure, and the rest of the ﬁgure
        shows the lists after each of the three iterations.                        


        2.8      Euler Tour

        The Euler tour technique on trees is a very powerful tool when designing
        parallel algorithms for trees. Let G be a directed graph. An Euler circuit in
        G is a cycle that visits each edge exactly once. G is said to be Eulerian if it
        has an Euler circuit. It is well-known that G is Eulerian if and only if the
        indegree of each vertex is equal to its outdegree. Let T = (V, E) be a given
        tree, and let T  = (V, E  ) be obtained from T by replacing each edge (u, v)
        of T by two directed edges (u, v) and (v, u) in opposite directions. Then, T 
        is Eulerian since the indegree of each vertex is equal to its outdegree (see
        Fig. 2.7). We now show how to construct an Euler circuit in T , which is
        commonly known as Euler tour.
May 7, 2022   11:14         Parallel Algorithms           9in x 6in           b4591-ch02            page 23




                                 Shared-memory Computers (PRAM)                                23




                                        Fig. 2.6.      Pointer jumping.


                      (a)                                       (b)
                            2                     5                   2                    5

                                 1          4                             1         4

                            3                     6                   3                    6

                                Fig. 2.7.       (a) A tree. (b) A directed tree.


            A tree T is represented by its adjacency lists as shown in Fig. 2.8 for
        the adjacency lists of the graph shown in Fig. 2.7(a). The edges in each list
        are listed in a counterclockwise order. We deﬁne the function next(e) to
        be the edge following edge e in the adjacency lists. Note that the lists are
        circular, so if e is the last edge in its list, then next(e) is the ﬁrst edge in
        the list. Each edge (i, j) in the adjacency lists has two pointers, one to the
        next edge and the other to the edge (j, i).
            An Euler tour can be deﬁned by specifying the successor function
        succ(e), which gives the next edge in the tour. Let v be a vertex in the
May 7, 2022   11:14        Parallel Algorithms           9in x 6in        b4591-ch02                    page 24




        24                                       Parallel Algorithms



                                1          (1,2)          (1,3)        (1,4)


                                2          (2,1)


                                3          (3,1)


                                4          (4,1)          (4,6)        (4,5)


                                5          (5,4)


                                6          (6,4)

                         Fig. 2.8.   Adjacency lists of the tree in Fig. 2.7(a).

        (undirected) tree T , and suppose that its degree is d. Let the vertices
        adjacent to v be u0 , u1 , . . . , ud−1 listed in counterclockwise order. Then,
        succ((ui , v)) = (v, u(i+1)mod d ). The successor function can also be obtained
        directly from the adjacency lists using the equation
                                       succ((i, j)) = next((j, i)).
        If the resulting tour is τ = e1 , e2 , . . . , ek , then τ deﬁnes a depth-ﬁrst order
        on the set of vertices.

        Example 2.6 Consider the tree shown in Fig. 2.7(a). Vertex 1 has the
        adjacent vertices 2, 3 and 4, in this order. Hence, succ((2, 1)) = (1, 3),
        succ((3, 1)) = (1, 4), and succ((1, 2)) = (2, 1). Using the next function,
        succ((2, 1)) = next((1, 2)) = (1, 3), and so on. The next and succ functions
        for all edges in the tree are shown in Table 2.1. It follows that the Euler
        tour starting from edge (1, 2) is
          τ = (1, 2), (2, 1), (1, 3), (3, 1), (1, 4), (4, 6), (6, 4), (4, 5), (5, 4), (4, 1), (1, 2).

        The above Euler tour τ deﬁnes the following depth-ﬁrst ordering on the set
        of vertices: 1, 2, 1, 3, 1, 4, 6, 4, 5, 4, 1.                           

           Clearly, using the two pointers in each node of the adjacency lists, it
        takes Θ(1) time to ﬁnd succ(e), and hence, the Euler tour can be computed
        in Θ(1) steps using O(n) processors on the EREW PRAM.
May 7, 2022    11:14         Parallel Algorithms       9in x 6in      b4591-ch02                page 25




                                  Shared-memory Computers (PRAM)                           25

                              Table 2.1.     The next and successor functions.

                             edge e                next(e)             succ(e)

                             (1,2)                  (1,3)               (2,1)
                             (1,3)                  (1,4)               (3,1)
                             (1,4)                  (1,2)               (4,6)
                             (2,1)                  (2,1)               (1,3)
                             (3,1)                  (3,1)               (1,4)
                             (4,1)                  (4,6)               (1,2)
                             (4,6)                  (4,5)               (6,4)
                             (4,5)                  (4,1)               (5,4)
                             (5,4)                  (5,4)               (4,1)
                             (6,4)                  (6,4)               (4,5)




        2.8.1          Directing a tree
        The Euler tour technique on trees can be used to make a tree directed. The
        ﬁrst step is to assign a root, which we will assume to be the ﬁrst vertex in
        the tour r. This can be done by deleting the last edge in the tour, which
        converts the Euler circuit into an Euler path. Next, we assign 1 to every
        edge in the resulting tour, and apply the preﬁx sums algorithm on the set of
        edges deﬁned by the tour. Finally, for each edge (u, v) assign the parent of v
        p(v) = u whenever the preﬁx sum of (u, v) is smaller than the preﬁx sum
        of (v, u). The algorithm is given as Algorithm directingtree. Clearly,
        the algorithm runs in O(log n) time using O(n) processors on the EREW
        PRAM.

          Algorithm 2.10 directingtree
          Input: A tree T and a vertex r in T .
          Output: Assign parents to all nodes in T except r.
              1.   Find an Euler tour τ for the tree T .
              2.   Remove the last edge (x, r) from τ .
              3.   Assign 1 to every edge of the tour τ .
              4.   Apply parallel preﬁx on the set of edges of τ .
              5.   Assign p(r) = 0, p(x) = r, and for each other edge (u, v) assign p(v) = u
                   whenever the preﬁx sum of (u, v) is smaller than the preﬁx sum of (v, u).
May 7, 2022    11:14               Parallel Algorithms               9in x 6in            b4591-ch02                  page 26




        26                                                Parallel Algorithms


                     (a)                                   (b)                             (c)
                           2                        5            2                    5           1
                                   1         1                           1       7
                           1           1            1            2           5        6
                               1            4                        1           4            2       3       4
                           1                       1             3                   8
                               1             1                       4           9
                           3                       6             3                   6                    5       6

                                       Fig. 2.9.        Directing the tree in Fig. 2.7(a).



        Example 2.7 We convert the tree shown in Fig. 2.7(a) into a directed
        tree rooted at vertex 1. Consider the tree shown in Fig. 2.9(a), which is the
        tree in Fig. 2.7(b) with assigned weights of 1 to all edges. The last edge
        in the tour τ has been deleted. The preﬁx sums are shown in Fig. 2.9(b),
        and the rooted tree is shown in Fig. 2.9(c). Note, for example, that 4 is the
        parent of 6 since the preﬁx sum on edge (4, 6) is smaller than the preﬁx
        sum on edge (6, 4) in Fig. 2.9(b).                                         



        2.8.2          Computing vertex levels in a tree
        Let T be a tree rooted at vertex r. The vertex level of a vertex v is the
        distance between v and the root r measured in the number of edges. Note
        that we have assumed here that T is rooted. Let τ be the Euler path starting
        at r. On this path, assign the weights w(p(v), v) = 1 and w(v, p(v)) = −1,
        and perform parallel preﬁx on τ . Finally, set level (v) to the preﬁx sum of the
        edge (p(v), v). The algorithm is given as Algorithm treelevels. Clearly,
        the algorithm runs in O(log n) time using O(n) processors on the EREW
        PRAM.

          Algorithm 2.11 treelevels
          Input: A tree T rooted at r.
          Output: Assign levels to all nodes in T .
              1.   Find an Euler tour τ for the tree T .
              2.   Remove the last edge from τ .
              3.   Assign the weights w(p(v), v) = 1 and w(v, p(v)) = −1.
              4.   Apply parallel preﬁx on the set of edges of τ .
              5.   Set level (v) to the preﬁx sum of the edge (p(v), v).
May 7, 2022   11:14           Parallel Algorithms             9in x 6in            b4591-ch02                           page 27




                                      Shared-memory Computers (PRAM)                                               27


                  (a)                               (b)                             (c)
                         2                   5            2                    5             1 0
                              1         -1                        1       1
                        -1        1          1            0           1        2     1       1           1
                              1         4                     1           4              2       3        4
                         1                   1            1                   2
                             -1         -1                    0           1                      2             2
                        3                    6            3                   6                      5        6

          Fig. 2.10.     Computing levels of the vertices in the tree shown in Fig. 2.7(a).

        Example 2.8 We compute the levels of the vertices in the tree shown
        in Fig. 2.7(a), where vertex 1 is the root. Consider the tree shown in
        Fig. 2.10(a), which is the tree in Fig. 2.7(b) with assigned weights of 1
        and −1 as explained above. The last edge in the tour τ has been deleted.
        The preﬁx sums are shown in Fig. 2.10(b), and the tree with levels of the
        vertices is shown in Fig. 2.10(c).                                     



        2.9     Merging by Ranking

        Given a sequence S and an element x, let rank(x, S) be the number of
        elements in S less than x. It is not hard to modify Algorithm parsearch
        given in Section 2.6 so that on input S and x, it returns rank(x, S). We will
        refer to the modiﬁed algorithm as Algorithm modparsearch.

        2.9.1         Computing ranks
        Let A = a1 , a2 , . . . , an  and B = b1 , b2 , . . . , bm  be two sequences of n + m
        distinct numbers, each sorted in increasing order. The problem of merging A
        and B into a new sequence C = c1 , c2 , . . . , cm+n  may be solved in parallel
        by computing for each x ∈ A ∪ B, rA = rank(x, A) and rB = rank(x, B),
        and setting ck = x, where k = rA + rB + 1. The ranks of all items in
        B are found in parallel, where a processor Pi is assigned to each element
        bi ∈ B. To ﬁnd rank(bi , A), Pi performs binary search on A, and this is
        done for all bi ∈ B in parallel. To compute the rank of bi in B, we use the
        identity rank(bi , B) = i − 1. Next, we repeat the above procedure for all
        items aj ∈ A to ﬁnd rank(aj , B) and set rank(aj , A) = j − 1. The above
        algorithm works on the CREW PRAM in time O(max{log n, log m}).
May 7, 2022   11:14           Parallel Algorithms           9in x 6in     b4591-ch02                  page 28




        28                                          Parallel Algorithms




                                     Fig. 2.11.       Computing rank(B, A).


            In the following, we present a faster algorithm that runs in time
        O(log log n) for the case m = n. First, we develop an algorithm for comput-
        ing rank(B, A) = {rank(b, A) | b ∈ B}; ﬁnding rank(A, B) can be achieved
                                                            √
        in a similar fashion. For clarity, let s = m. First, use Algorithm mod-
                                                                                               √
        parsearch to compute in parallel the ranks of bs , b2s , . . . , bm , using n
        processors for each rank. Call these ranks r(s), r(2s), . . . , r(m). This divides
        the remaining elements in B into s subsequences B0 , B1 , . . . , Bs−1 of s − 1
        elements each, where B0 = {b1 , b2 , . . . , bs−1 }, B1 = {bs+1 , bs+2 , . . . , b2s−1 },
        and in general Bi = {bis+1 , bis+2 , . . . , b(i+1)s−1 }. This induces a par-
        tition of {a1 , a2 , . . . , ar(m) } into s subsequences A0 , A1 , . . . , As−1 , where
        A0 = {a1 , a2 , . . . , ar(s) }, A1 = {ar(s)+1 , ar(s)+2 , . . . , ar(2s) }, and in general
        Ai = {ar(is)+1 , ar(is)+2 , . . . , ar((i+1)s) } (see Fig 2.11).
            Note that |Ai | may vary; it may be 0 or n. Let bis+j ∈ Bi . Then, we
        should search for rank(bis+j , Ai ) in Ai , and compute rank(bis+j , A) from
        the equation

                            rank(bis+j , A) = rank(bis , A) + rank(bis+j , Ai ).             (2.1)

        Note that this means

                      If rank(b(i+1)s , A) = rank(bis , A), then rank(bis+j , Ai ) = 0.

        Thus, the problem of computing the ranks of B in A reduces to computing
        the ranks of Bi in Ai , 0 ≤ i ≤ s − 1. Call the algorithm recursively on
        (Ai , Bi ) to compute rank(Bi , Ai ) for 0 ≤ i ≤ s−1. For bis+j ∈ Bi , let ri (j) =
        rank(bis+j , Ai ). Thus, as stated in Eq. 2.1, rank(bis+j , A) = r(is) + ri (j).
            The above discussion is outlined in Algorithm parrank. The algo-
        rithm returns R = {r(1), r(2), . . . , r(m)}, a set of m ranks, where
May 7, 2022   11:14        Parallel Algorithms       9in x 6in         b4591-ch02                   page 29




                                Shared-memory Computers (PRAM)                                 29


        r(i) = rank(bi , A). In Line 8, the algorithm returns Ri = {ri (1), ri (2), . . . ,
        ri (s − 1)}, a set of s − 1 ranks corresponding to rank(Bi , Ai ).


          Algorithm 2.12 parrank
          Input: A = a1 , a2 , . . . , an  and B = b1 , b2 , . . . , bm  are two sequences of
                 n + m distinct numbers, each sorted in increasing order.
          Output: rank(B, A) = {rank(bi , A) | bi ∈ B}.
           1. if m < 4 then for i ← 1 to m do in parallel
           2.     Use Algorithm modparsearch to compute
                  r(i) = rank(bi , A) using n processors.
           3. for i ← 1 to s do in parallel
           4.     Use Algorithm modparsearch to compute
                                                √
                  r(is) = rank(bis , A) using n processors.
           5. end for
           6. r(0) ← 0
           7. for i ← 0 to s − 1 do in parallel
           8.     if r(is) = r((i + 1)s) then Ri ← {0, 0, . . . , 0}
           9.     else
          10.         Ri ← parrank(Ai , Bi )
          11.         for j ← 1 to s − 1 do in parallel
          12.             r(is + j) ← r(is) + ri (j)
          13.         end for
          14.     end if
          15. end for
          16. return R = {r(1), r(2), . . . , r(m)}



             It is easy to see that the number of processors used by the algorithm
               √ √
        is O( m n) = O(m + n) as required by Steps 3 and 4 of the algorithm.
                                                                       √
        Steps 1–4 take constant time. Step 10 takes at most T (n, m) time since
        |Ai | can be as large as n. Hence, the running time is given by the recurrence
                                      
                                       O(1)                        if m < 4
                           T (n, m) ≤       √
                                       T (n, m) + O(1)             if m ≥ 4,

        whose solution is T (n, m) = O(log log m). The work done by Steps 1–2 of
        the algorithm is O(n). The number of operations done by Steps 3 and 4
             √ √
        is O( m n) = O(m + n) since the call to Algorithm modparsearch
                     √               √
        performs O( n) × O(1) = O( n) operations. The work done by Steps 7–16
        of the algorithm except for the recursive calls is O(m). It follows that the
        overall work done by the algorithm is W (n, m) = O((n + m) log log m).
May 7, 2022    11:14           Parallel Algorithms           9in x 6in     b4591-ch02                  page 30




        30                                           Parallel Algorithms


        Example 2.9           Let A = 10, 30, 40, 60, 70, 90, 110, 120 and B =
        20, 50, 80, 100, so m = 4 and n = 8. s = 2, b2 = 50 and b4 = 100. First, the
        ranks of b2 and b4 are computed: r(2) = 3 and r(4) = 6. Next, B0 , B1 , A0
        and A1 are computed: B0 = {20}, B1 = {80}, A0 = {10, 30, 40} and
        A1 = {60, 70, 90}. Now, the algorithm recursively computes the ranks of B0
        in A0 and B1 in A1 : r0 (1) = 1 (which is the rank of 20 in A0 ), so R0 = {1},
        and r1 (1) = 2 (which is the rank of 80 in A1 ), so R1 = {2}. Finally, we
        compute the ranks of B0 in A and B1 in A: r(1) = r(0) + r0 (1) = 0 + 1 = 1
        and r(3) = r(2) + r1 (1) = 3 + 2 = 5. It follows that R = R(B, A) =
        {1, 3, 5, 6}.                                                               

        Example 2.10 Suppose we change B in Example 2.9 to B = 7, 8,
        80, 100. Then, b2 = 8, r(2) = 0 and B0 = {7}. Also, A0 = {} and A1 =
        {10, 30, 40, 60, 70, 90}. By Step 8 of the algorithm, since r(0) = r(2), R0 =
        {0} and thus the algorithm will not be called recursively on A0 and B0 .
        Consequently, r(1) = r(0) + r0 (1) = 0 + 0 = 0.                             


        2.9.2          Merging
        To merge A and B, we only need to compute rank(B, A) and rank(A, B).
        Algorithm parmerge merges A and B into a sequence C. It is assumed
        here that |A| = |B| = n. Let bi ∈ B. Then, the index of bi in C is equal to
        rank(bi , B) + rank(bi , A) + 1 = (i − 1) + r(i) + 1 = r(i) + i. Similarly, for
        aj ∈ A, the index of aj in C is equal to rank(aj , A) + rank(aj , B) + 1 =
        (j − 1) + r(j) + 1 = r(j) + j.

          Algorithm 2.13 parmerge
          Input: A = a1 , a2 , . . . , an  and B = b1 , b2 , . . . , bn  are two sequences of 2n
                 distinct numbers each sorted in increasing order.
          Output: A sequence C = c1 , c2 , . . . , c2n  which is the merge of A and B.
              1.   {r(1), r(2), . . . , r(n)} ← parrank(A, B) (Find rank(B, A))
              2.   {r  (1), r  (2), . . . , r  (n)} ← parrank(B, A) (Find rank(A, B))
              3.   for i ← 1 to n do in parallel
              4.         ci+r(i) ← bi
              5.         ci+r (i) ← ai
              6.   end for
              7.   return C
May 7, 2022   11:14         Parallel Algorithms          9in x 6in         b4591-ch02             page 31




                                 Shared-memory Computers (PRAM)                              31


           Clearly, the running time of Algorithm parmerge is T (n)                           =
        O(log log n). The cost of the algorithm is C(n) = O(n log log n).


        2.9.3         Parallel bottom-up merge sorting
        The algorithm for bottom-up sorting works by merging pairs of consecutive
        elements, then merging consecutive pairs to form 4-element sequences, and
        so on. This algorithm can easily be parallelized as shown in Algorithm
        parbottomupsort. Note here that n = 2k for some positive integer k.


          Algorithm 2.14 parbottomupsort
          Input: A = a1 , a2 , . . . , an , a sequences of n distinct numbers, where n = 2k .
          Output: A sorted in increasing order.
           1.   for j ← 1 to n do in parallel
           2.       S0,j ← aj
           3.   end for
           4.   for i ← 1 to k do
           5.       t ← n/2i
           6.       for j ← 1 to t do in parallel
           7.           Si,j ← parmerge(Si−1,2j−1 , Si−1,2j )
           8.       end for
           9.   end for
          10.   A ← Sk,1
          11.   return A


            Algorithm parbottomupsort deﬁnes a (conceptual) complete binary
        tree whose nodes are the sequences Si,j , 0 ≤ i ≤ k, 1 ≤ j ≤ 2k−i . Ini-
        tially, the elements are stored at the leaves S0,j , 1 ≤ j ≤ n. Subsequently,
        the sequence Si,j corresponding to an internal node is computed by merg-
        ing its children Si−1,2j−1 and Si−1,2j . Now, we compute the running time
        of the algorithm. Algorithm parmerge is called in Step 7, and it takes
        O(log log |Si−1,2j−1 |) = O(log log 2i−1 ). This is repeated in the for loop in
        Step 4 k times, for sizes 1, 2, 4, . . . , n/2. Hence, the running time is

                                                  
                                                  k
                                        T (n) =         O(log log 2i−1 )
                                                  i=1
May 7, 2022   11:14            Parallel Algorithms               9in x 6in            b4591-ch02                       page 32




        32                                           Parallel Algorithms


                                                         
                                                         k
                                                     =         O(log(i − 1))
                                                         i=1

                                                         
                                                         k
                                                     =         O(log k)
                                                         i=1

                                                     = O(k log k)
                                                     = O(log n log log n).


        2.10       The Zero-one Principle

        A sorting algorithm is called oblivious if it consists of comparison-exchange
        operations that are prescribed and independent of the input elements and
        results of comparisons between them. The zero-one principle states that if a
        comparison-based oblivious algorithm sorts any sequence of zeros and ones,
        then it sorts any sequence of arbitrary values. It really simpliﬁes the proofs
        of correctness of many oblivious sorting algorithms.


        Lemma 2.1 If an oblivious comparison-exchange algorithm sorts any
        sequence of zeros and ones, then it sorts any sequence of arbitrary values.

        Proof.        Suppose for the sake of contradiction that an oblivious
        comparison-exchange algorithm sorts all sequences of zeros and ones, but
        fails to sort the input sequence x1 , x2 , . . . , xn  of arbitrary numbers. Let π
        be a permutation such that xπ(1) ≤ xπ(2) ≤ · · · ≤ xπ(n) , and for some per-
        mutation σ = π, let the output of the algorithm be xσ(1) , xσ(2) , . . . , xσ(n) .
        Then, there exists some integer j such that xσ(i) = xπ(i) for i < j and
        xσ(j) > xπ(j) . Hence, there must exist k > j such that xσ(k) = xπ(j) . For
        1 ≤ i ≤ n, deﬁne yi = 0 if xi ≤ xπ(j) , and yi = 1 if xi > xπ(j) . Now, con-
        sider the action of the algorithm on input y1 , y2 , . . . , yn  of 0’s and 1’s. The
        algorithm will perform the same set of comparison-exchange operations as
        it did for the original input x1 , x2 , . . . , xn . In particular, the output of the
        algorithm on the yi ’s input will be

              yσ(1) , yσ(2) , . . . , yσ(j−1) , yσ(j) , . . . , yσ(k) . . . = 0, 0, . . . , 0, 1, . . . , 0, . . . ,

        which is not sorted. This contradicts the assumption that the algorithm
        sorts all sequences of zeros and ones.                                
May 7, 2022    11:14          Parallel Algorithms        9in x 6in         b4591-ch02                      page 33




                                   Shared-memory Computers (PRAM)                                     33

        2.11       Odd–Even Merging

        Let A = a0 , a1 , . . . , an−1  and B = b0 , b1 , . . . , bn−1  be two sorted
        sequences of 2n distinct numbers, where n is a power of 2. The odd–even
        merging method is summarized in Algorithm oddevenmerge.

          Algorithm 2.15 oddevenmerge
          Input: Two sorted sequences A = a0 , a1 , . . . , an−1  and
                 B = b0 , b1 , . . . , bn−1  of n elements each sorted in ascending order,
                 where n = 2k .
          Output: The elements in S = A ∪ B in sorted order.
              1. if n ≤ 2 return the merge of A and B, and exit.
              2. Let Aeven = a0 , a2 , . . . , an−2  and Aodd = a1 , a3 , . . . , an−1  be the even
                 and odd subsequences of A, respectively.
              3. Let Beven = b0 , b2 , . . . , bn−2  and Bodd = b1 , b3 , . . . , bn−1  be the even
                 and odd subsequences of B, respectively.
              4. Recursively merge Aeven and Bodd to obtain C = c0 , c1 , . . . , cn−1 .
              5. Recursively merge Aodd and Beven to obtain D = d0 , d1 , . . . , dn−1 .
              6. Let E be the shuﬄe of C and D, that is,
                 E = c0 , d0 , c1 , d1 , . . . , cn−1 , dn−1 .
              7. Traverse the pairs (ci , di ) in E, 0 ≤ i ≤ n − 1, and interchange the
                 elements in each pair if they are out of order to obtain the sorted sequence
                 S = s0 , s1 , . . . , s2n−1 
              8. return S


            After the execution of Step 6, we have s0 = min{c0 , d0 }, s1 =
        max{c0 , d0 }, s2 = min{c1 , d1 }, s3 = max{c1 , d1 }, . . . , s2n−2 = min{cn−1 ,
        dn−1 }, s2n−1 = max{cn−1 , dn−1 }.
            The algorithm uses 2n processors on the EREW PRAM. Obviously,
        the time needed in each recursive call is Θ(1). Hence, the running time of
        the algorithm is governed by the recurrence T (n) = T (n/2) + Θ(1), whose
        solution is T (n) = Θ(log n). The work done by the algorithm is given by
        the recurrence W (n) = 2W (n/2) + Θ(n), and hence W (n) = Θ(n log n).

        Example 2.11 Let A = 1, 3, 4, 7 and B = 2, 5, 6, 8. Then, Aeven =
        {1, 4}, Aodd = {3, 7}, Beven = {2, 6}, Bodd = {5, 8}, C = 1, 4, 5, 8 and
        D = 2, 3, 6, 7. E = 1, 2, 4, 3, 5, 6, 8, 7. The pair (4, 3) is out of order,
        so 4 and 3 are exchanged. The same applies to the pair (8, 7). The sorted
        sequence is S = 1, 2, 3, 4, 5, 6, 7, 8. See Fig. 2.12.                      
May 7, 2022   11:14       Parallel Algorithms            9in x 6in             b4591-ch02         page 34




        34                                      Parallel Algorithms




                          Fig. 2.12.     An example of odd–even merging.



        Theorem 2.2 Algorithm oddevenmerge correctly merges A and B
        into S.

        Proof. Let A, B, C, D and E be as deﬁned in Algorithm oddevenmerge,
        and assume the elements in A ∪ B are distinct. By the zero-one principle
        (Lemma 2.1, we may assume that A and B consist of zeros and ones. Let x
        and y be the number of zeros in A and B, respectively. Then, Aeven has
         x2  zeros, Aodd has  x2  zeros, Beven has  y2  zeros, and Bodd has  y2  zeros.
        Consequently, C has w =  x2  +  y2  zeros and D has z =  x2  +  y2  zeros.
        Clearly, w and z diﬀer by at most 1, and hence we have the following three
        cases. If w = z or w = z + 1, then

                                      E = 0, 0, . . . , 0, 1, 1, . . . , 1,
                                                   w+z

        and E is sorted. If, however, w = z − 1, then

                                    E = 0, 0, . . . , 0, 1, 0, 1, . . . , 1,
                                                  2w

        and E will be sorted after making one exchange of 0 and 1.                          

           The algorithm for sorting is given as Algorithm oddevenmergesort.
        The running time of the algorithm is Θ(log2 n). Its work is Θ(n log2 n).
May 7, 2022    11:14            Parallel Algorithms                 9in x 6in        b4591-ch02        page 35




                                     Shared-memory Computers (PRAM)                               35


            Algorithm 2.16 oddevenmergesort
            Input: A sequence S = a0 , a1 , . . . , an−1  where n is a power of 2.
            Output: The elements in S in sorted order.
              1.    S1 ← a0 , a1 , . . . , an/2−1 .
              2.    S2 ← an/2 , an/2+1 , . . . , an−1 .
              3.    S1 ← oddevenmergesort(S1 )
              4.    S2 ← oddevenmergesort(S2 )
              5.    S ← oddevenmerge(S1 , S2 )
              6.    return S



        2.12            Bitonic Merging and Sorting

        A sequence S = a1 , a2 , . . . , an  is monotonically increasing if a1 ≤ a2 ≤
        · · · ≤ an , and is monotonically decreasing if a1 ≥ a2 ≥ · · · ≥ an . A
        sequence is monotone if it is monotonically increasing or monotonically
        decreasing. A monotone sequence can be represented pictorially as shown
        in Fig. 2.13(a), where there is a point for each item in the sequence. The
        sequence corresponding to this diagram is T = a1 , . . . , ai , . . . , aj , . . . , an ,
        where 1 < i < j < n. However, if we are not interested in the actual values
        of the items in the sequence, but only in their relative order, then we can
        simply represent a monotone sequence by a line segment. An example is
        shown in Fig. 2.13(b) for the monotonically increasing sequence T above.
        Figure 2.13(c) shows a generic monotone sequence in which the items and
        their number are immaterial. Thus, the diagram shown in Fig. 2.13(c) is
        the representation of any monotonically increasing sequence. Similarly, a
        monotonically decreasing sequence can be represented by a line segment
        with negative slope.

        (a)                         an         (b)                          an       (c)
                         aj
                                                                   aj

                                                      ai
                   ai
        a1                                    a1


        1           i     j         n          1           i            j        n

                                         Fig. 2.13.            A monotone sequence.
May 7, 2022   11:14      Parallel Algorithms           9in x 6in         b4591-ch02            page 36




        36                                     Parallel Algorithms


           A sequence S = a1 , a2 , . . . , an  is bitonic if it monotonically increases
        and then monotonically decreases, that is, there is an i, 1 ≤ i ≤ n, such
        that

                        a1 ≤ a2 ≤ · · · ≤ ai ≥ ai+1 ≥ ai+2 ≥ · · · ≥ an ,

        or can be circularly shifted to become monotonically increasing and then
        monotically decreasing. Thus, a sequence is also bitonic if it is monotone.
        For example, the sequence 1, 3, 5, 7, 4, 2 is bitonic, while 1, 3, 1, 2 is not.
        The sequence 7, 8, 3, 1, 0, 4 is also a bitonic sequence, because it is a cyclic
        shift of 0, 4, 7, 8, 3, 1. We will represent a bitonic sequence by a diagram
        consisting of a polygonal chain composed of line segments intersecting at
        their internal endpoints, with at most one local maximum and one local
        minimum. Each line segment represents a monotone sequence. Figure 2.14
        shows the diagrams of two bitonic sequences. In part (a) there is one local
        maximum, and in part (b) there is one local maximum and one local
        minimum. If the number of line segments is 1 or 2, then the diagram is
        a bitonic sequence. If the number of line segments is more than 2, then the
        diagram is a bitonic sequence if and only if there does not exist a horizon-
        tal line that intersects the polygonal chain at more than 2 points. To see
        this, consider Fig. 2.15, which shows the diagram of a sequence with three
        intersections of the polygonal chain with a horizontal line.
               The sequence corresponding to this diagram is a1 , . . . , ai , . . . , aj ,
        . . . , an , where 1 < i < j < n, with the following inequalities: a1 > ai , ai <
        aj , aj > an and an < a1 . If this sequence is bitonic, then the sequence
        α = a1 , ai , aj , an  such that a1 > ai < aj > an < a1 is bitonic. Then,
        it is possible through circular shifts to transform α into two monotonic


                (a)                                           (b)




                                    Fig. 2.14.      Bitonic sequences.
May 7, 2022   11:14        Parallel Algorithms             9in x 6in       b4591-ch02           page 37




                                Shared-memory Computers (PRAM)                            37




                                         1       i              j      n

                                 Fig. 2.15.          A non-bitonic sequences.


        sequences, one increasing followed by one decreasing. It can be shown, how-
        ever, that α cannot be converted to such a sequence. Hence, the sequence
        is not bitonic.
            Now, consider the sequence α obtained from α by increasing the value
        of an so that an > a1 . Then, we have a1 > ai < aj > an > a1 , and thus α
        is bitonic, as it can be transformed into α = ai , aj , an , a1 , which consists
        of two monotonic sequences. The diagram of α is similar to the one shown
        in Fig. 2.14(b); there does not exist a horizontal line that intersects this
        diagram at more than 2 points. The diagram of α is similar to the one
        shown in Fig. 2.14(a).


        Example 2.12 Consider the sequence α = 4, 1, 6, 3. Its diagram is the
        one shown in Fig. 2.15. In this sequence, 4 > 1 < 6 > 3 < 4, so α is
        obviously not a bitonic sequence. However, if we change 3 to 5 to obtain
        α = 4, 1, 6, 5, the new sequence is bitonic since in this case 4 > 1 <
        6 > 5 > 4. Its diagram is similar to the one shown in Fig. 2.14(b). With
        one cyclic shift, α is converted to α = 1, 6, 5, 4, which consists of two
        monotonic sequences — one increasing and one decreasing. Its diagram is
        similar to the one shown in Fig. 2.14(a).                                    

              Let S = a1 , a2 , . . . , an  be a bitonic sequence. Deﬁne

                S1 = min(a1 , an/2+1 ), min(a2 , an/2+2 ), . . . , min(an/2 , an ),   (2.2)

        and

               S2 = max(a1 , an/2+1 ), max(a2 , an/2+2 ), . . . , max(an/2 , an ).    (2.3)
May 7, 2022   11:14               Parallel Algorithms               9in x 6in         b4591-ch02                                   page 38




        38                                               Parallel Algorithms

                        (a)                                                     (b)




                                                                                                       z




                                             Fig. 2.16.          Bitonic sequences.

                  (a)                                                           (b)
                                        9     10
                                                                                              9            10
                                                 8
                              7                                                                                8
                        5                            6                                                                 7
                    3                                    4                                                         6
                                                                                                           5           4
              2                                              1                                     3
                                                                                             2                             1

                                       Fig. 2.17.        Bitonic sequences example.

              Then, both S1 and S2 are bitonic sequences. Moreover,
                                                   max(S1 ) ≤ min(S2 ).                                                    (2.4)
        Consider, for example, the bitonic sequence u, v, w, x, y shown in
        Fig. 2.16(a). Here, the line segment u, v accounts for approximately half
        the elements in the sequence. Shift the line segment u, v to the right until
        the vertex u is aligned vertically with w. The resulting line segment u , v 
        intersects the line segment x, y at the vertex z. Then, S1 = u , z, y and
        S2 = w, x, z, v  as shown in Fig. 2.16(b) are bitonic. It is clear from the
        ﬁgure that max(S1 ) ≤ min(S2 ).

        Example 2.13 Consider the bitonic sequence S = 2, 3, 5, 7, 9, 10, 8, 6, 4, 1
        shown in Fig. 2.17(a). If we apply the procedure described above for split-
        ting this sequence, we obtain the two bitonic sequences S1 = 2, 3, 5, 4, 1
        and S2 = 9, 10, 8, 6, 7 shown in Fig. 2.17(b). S2 is a cyclic shift of the
        sequence 10, 8, 6, 7, 9. Furthermore, max(S1 ) = 5 ≤ 6 = min(S2 ).        
           By Eq. (2.4), every element of the sequence S1 is less than or equal
        to every element of the sequence S2 . Thus, the problem of sorting the
May 7, 2022    11:14         Parallel Algorithms       9in x 6in      b4591-ch02                 page 39




                                  Shared-memory Computers (PRAM)                            39

        elements in S is reduced to sorting the elements in S1 and S2 separately.
        This is summarized in Algorithm bitonicmerge. It is important to note
        that the input to the algorithm is a bitonic sequence S of length n, where n
        is a power of 2, and the output is the elements in S in sorted order. The
        algorithm ﬁrst computes S1 and S2 as in Eqs. 2.2 and 2.3. Now, S1 and S2
        are bitonic sequences, so the algorithm recursively computes the two sorted
        sequences S1 and S2 , and returns their concatenation sequence S1 ||S2 .


          Algorithm 2.17 bitonicmerge
          Input: A bitonic sequence S = a1 , a2 , . . . , an , where n is a power of 2.
          Output: The elements in S in sorted order.
              1.   if |S| = 1 then return S
              2.   for i ← 1 to n/2 do in parallel
              3.       if ai > ai+n/2 then interchange ai and ai+n/2
              4.   end for
              5.   S1 = a1 , a2 , . . . , an/2 
              6.   S2 = an/2+1 , an/2+2 , . . . , an 
              7.   S1 ← bitonicmerge(S1 )
              8.   S2 ← bitonicmerge(S2 )
              9.   return S1 ||S2 , the concatenation of S1 and S2



            Algorithm bitonicmerge works on the EREW PRAM with n pro-
        cessors. The running time is Θ(log n) and the total amount of work is
        Θ(n log n), which is not optimal in view of the O(n) time sequential
        algorithm.


        Example 2.14 Consider the instance given in Fig. 2.18. Line 1 is the
        input bitonic sequence. Line 2 shows the ﬁrst split into two bitonic
        sequences. Lines 3 and 4 show the second and third splits, respectively. 




                              Fig. 2.18.     Bitonic merge example for n = 8.
May 7, 2022   11:14           Parallel Algorithms           9in x 6in     b4591-ch02     page 40




        40                                          Parallel Algorithms

                      (a)                                          (b)




                 Fig. 2.19.     (a) Increasing comparator. (b) Decreasing comparator.




                               Fig. 2.20.     Bitonic merge network for n = 8.



            A comparator is a devise with two inputs x and y, and two outputs
        min(x, y) and max(x, y). It is either an increasing comparator, shown in
        Fig. 2.19(a), or decreasing comparator, shown in Fig. 2.19(b). A network
        of comparators is composed solely of wires and comparators. Algorithm
        bitonicmerge can be implemented on a network of comparators, also
        called a merging network, as illustrated in Fig. 2.20. A sample input of a
        bitonic sequence OR bitonic sequences are shown on the wires. The merging
        network with n inputs consists of log n columns, called stages.


        2.12.1        Bitonic sorting
        Bitonic sorting essentially works like Algorithm mergesort in that it
        divides the input into two halves, sorts each half recursively and uses Algo-
        rithm bitonicmerge to merge the two sorted sequences. It is given in
        Algorithm bitonicsort. To merge two monotonic sequences S1 and S2
        sorted in ascending order, ﬁrst reverse S2 and form the bitonic sequence
        S3 obtained by concatenating S1 and S2 , where S2 is the reverse of S2 .
        Finally, apply Algorithm bitonicmerge to S3 .
May 7, 2022   11:14       Parallel Algorithms          9in x 6in        b4591-ch02        page 41




                               Shared-memory Computers (PRAM)                        41


          Algorithm 2.18 bitonicsort
          Input: A sequence S of n elements, where n is a power of 2.
          Output: The elements in S in sorted order.
           1. if |S| > 1 then
           2.     S1 ← a1 , a2 , . . . , an/2 
           3.     S2 ← an/2+1 , an/2+2 , . . . , an 
           4.     S1 ← bitonicsort(S1 )
           5.     S2 ← bitonicsort(S2 )
           6.     S2 ← Reverse of S2
           7.     S3 ← S1 ||S2 , the concatenation of S1 and S2
           8.     S ← bitonicmerge(S3 )
           9.     return S
          10. end if


           The algorithm uses n processors on the EREW PRAM. Obviously, the
        time needed in each recursive call is Θ(log n). Hence, the running time of
        the algorithm is governed by the recurrence
                                  
                                    c                    if n = 1
                          T (n) =
                                    T (n/2) + Θ(log n) if n ≥ 2,

        whose solution is T (n) = Θ(log2 n). The work done by the algorithm is
        W (n) = Θ(n log2 n), which is not optimal.

        Theorem 2.3 Algorithm bitonicsort correctly sorts a given sequence
        of numbers in ascending order.
        Proof.     By the zero-one principle (Lemma 2.1, we may assume that the
        input consists of 0’s and 1’s. Let A and B be two strings of 0’s and 1’s such
        that |A| + |B| = n, and assume without loss of generality that n = 2m ≥ 2.
        The proof is by induction on m. If m = 1, then clearly the input will
        be sorted, so assume that the algorithm correctly sorts its input for all
        powers h, 1 ≤ h < m, and let |A| + |B| = 2m . First, A and B will be sorted
        separately, and B will be reversed, and so they will look like the following:
                                         A = 0i 1j ,    B = 1k 0l .
        Next, some 1’s in A will be swapped with 0’s in B by Step 3 of Algorithm
        bitonicmerge. Let A and B  be A and B after swapping, respectively.
May 7, 2022   11:14        Parallel Algorithms           9in x 6in           b4591-ch02   page 42




        42                                       Parallel Algorithms


        If j ≤ l, all 1’s in A will be swapped with 0’s in B, and A will consist of
        0’s only. In this case, A and B  will look like:

                                      A = 0i+j ,       B  = 1k 0l−j 1j .

        If, however, j > l, then l 1’s in A will be swapped with l 0’s in B, and A
        and B  will look like the following:

                                      A = 0i 1j−l 0l ,      B  = 1k+l .

        Finally, A and B  will be merged separately and concatenated by Algorithm
        bitonicmerge to produce A ||B  , which is sorted in ascending order. 
            We can derive a sorting network by unrolling recursion as follows:
        Starting from n = 1, any sequence of length 1 is monotonic, and hence
        any sequence of length 2 is bitonic. In the ﬁrst stage of bitonic sort,
        bitonic sequences of size 2 are merged to create ordered lists of size 2.
        If these sequences alternate between being ordered into increasing and
        decreasing order, then at the end of this stage of merging, we have n/4
        bitonic sequences of size 4. In the next stage, bitonic sequences of size 4
        are merged into sorted sequences of size 4, alternately into increasing and
        decreasing orders so as to form n/8 bitonic sequences of size 8. Given an
        unordered sequence of size n, exactly log n stages of merging are required to
        produce a completely ordered sequence. Figure 2.21 shows a bitonic sorting




                      Merge (2) Merge (4)                     Merge (8)

                             Fig. 2.21.     Bitonic sort network for n = 8.
May 7, 2022   11:14     Parallel Algorithms    9in x 6in     b4591-ch02                  page 43




                             Shared-memory Computers (PRAM)                        43

        network with sample input of size 8. This network has three stages labeled
        Merge(2), Merge(4) and Merge(8). Stage 3 in the ﬁgure is identical to the
        merging network of Fig. 2.20.



        2.13      Pipelined Mergesort

        Recall the parallel bottom-up merge sorting algorithm, Algorithm par-
        bottomupsort, discussed in Section 2.9.3. The algorithm works by merg-
        ing pairs of consecutive elements, then merging consecutive pairs to form
        4-element sequences, and so on. The running time of the algorithm was
        shown to be O(log n log log n). In fact, there is a Ω(log log n)-time-lower
        bound for merging two sorted sequences of n elements using n processors
        on the CREW PRAM. In this section, we sketch an optimal Θ(log n) time
        algorithm for sorting n items on the CREW PRAM with Θ(n) processors.
        The algorithm can be modiﬁed to work on the EREW PRAM with the same
        time complexity. It is a modiﬁcation of Algorithm parbottomupsort, in
        which merges are pipelined eﬃciently. We will assume in this section that
        the elements to be sorted are all distinct and that n is a power of 2.
            Let a, b and c be three numbers such that a < c. We say that b is
        between a and c if a ≤ b < c. We also say that a and c straddle b.
        Given a sequence A and an element a, recall that rank(a, A) denotes the
        number of elements in A less than a. We will assume that all sequences
        and arrays are implicitly augmented with −∞ and ∞, so the rank of the
        minimum element is 1, not 0. Given two arrays A and B, the cross rank
        R(A, B) = rank(a, B) | a ∈ A. Let a and b be two adjacent items in B (if
        necessary, we let a = −∞ or b = ∞). We deﬁne the range [a, b) to be the
        interval induced by item a (including the cases a = −∞ and b = ∞). Let
        C be a sorted sequence of numbers. C will be called a 3-cover or simply a
        cover of A if each interval induced by consecutive elements of C contains
        at most three elements from A. More precisely, for any two consecutive ele-
        ments a and c in C∞ , the set {b ∈ A | a ≤ b < c} has at most 3 elements,
        where C∞ = {−∞} ∪ C ∪ {+∞}. For example, if C contains the numbers
        9, 18 and 30 while A contains 1, 5, 20, 23, 25 and 35, then C is a 3-cover for
        A. If, however, A also contains 28, then C is not a 3-cover for A, since in
        this case the number of elements between 18 and 30 is more than 3.
May 7, 2022   11:14        Parallel Algorithms           9in x 6in       b4591-ch02        page 44




        44                                       Parallel Algorithms

        2.13.1        The algorithm
        The sorting algorithm is described in terms of a complete binary tree T with
        n leaves. Initially, the n elements to be sorted are placed at the leaves of T ,
        one element per leaf, and the internal nodes contain empty sequences. Let v
        be an internal node in the tree. Lv will denote the sequence of leaves of the
        subtree Tv rooted at v. In the course of the algorithm, the internal nodes
        of T will contain sorted sequences of elements. The task of node v is to sort
        the sequence Lv . The algorithm goes through stages t, 1 ≤ t ≤ 3 log n − 2.
        By Av (t) we denote the sequence associated with node v at stage t. The
        items in Av (t) will be a rough sample of the items in Lv . As the algorithm
        proceeds, the size of Av (t) increases, and Av (t) becomes a more accurate
        approximation of Lv , and it will always be a sorted subsequence of Lv . We
        say that node v is complete at stage t if and only if Av (t) = Lv ; otherwise v
        is said to be active. Throughout the algorithm, node v from its left son x
        a sorted sequence Bx (t), and from its right son y a sorted sequence By (t)
        hence producing the sequence Bv (t + 1), which is sent to the parent of v.
        In each of these sequences, the size of the next object is twice as big as the
        size of the preceding one. That is, for all nodes v,

                      |Av (t + 1)| = 2|Av (t)|,        and     |Bv (t + 1)| = 2|Bv (t)|.

        We explain the processing performed in one stage at an arbitrary internal
        node v of the tree. The array Av (t) is the array at hand at the start of
        the stage; Av (t + 1) is the array at hand at the start of the next stage, and
        Av (t− 1) is the array at hand at the start of the previous stage, if any. Also,
        in each stage, we will create an array Bv (t) at node v; Bv (t + 1), Bv (t − 1)
        are the corresponding arrays in respectively, the next, and previous, stage.
        Bv (t) is a sorted array comprising every fourth item in Av (t), for the active
        node v.
            The computation performed during each stage at each internal node v
        comprises the following two phases:

        (1) Compute Bv (t)← α(Av (t)) and send it to the parent of v, where
            α(Av (t)) is computed as follows: If v is active, then α(Av (t)) consists
            of every fourth element of Av (t). During the ﬁrst stage after v becomes
            complete, α(Av (t)) consists of every fourth element of Av (t). During
            the second stage after v becomes complete, α(Av (t)) consists of every
May 7, 2022   11:14          Parallel Algorithms      9in x 6in      b4591-ch02        page 45




                                  Shared-memory Computers (PRAM)                  45


            second element of Av (t), while in the third stage α(Av (t)) consists of
            every element of Av (t).
        (2) If v is active, then merge Bx (t) with By (t) using the cover Av (t) to
            obtain Av (t + 1). That is, Av (t + 1)← Bx (t) ∪ By (t), where ∪ denotes
            merging. If v is complete, then v ignores its inputs Bx (t) and By (t).

           By (1) above, three stages after node v becomes complete, its parent
        becomes complete too. The exception is in stage 1 in which the nodes at
        the level before the last merge their inputs and become complete in one
        stage. Hence, the total number of stages of the algorithm is 3 log n − 2.
           Figure 2.22 illustrates the ﬂow of the algorithm with n = 8 by depicting
        stages 2–7, that is, after nodes d, e, f and g become complete. Note that
        the total number of stages is 3 log 8 − 2 = 7. In part (c) of this ﬁgure, we
        have Aa (4) = {}, Bb (4) = 8 and Bc (4) = 6. In part (d) of this ﬁgure,
        we have Aa (5) = 6, 8, Bb (5) = 5, 8 and Bc (5) = 3, 6.
           The proof of the following theorem is omitted.


                      (a)                              (b)




                      (c)                              (d)




                      (e)                              (f)




                            Fig. 2.22.    The ﬂow of the algorithm with n = 8.
May 7, 2022   11:14       Parallel Algorithms           9in x 6in     b4591-ch02            page 46




        46                                      Parallel Algorithms


        Theorem 2.4         Bv (t) is a 3-cover of Bv (t + 1).

        We will need the following observation to show that the merge can be
        performed in O(1) time.

        Observation 2.1 Let A and C be two sorted sequences such that C is a
        cover for A. Then, for any sorted sequence D, C ∪ D is a cover for A, where
        ∪ denotes the merge operation.

            By the above theorem, Bx (t − 1) is a 3-cover for Bx (t) for each node x.
        By the above observation, since Av (t) = Bx (t − 1) ∪ By (t − 1), we deduce
        Av (t) is a 3-cover for Bx (t); similarly, Av (t) is a 3-cover for By (t). Since
        Av (t + 1) = Bx (t) ∪ By (t), it follows that Av (t) is a 3-cover for Av (t + 1).
            We will assume that R(Av (t), Bx (t)) and R(Av (t), By (t)) are available.
        Let a be an item in Bx (t); the rank of a in Av (t+1) = Bx (t)∪By (t) is equal
        to the sum of its ranks in Bx (t) and By (t). So to perform the merge we
        compute the cross ranks R(Bx (t), By (t)) and R(By (t), Bx (t)) (the method
        is given below).


        2.13.2        Computing and maintaining ranks
        In order for the algorithm to perform the merges quickly in Θ(1) time, we
        show how to compute the ranks in Θ(1) time. We compute and maintain
        ranks as described in the following steps.

        (1) The ﬁrst step is to compute R(Bx (t), Av (t)) and R(By (t), Av (t)). For
            two adjacent items a and b with a < b, recall that the interval induced
            by item a is the range [a, b) (including the cases a = −∞ and b = ∞).
            Let u be an item in Av (t); u may be −∞. Consider the interval I(u)
            in Av (t) induced by u, and consider the set of items X(u) in Bx (t)
            contained in I(u) (there are at most three items in X(u) by the 3-cover
            property). X(u) can be found in Θ(1) time since R(Av (t), Bx (t)) is
            available, which means rank(u, Bx) is known. Each item a in X(u) is
            given its rank in Av (t) as rank(a, Av (t)) = rank(u, Av (t)) + 1 (note
            that all elements are distinct, which means a > u). For example, in
            Fig. 2.22(d), with t = 5, we have Aa (5) = {6, 8}, Bb (5) = {5, 8}.
            If we let u = −∞, then I(u) = (−∞, 6) and X(u) = {5}. Hence,
            rank(5, Aa (5)) = 0 + 1 = 1. This takes care of R(Bx (t), Av (t)). We
            repeat the symmetrical procedure to compute R(By (t), Av (t)). These
May 7, 2022   11:14     Parallel Algorithms           9in x 6in      b4591-ch02           page 47




                             Shared-memory Computers (PRAM)                         47




                            Fig. 2.23.        Computing R(Bx (t), By (t)).


            ranks are needed for computing R(Bx (t), By (t)) and R(By (t), Bx (t)),
            which are required by the merge step Av (t + 1)← Bx (t) ∪ By (t).
        (2) Now, we show how to compute R(Bx (t), By (t)); R(By (t), Bx (t)) can be
            found in a similar fashion. Let a be an item in Bx (t); we show how to
            compute its rank in By (t). (See Fig. 2.23.) We determine the two items
            b and c in Av (t) that straddle a, using rank(a, Av (t)) computed above.
            Suppose that b and c have ranks r and t, respectively, in By (t). Then, all
            items of rank r or less are smaller than item a (recall we assumed that
            all the inputs were distinct), while all items of rank greater than t are
            larger than item a; thus the only items about which there is any doubt
            as to their sizes relative to a are the items with rank s, r < s ≤ t. But
            there are at most three such items by the 3-cover property. By means
            of at most two comparisons, the relative order of a and these (at most)
            three items can be determined.
        (3) At this point, we ﬁnd the value for each item a in Bx (t), using its
            rank in By (t) computed above, the two items b and c in By (t) that
            straddle a, and the ranks of b and c in Av (t + 1). Similarly, we ﬁnd the
            value for each item d in By (t), using its rank in Bx (t), the two items e
            and f in Bx (t) that straddle d, and the ranks of e and f in Av (t + 1).
            This information is needed for computing R(Av (t + 1), Bx (t + 1)) and
            R(Av (t + 1), By (t + 1)).
        (4) Now, we show how to compute R(Av (t + 1), Bx (t + 1)) and R(Av
            (t + 1), By (t + 1)) can be found by a similar means. For each item
            a in Av (t + 1), we want to determine its rank in Bx (t + 1). Given the
            ranks for an item from Av (t) in both Bx (t) and By (t), we can imme-
            diately deduce the rank of this item in Av (t + 1) = Bx (t) ∪ By (t)
            (the new rank is just the sum of the two old ranks). Similarly, we
May 7, 2022   11:14       Parallel Algorithms           9in x 6in     b4591-ch02              page 48




        48                                      Parallel Algorithms


              obtain the ranks for items from Ax (t) in Ax (t + 1). This yields the
              ranks of items from Bx (t) in Bx (t + 1) (for each item in Bx (t)
              came from Ax (t), and Bx (t + 1) comprises every fourth or second
              item in Ax (t + 1), or every item in Ax (t + 1)). Consequently, for
              a ∈ Bx (t), rank(a, Bx (t + 1)) = 14 rank(a, Ax (t + 1)), if in stage t + 1 x
              is active or in the ﬁrst stage after being complete, rank(a, Bx (t + 1)) =
              1
              2 rank(a, Ax (t + 1)), if in stage t + 1 x is in the second stage after
              being complete, and rank(a, Bx (t + 1)) = rank(a, Ax (t + 1)), if in
              stage t + 1 x is in the third stage after being complete. For exam-
              ple, in Fig. 2.22, if t = 4, then we have Ab (4) = {2, 5, 7, 8},
              Bb (4) = {8}, Ab (5) = Ab (4), Bb (5) = {5, 8}, and rank(8, Bb (5)) =
              1
              2 rank(8, Ab (5)) = 2 (note that stages 4 and 5 are in parts (c) and
              (d) of the ﬁgure). Thus, for every item in Av (t + 1) that came from
              Bx (t) we have its rank in Bx (t + 1); it remains to compute the rank for
              those items in Av (t + 1) that came from By (t).
                 Let a be an item in By (t). We compute rank(a, Bx (t + 1)) as follows:
              Recall that for each item a from By (t), we computed the straddling
              items b and c from Bx (t). (See Fig. 2.24.) We know the ranks r and t of b
              and c, respectively, in Bx (t+1) (as asserted in the previous paragraph).
              Every item of rank r or less in Bx (t + 1) is smaller than a, while every
              item of rank greater than t is larger than a. Thus, the only items about
              which there is any doubt concerning their size relative to a are the items
              with rank s, r < s ≤ t. But there are at most three such items by the
              3-cover property. As before, the relative order of a and these (at most)
              three items can be determined by means of at most two comparisons.




                      Fig. 2.24.   Computing rank(a, Bx (t + 1)) for a ∈ By (t).
May 7, 2022   11:14        Parallel Algorithms    9in x 6in       b4591-ch02                    page 49




                                Shared-memory Computers (PRAM)                            49

        2.13.3        Analysis of the algorithm
        It is not diﬃcult to prove that the merge step takes Θ(1) time at each stage
        of the algorithm, given that we assign a processor to every array element.
        Hence, the total running time is Θ(log n). Now, we estimate the number of
        processors needed, which is equal to the total array elements at any stage
        of the algorithm. First, we compute the total number of items in the A(t)
        arrays. Let v be an internal node, and assume, as before, that x and y are
        the children of v. If |Av (t)| = 0 and x is not complete, then

                                                          1                      1
        2|Av (t)| = |Av (t + 1)| = |Bx (t)|+|By (t)| =      (|Ax (t)|+|Ay (t)|) = |Ax (t)|,
                                                          4                      2
        that is, |Av (t)| = 14 |Ax (t)|. So the total size of the A(t) arrays at v’s level is
        1
        8 the size of the A(t) arrays at x’s level, if x is not complete (the number
        of nodes at v’s level is 12 of that at x’s level). This need not be true at
        complete nodes x. It is true for the ﬁrst stage in which x is complete; but
        for the second stage, |Av (t)| = 12 |Ax (t)|, and so the total size of the A(t)
        arrays at v’s level is 14 of the total size of the arrays at x’s level; likewise, for
        the third stage, |Av (t)| = |Ax (t)|, and so the total size of the A(t) arrays
        at v’s level is 12 of the total size of the A(t) arrays at x’s level.
            Thus, on the ﬁrst stage in which x is complete, the total size of the
        A(t) arrays is bounded above by n + n/8 + n/64 + · · · = n + n/7; on the
        second stage, by n + n/4 + n/32 + · · · = n + 2n/7; on the third stage,
        by n + n/2 + n/16 + · · · = n + 4n/7. Using a similar argument, it can be
        shown that on the ﬁrst stage, the total size of the B(t) arrays is bounded
        above by 2n/7; on the second stage, by 4n/7; on the third stage, by 8n/7.
        We conclude that the algorithm needs Θ(n) processors (so as to have a
        processor standing by each item in the A(t) and B(t) arrays) and takes
        constant time for the merge step.
            The following theorem summarizes the main result. Its proof follows
        from Theorem 2.4 and the algorithm’s description and timing analysis.
        Recall that the algorithm can be modiﬁed to run on the EREW PRAM
        with the same complexities.


        Theorem 2.5 The pipelined mergesort algorithm sorts a sequence of n
        elements in Θ(log n) time using Θ(n) processors on the EREW PRAM.
May 7, 2022   11:14      Parallel Algorithms           9in x 6in     b4591-ch02               page 50




        50                                     Parallel Algorithms

        2.14      Selection

        The problem of selection is deﬁned as follows: Given a sequence A =
        a1 , a2 , . . . , an  of n elements and a positive integer k, 1 ≤ k ≤ n, ﬁnd the
        kth smallest element in A. A straightforward solution would be to sort A
        in Θ(log n) time and return the kth smallest element. However, the work
        done by this approach is Θ(n log n), which is not optimal. There is an opti-
        mal sequential algorithm that runs in Θ(n) time. It can be shown that this
        sequential algorithm can be parallelized to run on the PRAM in Θ(log2 n)
        time using n/ log n processors. In this section, we present an algorithm,
        which is shown as Algorithm parselect, to solve the selection problem,
        that runs in time O(log n log log n) and uses n/ log n processors. This algo-
        rithm is a modiﬁcation of the parallel version of the sequential selection
        algorithm.

          Algorithm 2.19 parselect
          Input: A sequence A = a1 , . . . , an  of elements and an integer k, 1 ≤ k ≤ n.
          Output: The kth smallest element in A
           1. c ← 1/ log (4/3)
           2. for j ← 1 to c log log n
           3.     Divide A into |A|/ log |A| groups of log |A| elements each.
           4.     Find the median of each group individually.
                  Let the set of medians be M .
           5.     Sort M and ﬁnd its median m.
           6.     Partition A into three sequences:
                  A1 = {a | a < m}
                  A2 = {a | a = m}
                  A3 = {a | a > m}
           7.     case
                      |A1 | ≥ k: A ← A1
                      |A1 | + |A2 | ≥ k: return m
                      |A1 | + |A2 | < k:
           8.              A = A3
           9.              k ← k − |A1 | − |A2 |
          10.     end case
          11. end for
          12. Sort A and return the kth smallest element in A.
May 7, 2022   11:14     Parallel Algorithms        9in x 6in         b4591-ch02            page 51




                             Shared-memory Computers (PRAM)                          51


            The for loop is executed c log log n times, where c = 1/ log (4/3), after
        which the number of elements in A drops to O(n/ log n). The algorithm
        then sorts A using the pipelined mergesort algorithm and the kth smallest
        element is returned in O(log n) time, using O(n/ log n) processors. Within
        the for loop, ﬁrst A is partitioned into |A|/ log |A| blocks of log |A| elements
        each. The median of each block is found using one processor in Θ(|A|)
        sequential time, and the median of medians m is computed by sorting the
        set M using the pipelined mergesort algorithm in Θ(log(|A|/ log |A|)) =
        O(log n) time, using Θ(|A|/ log |A|) processors. A is then partitioned to A1 ,
        of elements smaller than m, A2 of elements equal to m and A3 of elements
        greater than m. If |A1 | < k ≤ |A1 | + |A2 |, the algorithm terminates and
        returns m. Else, if |A1 | ≥ k, A is set to A1 . Otherwise, if |A1 | + |A2 | < k,
        then A is set to A3 and k is set to k − |A1 | − |A2 |.
            Partitioning A can be achieved by labeling the elements in A with num-
        bers 1, 2 and 3 according to whether a < m, a = m or a > m, respectively.
        Then, the parallel preﬁx algorithm can be used to extract and compact
        the arrays A1 , A2 and A3 . This can be achieved in Θ(log |A|) time using
        O(|A|/ log |A|) = O(n/ log n) processors. It follows that the for loop takes
        O(log n) time in each iteration.
            If we let s denote the group size, then the median of medians m is
        smaller than (and greater than) at least (|A|/2s)(s/2) = |A|/4 elements.
        That is, it is greater than (and smaller than) at most 3|A|/4 elements
        (Exercise 2.17). Thus, in the second iteration, |A| ≤ 3n/4, and in the jth
        iteration |A| ≤ (3/4)j n. Consequently, after c log log n iterations, the size
        of A is at most
                                       c log log n
                                        3
                                                     ×n
                                        4
                                     = (log n)c log (3/4) × n
                                               n
                                     =
                                       (log n)c log (4/3)
                                                    n
                                     =
                                       (log n)log (4/3)/ log (4/3)
                                          n
                                     =
                                       log n
                                     = p.

           Therefore, in Step 12, there will be enough processors to sort A in
        O(log p) = O(log n) time. Since the time required in each iteration is
May 7, 2022   11:14         Parallel Algorithms           9in x 6in     b4591-ch02                 page 52




        52                                        Parallel Algorithms


        O(log n), the running time of the algorithm is O(log n log log n). The work
        done in each iteration is O(|A|). Hence, the total work done is at most

                      n + (3/4)n + (3/4)2 n + · · · + (3/4)c log log n n = Θ(n),

        which is optimal. However, the cost, which is O(n log log n), is not optimal.


        2.15      Multiselection

        Let A = a1 , a2 , . . . , an  be a sequence of n elements drawn from a linearly
        ordered set, and let K = k1 , k2 , . . . , kr  be a sorted sequence of positive
        integers between 1 and n. The multiselection problem is to select the ki th
        smallest element for all values of i, 1 ≤ i ≤ r. To make the presentation sim-
        ple, we will assume that all elements in A are distinct. Consider Algorithm
        parmultiselect1. The algorithm initially uses n/ log n processors. In the
        two recursive calls, it uses p|A1 |/|A| and p|A2 |/|A| processors, where p is
        the current number of processors. The recurrence for the running time of
        this divide and conquer algorithm is T (n, r) = T (n, r/2) + O(log n log log n)
        since we used the parallel algorithm for selection, Algorithm parselect, of
        Section 2.14. As the recursion depth is log r, the solution to this recurrence
        is T (n, r) = O(log n log log n log r).

          Algorithm 2.20 parmultiselect1
          Input: A sequence A = a1 , a2 , . . . , an  of n elements, and a sorted sequence
                 of r positive integers K = k1 , k2 , . . . , kr . The number of processors p.
          Output: The ki th smallest element in A, 1 ≤ i ≤ r.
           1. r ← |K|
           2. If r > 0 then
           3.      Set k = kr/2 .v
           4.      Use Algorithm parselect to ﬁnd a, the kth smallest element in A.
           5.      Output a.
           6.      Let A1 = ai | ai < a and A2 = ai | ai > a.
           7.      Let K1 = k1 , k2 , . . . , kr/2−1  and
                   K2 = kr/2+1 − k, kr/2+2 − k, . . . , kr − k.
           8.      parmultiselect1(A1 , K1 , p|A1 |/|A|).
           9.      parmultiselect1(A2 , K2 , p|A2 |/|A|).
          10. end if
May 7, 2022   11:14      Parallel Algorithms         9in x 6in     b4591-ch02                   page 53




                              Shared-memory Computers (PRAM)                              53

            In the remaining of this section, we present an eﬃcient algorithm to
        solve this problem that runs in time

                        T (n, p) = O((n/p + ts (p, p))(log r + log(n/p)))

        on the PRAM with p processors, r ≤ p < n, where ts (p, p) is the time
        needed to sort p elements using p processors. If p = n/ log n, the running
        time becomes T (n, n/ log n) = O(log n(log r + log log n)).
            In the algorithm to be presented, we will use the following notation
        to repeatedly partition A into smaller subsets: Let a ∈ A with rank ka .
        Partition A into two subsets A = {x ∈ A | x ≤ a} and A = {x ∈
        A | x > a}. This partitioning of A induces the following bipartitioning of
        K: B  = {k ∈ K | k ≤ ka } and B  = {k − ka | k ∈ K and k > ka }. In
        this case, we will call each of (A , B  ) and (A , B  ) a selection pair . Let
        (A , B  ) be a selection pair. We will label (A , B  ) as “active” if |B  | > 0;
        otherwise it will be called “inactive”. The algorithm is given as Algorithm
        parmultiselect2.
            We turn to the analysis of the algorithm. First, we allocate a number
        of processors for each active set. Speciﬁcally, we assign p = (|A|/s)p pro-
        cessors for active set (A, B), where s is the number of remaining elements
        computed in Line 15. There are enough processors for all active sets. The
        set A is partitioned into p groups of w = |A|/p = s/p elements each. Note
        that w ≤ n/p = q. The median of medians m is smaller than (and greater
        than) at least (|A|/2w)(w/2) = |A|/4 elements. That is, it is greater than
        (and smaller than) at most 3|A|/4 elements (Exercise 2.17). Hence, after
        c log r iterations, the size of each subset is at most

                                                c log r
                                                3
                                                          ×n
                                                4
                                          = rc log (3/4) × n
                                                  n
                                          = c log (4/3)
                                            r
                                                      n
                                          = log (4/3)/ log (4/3)
                                            r
                                            n
                                          = .
                                             r
May 7, 2022    11:14         Parallel Algorithms           9in x 6in     b4591-ch02             page 54




        54                                         Parallel Algorithms


          Algorithm 2.21 parmultiselect2
          Input: A sequence A = a1 , . . . , an  of elements and a sorted sequence of
                 positive integers B = k1 , k2 , . . . , kr , 1 ≤ ki ≤ n. The number of
                 processors p.
          Output: The ki th smallest element in A, 1 ≤ i ≤ r.
              1. L ← {(A, B)}; Mark (A, B) “active”; s ← n; q ← n/p.
              2. c ← 1/ log (4/3)
              3. Repeat Steps 4–16 c(log r + log q) times.
              4. for each active pair (A, B) ∈ L do in parallel
              5.     Assign p = (|A|/s)p processors for active set (A, B).
              6.     if |A| ≤ p then sort A and return the ki th smallest element for
                     1 ≤ i ≤ |B|.
              7.     else do
              8.          w ← |A|/p = s/p. Partition A into p subsequences
                          A1 , A2 , . . . , Ap of size at most w ≤ q each. Find the
                          median mi of each Ai . Sort these medians to obtain the
                          median of medians m.
           9.             Find k, the rank of m in A.
          10.             Partition A into A and A , where A (resp. A ) is the set of
                           elements in A less than or equal to (resp. greater than) m.
          11.              Partition B into B  and B  , where B  (resp. B  ) is the set
                           of elements in B less than or equal to (resp. greater than) k.
                           Subtract k from each rank in B  .
          12.              Replace (A, B) in L by (A , B  ) and (A , B  ).
          13.              If B  is empty, then mark (A , B  ) as “inactive”; otherwise
                           mark it as “active”. If B  is empty, then mark (A , B  ) as
                           “inactive”; otherwise mark it as “active”. Discard inactive
                           pairs.
          14.          end if
          15.          Let s be the number of all remaining elements.
          16.      end for
          17.      Sort all partitions A in all active pairs (A, B) ∈ L, and for each element
                   in B return its corresponding element in A.




           We observe that if A is partitioned into more than r subsets, then at
        most r of these subsets are active, and the rest are inactive, since the num-
        ber of ranks in B is ≤ r. Consequently, after c log r iterations, there are
        at most r subsets of size at most n/r each. Clearly, after c log q additional
May 7, 2022   11:14     Parallel Algorithms       9in x 6in       b4591-ch02                page 55




                             Shared-memory Computers (PRAM)                           55

        iterations, the size of active subsets in the ﬁrst stage will be reduced fur-
        ther by a factor of q, so that the size of each subset is upperbounded by
        n/rq = p/r. In other words, after c log q additional iterations, there are at
        most r subsets of size at most p/r each.
            Now, we compute the overall time needed by the algorithm in the ﬁrst
        log r iterations. Consider an arbitrary iteration where there are a number
        of subsets of total size less than or equal to n. We analyze the running
        time taken by a pair (A, B) of maximum size, that is, |A| ≤ n is maximum
        among all active pairs. Finding the medians mi takes O(q) sequential time.
        Sorting the medians can be done in ts (|A|/w, p ) = ts (p , p ) ≤ ts (p, p)
        parallel time. Computing k, the rank of m in A, and the sets A and A
        can be achieved in O(w + log p ) = O(q + log p) parallel time using parallel
        preﬁx and compaction. Since K is sorted, both B  and B  are computed
        using parallel p -search in O(logp r) = O(log r/ log p ) time. Hence, the time
        needed by the ﬁrst log r iterations is
                                                                  
                                                            log2 r
                          O (q + ts (p, p) + log p) log r +          .
                                                            log p

        Observe that ts (p, p) ≥ log p and since r ≤ p, we have

                                log2 r    log p log r
                                      
                                        ≤             ≤ log p log r.
                                log p       log p

        Hence, the above expression reduces to

                      O((q + ts (p, p)) log r) = O((n/p + ts (p, p)) log r).

            The time taken by the next log q iterations is asymptotically the same as
        that taken by the ﬁrst log r iterations, except that the number of iterations
        log r is replaced by log q. Hence, the remaining iterations can be completed
        in time O((q + ts (p, p)) log q) = O((n/p + ts (p, p)) log(n/p)).
            As to the sorting step in Line 17 of the algorithm, we have at most r
        subsets of size at most n/rq = p/r each to be sorted. If we allocate p/r pro-
        cessors to each of the r subsets, the time needed for sorting is ts (p/r, p/r),
        which is negligible.
            It follows that the time complexity of the algorithm is

                       T (n, p) = O((n/p + ts (p, p))(log r + log(n/p))).
May 7, 2022   11:14       Parallel Algorithms           9in x 6in     b4591-ch02               page 56




        56                                      Parallel Algorithms


        If, for example, we set p = n1− , 0 <  < 1, we may use a simple O(log2 p)
        sorting algorithm, and the above expression reduces to

                         T (n, n1− ) = O((n + log2 p)(log r + log(n )))
                                        = O(n (log r + log(n ))),

        which is optimal for r ≥ n , since the cost of the algorithm will be O(n log r).
        If, on the other hand, we set p = n/ log n and use the pipelined mergesort
        algorithm of Section 2.13, the time complexity becomes

                  T (n, n/ log n) = O((log n + log(n/ log n))(log r + log log n))
                                  = O(log n(log r + log log n)),

        which is optimal for r ≥ log n. This is superior to the running time of
        Algorithm parmultiselect1. If we let r = O(log n), the time complexity
        becomes O(log n log log n), which is the same as the running time for the
        classical selection of one element presented in Section 2.14. In the special
        case when r = 1 and p = n/ log n, the running time reduces to that of the
        O(log n log log n) parallel selection algorithm of Section 2.14.


        2.16      Matrix Multiplication

        Given two n × n matrices A and B, consider the problem of computing the
        product C = AB, where n = 2k for some positive integer k. Assume that
        there are n3 processors available, labeled Pi,j,l , 1 ≤ i, j, l ≤ n. Each entry
        ci,j of C is the dot product of two vectors: row i of A and column j of B.
        First we present an algorithm for the dot product. Algorithm dotproduct
        computes the dot product of two given vectors row i of A and column j
        of B of dimension n each using n processors. Lines 1 and 2 compute
        W = A[i, ∗]B[∗, j] in Θ(1) time. The rest of the algorithm is similar to
        Algorithm paraddition in Section 2.2. The second for loop copies the
        numbers in W into V [n], V [n + 1], . . . , V [2n − 1], which correspond to the
        leaves of the binary tree. The for loop in Line 5 is repeated k = log n
        times, once for each internal level of the tree. The for loop at line 6 is for
        performing 2r additions in parallel, r = k − 1, k − 2, . . . , 0. (See Section 2.2).
May 7, 2022    11:14       Parallel Algorithms    9in x 6in     b4591-ch02                   page 57




                                Shared-memory Computers (PRAM)                         57


          Algorithm 2.22 dotproduct
          Input: Two n × n matrices A and B and two indices i and j, n = 2k .
          Output: The dot product of row i of A and column j of B.
           1.    for l ← 1 to n do in parallel
           2.        W [l] ← A[i, l] ∗ B[l, j]
           3.    end for
           4.    for l ← 1 to n do in parallel
           5.        V [l + n − 1] ← W [l]
           6.    end for
           7.    for r ← k − 1 downto 0 do
           8.        for t ← 2r to 2r+1 − 1 do in parallel
           9.             V [t] ← V [2t] + V [2t + 1]
          10.        end for
          11.    end for
          12.    return V [1]



            The algorithm for matrix multiplication is a parallelization of the tradi-
        tional Θ(n3 ) time sequential algorithm. It is shown as Algorithm parma-
        trixmult. It uses n3 processors. The n processors Pi,j,1 , Pi,j,2 , . . . , Pi,j,n
        compute C[i, j] using Algorithm dotproduct.


          Algorithm 2.23 parmatrixmult
          Input: Two n × n matrices A and B, n = 2k .
          Output: The product C = AB.
              1. for i ← 1 to n do in parallel
              2.     for j ← 1 to n do in parallel
              3.         C[i, j] ← dotproduct(A, B, i, j)
              4.     end for
              5. end for
              6. return C



           Thus, the running time of the algorithm is dominated by the call to
        Algorithm dotproduct, which takes Θ(log n) time. The work done by the
        algorithm can be computed as follows. Line 3 is executed n2 times, and in
        each call to Algorithm dotproduct, it performs Θ(n) operations. Hence,
        the work done by the algorithm is Θ(n3 ). Notice that the algorithm requires
        concurrent read capability, and hence it runs on the CREW PRAM.
May 7, 2022   11:14      Parallel Algorithms           9in x 6in     b4591-ch02               page 58




        58                                     Parallel Algorithms

        2.17      Transitive Closure

        Assume that an n × n adjacency matrix representation of a directed graph
        G = (V, E) is given, where |V | = n. In such a representation, A(i, j) = 1 if
        and only if there is an edge from vi to vj in E, and A(i, j) = 0 if (vi , vj ) ∈
                                                                                       / E.
        The transitive closure of A is represented as an n × n Boolean matrix A∗
        in which A∗ (i, j) = 1 if and only if there is a path in G from vi to vj .
        A∗ (i, j) = 0 if no such path exists. One way to obtain the transitive clo-
        sure of A is to compute An by performing log n operations of squaring the
        matrix: A× A = A2 , A2 × A2 = A4 , and so on until a matrix Am is obtained
        where m ≥ n. Here, we use the Boolean matrix multiplication method, in
        which the operations of scalar multiplication and addition in the standard
        matrix multiplication are replaced by the logical “AND” and “OR” opera-
        tions, respectively. Since there are log n matrix multiplications, A∗ = An
        can be obtained in time Θ(log2 n) with Θ(n3 ) processors on the CREW
        PRAM using Boolean matrix multiplication (see Section 2.16). The total
        number of operations is Θ(n3 log n).

        2.18      Shortest Paths

        Let G = (V, E) be a weighted directed graph on n vertices, in which each
        edge (i, j) has a weight w[i, j]. If there is no edge from vertex i to vertex j,
        then w[i, j] = ∞. For simplicity, we will assume that V = {1, 2, . . . , n}. We
        assume that G does not have negative weight cycles, that is, cycles whose
        total weight is negative. The problem is to ﬁnd the distance from each
        vertex to all other vertices, where the distance from vertex i to vertex j is
        the length of a shortest path from i to j. Let i and j be two diﬀerent vertices
        in V . Deﬁne dki,j to be the length of a shortest path from i to j that contains
        at most k edges, 1 ≤ k ≤ n − 1. Thus, for example, d1i,j = w[i, j], d2i,j is
        the length of a shortest path from i to j that contains at most two edges,
        and so on. Then, by deﬁnition, dn−1 i,j is the length of a shortest path from i
        to j, i.e., the distance from i to j. Given this deﬁnition, we can compute
        dki,j recursively as follows.
                                    ⎧
                                    ⎪
                                    ⎨0                      if i = j
                             dki,j = w[i, j]                if k = 1
                                    ⎪
                                    ⎩ min {dk/2 + dk/2 } if k ≥ 2.
                                          l   i,l    l,j
May 7, 2022   11:14      Parallel Algorithms     9in x 6in      b4591-ch02                   page 59




                              Shared-memory Computers (PRAM)                           59


            Let Dk be the matrix whose entries are dki,j , 1 ≤ i, j ≤ n. Then, Dk
        can be obtained from Dk/2 by squaring, except that the operations “+”
        and “min” replace the usual matrix operations “×” and “+”, respectively.
        Letting D1 = (d1i,j ), we can use the operations “+” and “min” to evaluate
        D2 , D4 , . . . , Dm , where m is the smallest power of 2 ≥ n − 1. This takes
        log(n − 1) matrix multiplications. Hence, the running time is Θ(log2 n)
        using Θ(n3 ) processors on the CREW PRAM (see Section 2.16). The total
        number of operations is Θ(n3 log n).

        2.19      Minimum Spanning Trees

        Let G = (V, E) be a weighted undirected graph on n vertices, in which each
        edge (i, j) has a weight w[i, j]. If there is no edge from vertex i to vertex j,
        then w[i, j] = ∞. We will assume that V = {1, 2, . . . , n}. A spanning tree T
        of G is a subgraph T = (V, E  ) such that T is a tree. In what follows, we
        present an algorithm to construct a minimum spanning tree for a graph that
        is denoted by its weight matrix. We will assume without loss of generality
        that the weights are distinct. If they are not distinct, each weight of an
        edge e can be appended by the label of that edge. The algorithm to be
        presented is based on the following theorem whose proof is easy.

        Theorem 2.6 Let G = (V, E) be a weighted undirected graph. Partition
        the set of vertices into {V1 , V2 }. Let e be the edge of minimum weight
        connecting V1 and V2 . Then e belong to the minimum weight spanning
        tree.

            A rooted directed tree of G is a tree in which every edge is directed and
        every vertex has outdegree 1. A rooted star is a rooted directed tree in
        which every vertex is directly connected to the root. Figure 2.25(a) shows
        a directed rooted tree, and Fig. 2.25(b) shows a rooted star.
            The algorithm for ﬁnding a minimum spanning tree is given as Algo-
        rithm prammst. The algorithm proceeds through stages. In the beginning,
        there is a forest of trees consisting of all vertices and no edges. Each tree con-
        sists of exactly one vertex. Subsequently, during each stage, the edge with
        the minimum weight incident on each tree is selected. The newly selected
        edges are added to the current forest to yield a new forest. This continues
        until there is only one tree in the forest, that is, the minimum spanning
        tree.
May 7, 2022   11:14        Parallel Algorithms           9in x 6in         b4591-ch02     page 60




        60                                       Parallel Algorithms

                               (a)                                   (b)




                      Fig. 2.25.     (a) A directed rooted tree. (b) A rooted star.


          Algorithm 2.24 prammst
          Input: A graph G represented by its n × n weight matrix W .
          Output: A minimum spanning tree T of G.
           1. T ← {};    m = n.
           2. while m > 1 do
           3.    for all vertices v ∈ V (G) do
           4.        let C(v) = u, where W (v, u) = min{W (v, x) | x = v}.
           5.        T ← T ∪ {(u, v)}
           6.    end for
           7.    Shrink each directed tree of the forest deﬁned by C to a
                 rooted star. Set m ← Number of rooted stars.
           8.    Compress each rooted star to a supervertex. Assign the
                 labels (numbers) 1, 2, 3, . . . , m to these supervertices.
           9.    Let W  be the reduced m × m adjacency matrix of the graph
                 whose rows and columns correspond to the newly created
                 supervertices.
          10.    Set W ← W  . Let G be the corresponding graph.
          11. end while




            In the algorithm, the vector C deﬁned by the newly selected edges
        deﬁnes directed rooted trees. These rooted trees are converted to rooted
        stars. Every star is then compressed into a supervertex. In other words,
        replace each star by a new vertex. Label these new vertices as 1, 2, . . . , m,
        where m is the number of stars. Let W  be the reduced m × m adjacency
        matrix of the graph whose rows and columns correspond to the newly cre-
        ated supervertices. We store the edge (x, y) of the original graph next to the
        W  (i, j) entry, where (x, y) is the edge of minimum weight connecting the
        trees corresponding to supervertices i and j. This will enable us to recover
        an edge in the original graph quickly. It can be shown that the construction
        of the matrix W  from W takes O(log n) time using O(n2 ) processors on
May 7, 2022   11:14                      Parallel Algorithms                         9in x 6in                         b4591-ch02                   page 61




                                                     Shared-memory Computers (PRAM)                                                            61


        the CREW PRAM (Exercise 2.49). The foregoing procedure of compressing
        nodes, ﬁnding minimum-weight incident edges, and reducing the adjacency
        matrix is continued until there is only one tree spanning all the vertices of G.

        Example 2.15 Consider the graph shown in Fig. 2.26(a). During the ﬁrst
        iteration of the while loop, the adjacency vector C is given by C(1) = 2,
        C(2) = 1, C(3) = 1, C(4) = 8, C(5) = 7, C(6) = 3, C(7) = 5, and C(8) = 4,
        and the following edges are added to T : (1, 2), (1, 3), (4, 8), (5, 7), (3, 6).
        Hence, there are three rooted directed trees as shown in Fig. 2.26(b).
        By Step 7, the rooted trees are converted to rooted stars as shown in
        Fig. 2.26(c), and m is set to 3. The new matrix W with the newly
        assigned labels, and augmented with the minimum weight edges is then
        given by
                            ⎡                                ⎤
                                 ∞       5, (2, 4) 10, (4, 5)
                            ⎢                                ⎥
                            ⎣ 5, (2, 4)     ∞      12, (3, 7)⎦ .
                                                         10, (4, 5) 12, (3, 7)                             ∞
        The corresponding graph is shown in Fig. 2.26(d). The vertices in this
        graph were labeled as 1, 2, and 3. Thus, vertex 1 represents the set {4, 8},
        vertex 2 represents the set {1, 2, 3, 6}, and vertex 3 represents the set {5, 7}.

                       (a)                                                              (b)
                                             2               5                                    2
                                 1     4             2                                                                     4       5
                         7 3 4    13 10  1
                                                                                                  1
                        6 6 3 12 7 9 5 11 8                                                                                8       7
                                                                                                               3

                                                                                             6
                       (c)                                               (d)                        (e)
                                                                                 5,(2,4)                                       2   {1,2,3,6}
                                     1                   4       5       {4,8} 1         2 {1,2,3,6}
                                                                          10,(4,5)            12,(3,7)                         1
                        2        3               6       8       7                      3                                          {4,8}
                                                                                    {5,7}
                                                                                                                               3 {5,7}
                       (f)                                           (g)
                                 1 {4,8}                                        2                 5
                                                                           1             2                 4
                                                                                    4                 10           1
                             2           3                                  3
                                                                     6     6    3             7            5           8
                      {1,2,3,6} {5,7}                                                             9

                Fig. 2.26.                   Example of the construction of minimum spanning tree.
May 7, 2022   11:14                   Parallel Algorithms                9in x 6in                      b4591-ch02                    page 62




        62                                                   Parallel Algorithms


                  (a)                   8                  (b)                                         (c)
                                  8         1
                              4                   1              1   3         5               7             1       3     5      7
                          7                           2
                      6                                5
                                                                 2   4         6               8
                          6                           3                                                      2       4     6      8
                              3             4 2
                                  5
                                       7
                  (d)                                       (e)            (f)                         (g)
                  {7,8} 4                   1 {1,2}
                                  8,(1,8)                        1   3                 1       3                 1 {1,2,3,4}
                  6,(6,7)                       5,(2,3)                                                              7,(4,5)
                                                                 2   4                 2       4                 2    {5,6,7,8}
                  {5,6} 3                   2 {3,4}
                                  7,(4,5)
                                                                         (j)               8       1
                  (h)                       (i)                                    4                    1
                              1                        1                       7                            2
                                                                           6                                 5
                              2                        2                       6                            3
                                                                                   3               4 2
                                                                                           5
                                                                                               7

                Fig. 2.27.             Example of the construction of minimum spanning tree.

        Hence, during the second iteration, C(1) = 2, C(2) = 1 and C(3) = 1,
        and the following edges are added to T : (2, 4), (4, 5). Figure 2.26(e) shows
        the new rooted tree. Figure 2.26(f) shows the new star formed from the
        directed rooted tree in Fig. 2.26(e). Next, m is set to 1, and the while loop
        terminates. Figure 2.26(g) shows the resulting minimum spanning tree. 


        Example 2.16 Consider the graph shown in Fig. 2.27(a). During the
        ﬁrst iteration of the while loop, the adjacency vector C is given by
        C(1) = 2, C(2) = 1, C(3) = 4, C(4) = 3, C(5) = 6, C(6) = 5, C(7) = 8, and
        C(8) = 7, and the following edges are added to T : (1, 2), (3, 4), (5, 6), (7, 8).
        Hence, there are four rooted directed trees as shown in Fig. 2.27(b).
        By Step 7, the rooted trees are converted to rooted stars as shown in
        Fig. 2.27(c), and m is set to 4. The new matrix W with the newly assigned
        labels, and augmented with the minimum weight edges is then given by
                          ⎡                                        ⎤
                               ∞      5, (2, 3)    ∞      8, (1, 8)
                          ⎢                                        ⎥
                          ⎢5, (2, 3)     ∞      7, (4, 5)    ∞ ⎥
                          ⎢                                        ⎥.
                          ⎢ ∞                      ∞      6, (6, 7)⎥
                          ⎣           7, (4, 5)                    ⎦
                            8, (1, 8)    ∞      6, (6, 7)    ∞
May 7, 2022   11:14      Parallel Algorithms      9in x 6in       b4591-ch02                    page 63




                              Shared-memory Computers (PRAM)                              63


        The corresponding graph is shown in Fig. 2.27(d). The vertices in this
        graph were labeled as 1, 2, 3 and 4. Thus, vertex 1 represents the set
        {1, 2}, vertex 2 represents the set {3, 4}, vertex 3 represents the set {5, 6}
        and vertex 4 represents the set {7, 8}. Hence, during the second iteration,
        C(1) = 2, C(2) = 1, C(3) = 4 and C(4) = 3, and the edges (2, 3) and
        (6, 7) are added to T . Thus, there are two rooted directed trees as shown
        in Fig. 2.27(e). By Step 7, the rooted trees are converted to rooted stars
        as shown in Fig. 2.27(f), and m is set to 2. The new matrix W with the
        newly assigned labels, and augmented with the minimum weight edges is
        then given by
                                                        
                                        ∞       7, (4, 5)
                                                           .
                                     7, (4, 5)     ∞

        The corresponding graph is shown in Fig. 2.27(g). During the third itera-
        tion, the vector C is given by C(1) = 2 and C(2) = 1, and the edge (4, 5)
        is added to T . Figure 2.27(h) shows the new rooted tree. Figure 2.27(i)
        shows the new star formed from the directed rooted tree in Fig. 2.27(h).
        Next, m is set to 1, and the while loop terminates. Figure 2.27(j) shows
        the resulting minimum spanning tree.                                   

            The running time is computed as follows. Step 4 of computing C takes
        O(log m) = O(log n) time using O(m2 ) = O(n2 ) processors, since it com-
        putes m minima; one minimum per row. Step 7 of shrinking trees into stars
        takes O(log m) = O(log n) using O(n) processors by the technique of pointer
        jumping. As noted above, the construction of the m × m matrix in Step 9
        takes O(log n) time using O(n2 ) processors. Steps 7 and 9 require simulta-
        neous memory access, and hence the algorithm works on the CREW model.
        After each iteration of the while loop, the number of stars is reduced by at
        least a half, and hence there are at most log n iterations. It follows that the
        overall running time of the algorithm is O(log2 n) using a total of O(n2 )
        processors.


        2.20      Computing the Convex Hull of a Set of Points

        Let S = {p1 , p2 , . . . , pn } be a set of n points in the plane, where n is a power
        of 2. The convex hull of S, denoted by CH(S), is the smallest convex poly-
        gon containing all the points of S. The convex hull is usually represented
May 7, 2022   11:14         Parallel Algorithms           9in x 6in        b4591-ch02             page 64




        64                                        Parallel Algorithms


                      (a)



                                                                                        v
                       u




                      (b)


                                                      CH(S)                             v
                       u




                      (c)


                                                                      CH(S2)
                                   CH(S1)




        Fig. 2.28.    (a) The set of points S. (b) Convex hull of S. (c) Convex hulls of S1
        and S2 .


        by a list of points, called vertices, ordered clockwise (or counterclockwise).
        See Figs. 2.28(a) and (b) for an example, in which S consists of 32 points.
        In what follows, we present a divide-and-conquer parallel algorithm to ﬁnd
        CH(S) in Θ(log n) time using O(n) processors on the CREW PRAM.
            As a preprocessing step, the points in S are ﬁrst sorted in ascending
        order of their x-coordinates in Θ(log n) time using the pipelined merge-
        sort algorithm. So, assume that x(p1 ) ≤ x(p2 ) ≤ · · · ≤ x(pn ), where
        x(pi ) denotes the x-coordinate of point pi . We will assume for simplic-
        ity that no three points of S are collinear, and no two points have the
        same x-coordinate. Next, the set of points S is divided into two halves
        S1 = p1 , p2 , . . . , pn/2  and S2 = pn/2+1 , pn/2+2 , . . . , pn . Now, we recur-
        sively determine the two convex hulls of the two halves CH(S! ) and
May 7, 2022   11:14        Parallel Algorithms       9in x 6in        b4591-ch02                      page 65




                                Shared-memory Computers (PRAM)                                  65


        CH(S2 ). Figure 2.28(c) shows the two convex hulls of the points in part (a)
        of the ﬁgure.
            Consider the convex hull CH(S) shown in Fig. 2.28(b). Here, u and v
        are the two points with minimum and maximum x-coordinates, respectively
        (recall that no two points have the same x-coordinate). These two points
        are clearly part of CH(S). The polygonal chain deﬁned by the edges from
        u to v in clockwise traversal is called the upper hull U H(S). The lower
        hull, LH(S), is deﬁned similarly as the polygonal chain deﬁned by the
        edges from v to u in clockwise traversal. The algorithm, after determining
        CH(S1 ) and CH(S2 ), proceeds by constructing the upper and lower hulls
        of S. The upper hull of S, U H(S), is constructed by joining U H(S1 ) and
        U H(S2 ) by a line segment, called a tangent, such that CH(S1 ) and CH(S2 )
        are below it. The lower hull LH(S) is constructed in a similar manner to
        obtain the desired CH(S). In what follows, we compute the upper tangent
        and upper hull U H(S).
            Let x1 , x2 , . . . , xr  and y1 , y2 , . . . , ys  be the upper hulls U H(S1 ) and
        U H(S2 ) of S1 and S2 , respectively. We now show how to ﬁnd the line of
        the tangent x∗ y ∗ with the property that both of U H(S1 ) and U H(S2 ) are
        below it. That is, x∗ y ∗ is a tangent to both U H(S1 ) and U H(S2 ). The most
        crucial phase of the algorithm is the identiﬁcation of the upper and lower
        tangents. We outline the steps of the algorithm for determining x∗ y ∗ in the
        following two observations.


        Observation 2.2 If xi is a vertex of U H(S1 ), its tangent line xi vi with
                                                √
        U H(S2 ) can be found in Θ(1) time using s processors.

        Proof.      We ﬁnd the vertex vi in U H(S2 ) such that xi vi is a tangent of
        U H(S2 ) as follows. Let yj be any vertex in U H(S2 ), and let yj−1 and yj+1
        be the two vertices to the left and right of yj , respectively. If xi yj yj−1 is
        a right turn and xi yj yj+1 is a left turn, then vi is to the right of yj (see
        Fig. 2.29(a)). If xi yj yj−1 is a left turn and xi yj yj+1 is a right turn, then
        vi is to the left of yj (see Fig. 2.29(b)). If both xi yj yj−1 and xi yj yj+1 are
        right turns, then vi = yj (see Fig. 2.29(c)). Hence, we do parallel search on
                                                √
        the set of vertices of U H(S2 ) using s processors to identify the vertex yk
        such that vi = yk . There are log√s s = 2 iterations in this search, which
        implies that the running time is Θ(1).                                         
May 7, 2022   11:14      Parallel Algorithms             9in x 6in            b4591-ch02                 page 66




        66                                     Parallel Algorithms


                                                                                yj+1
                  (a)                                            yj
                                               xi
                             UH(S1)                                         UH(S2)
                                                       yj-1



                                                                                yj-1

                                               xi                                           yj
                  (b)

                             UH(S1)                                         UH(S2)                yj+1

                                                                               yj

                                               xi
                  (c)                                                                      yj+1
                                                                     yj-1
                             UH(S1)                                         UH(S2)




                                 Fig. 2.29.         Tangents to U H(S2 ).


        Observation 2.3 The common tangent x∗ y ∗ of U H(S1 ) and U H(S2 ) can
                                        √ √
        be determined in Θ(1) time using r s processors.

        Proof.       Let xi vi be a tangent to U H(S2 ) at vi determined as described
        in Observation 2.2, and let xi−1 and xi+1 be the two vertices to the left and
        right of xi , respectively. If xi vi is also a tangent to U H(S1 ), then x∗ = xi .
        If xi−1 xi vi is a left turn, then x∗ is to the left of xi (see Fig. 2.30(a)). If
        xi−1 xi vi is a right turn, then x∗ is to the right of xi (see Fig. 2.30(b)).
        This allows us to determine, for any given vertex xi of U H(S1 ), whether
        the vertex x∗ appears to the left of, to the right of, or equal to xi in Θ(1)
        time. Thus, to locate x∗ , we do double parallel search, the outer search is on
        the vertices of U H(S1 ), and for each vertex xi in U H(S1 ), we do the inner
        parallel search on the vertices of U H(S2 ). The parallel search performed on
        the set of vertices of U H(S2 ) is done as outlined in Observation 2.2 to obtain
        the tangent xi vi and next, we the test for the location of x∗ relative to xi as
                                     √
        stated above. We will use r processors for the outer search on the vertices
                                                                                      √
        of U H(S1 ), and so there are log√r r = 2 iterations in this search. We use s
        processors for the inner search on the vertices of U H(S2 ), which amounts
May 7, 2022   11:14                 Parallel Algorithms                    9in x 6in        b4591-ch02                    page 67




                                         Shared-memory Computers (PRAM)                                              67


                           (a)                                                                     vi
                                                  x*      xi-1
                                                                 xi


                                                                       xi+1



                                                  x*                                               vi
                      (b)         xi+1

                           xi


                      xi-1

                                            Fig. 2.30.           Tangents to U H(S1 ).


                                             xi                  UH(S)                  e     yj
                                 xi-1                  xi+1                      yj-1
                                                                                                         yj+1
                       x2                                                   y1
                                                                      xr                                        ys
                      x1

                                          Fig. 2.31.        Upper hull of S, U H(S).


        to two iterations for the inner search. Thus, the total number of processors
                √ √                    √
        used is r s ≤ n, that is, s processors for every vertex considered in
        U H(S1 ). It follows that the overall running time to ﬁnd the upper tangent
                       √ √
        is Θ(1) using r s ≤ n processors.                                         

            Observations 2.2 and 2.3 provide the steps for ﬁnding the upper common
        tangent x∗ y ∗ . The lower common tangent can be found in a similar fashion.
        It remains to ﬁnish the construction of CH(S). Let xi = x∗ and yj = y ∗ .
        To construct U H(S), ﬁrst, we remove the vertices xi+1 , xi+2 , . . . , xr from
        U H(S1 ) and remove the vertices y1 , y2 , . . . , yj−1 from U H(S2 ) to obtain
        U H  (S1 ) and U H  (S2 ), respectively. That is, U H  (S1 ) = x1 , x2 , . . . , xi 
        and U H  (S2 ) = yj , yj+1 , . . . , ys . Next, connecting xi in U H  (S1 ) to yj in
        U H  (S2 ) by the edge e = x∗ y ∗ = xi yj yields the desired upper hull U H(S)
        (see Fig. 2.31). Finally, the problem of computing LH(S) can be solved in
        a similar fashion.
May 7, 2022    11:14       Parallel Algorithms           9in x 6in     b4591-ch02                    page 68




        68                                       Parallel Algorithms

           The above discussion is summarized in Algorithm parconvexhull.
        The recurrence for the running time of the algorithm is T (n) = T (n/2) +
        Θ(1), which implies a running time of Θ(log n). Clearly, there are concurrent
        read operations, and hence the algorithm works on the CREW PRAM.

          Algorithm 2.25 parconvexhull
          Input: A set S = {p1 , . . . , pn } of n points in the plane, where n is a power of 2.
          Output: The convex hull of S, CH(S).
              1. Sort The points in S in nondecreasing order of their x-coordinates.
              2. CH(S) ← ch(S)
              3. return CH(S)

          Procedure ch(S)
           1. if |S| ≤ 4 then
           2.     compute CH(S) by a straightforward method.
           3.     return (CH(S))
           4. end if
           5. Divide S into two halves S1 = p1 , p2 , . . . , pn/2  and S2 =
              pn/2+1 , pn/2+2 , . . . , pn .
           6. CH(S1 ) ← ch(S1 );              CH(S2 ) ← ch(S2 )
           7. Let U H(S1 ) ← x1 , x2 , . . . , xr  and U H(S2 ) ← y1 , y2 , . . . , ys  be the
              upper hulls of S1 and S2 , respectively.
           8. Find the common upper tangent xi yj .
           9. U H  (S1 ) ← x1 , x2 , . . . , xi  and U H  (S2 ) ← yj , yj+1 , . . . , ys .
          10. U H(S) ← U H  (S1 ) ∪ U H  (S2 ) ∪ xi yj .
          11. Repeat Steps 7 to 10 to ﬁnd the lower hull of S, LH(S).
          12. CH(S) ← U H(S) ∪ LH(S)
          13. return CH(S)




        2.21       Bibliographic Notes

        There are a number of books on parallel algorithms on the PRAM. These
        include Akl [4], Akl [5], Akl [6], Akl and Lyons [8], Chaudhuri [21],
        Cosnard and Trystram [29], Gibbons and Rytter [37], Grama, Gupta,
        Karypis and Kumar [39], Horowitz, Sahni and Rajasekaran [43], JáJá [44],
        Lakshmivarahan and Dhall [53], Miller and Boxer [66], Roosta [77],
        and Xavier and Iyengar [104]. Preﬁx computations are described in
        Lakshmivarahan and Dhall [53], which is a book devoted to parallel preﬁx
        computations. The O(log log n) time algorithm for merging on the PRAM
May 7, 2022   11:14       Parallel Algorithms       9in x 6in       b4591-ch02                     page 69




                               Shared-memory Computers (PRAM)                                69


        is due to Kruscal [49]. The O(log log n) time algorithm for computing the
        maximum as well as algorithms for merging and sorting were given in
        Shiloach and Vishkin [87]. Bitonic and odd–even sorting networks were
        described in Batcher [15]. Multiselection on the PRAM is a modiﬁcation
        of an algorithm in Alsuwaiyel [11]. The pipelined mergesort algorithm is
        due to Cole [26]. A survey of parallel sorting and selection algorithms can
        be found in Rajasekaran [75]. The ideas for selection on the PRAM are
        from Akl( [7] and Vishkin [102]. The algorithm for the minimum spanning
        tree problem is due to Sollin, and was inspired by the one presented in
        JáJá [44]. Parallel algorithms for graph problems on the PRAM can be
        found in Gibbons and Rytter [37]. Parallel algorithms for problems in com-
        putational geometry on the PRAM can be found in Akl and Lyons [8]. The
        divide-and-conquer approach for computing the planar convex hull is due
        to Shamos [82]. For more references on parallel algorithms on the PRAM,
        see for instance JáJá [44].




        2.22      Exercises

         2.1. Give a parallel algorithm to compute the maximum of n numbers
              in the sequence x1 , x2 , . . . , xn  on the EREW PRAM. What is the
              running time of your algorithm?

         2.2. Consider Algorithm sortingcrew presented in Section 2.4.1. Sup-
              pose we change the outer loop in Line 3 to sequential and change the
              inner loop in Line 4 to parallel, will the algorithm still work on the
              CREW PRAM? Explain.

         2.3. Use parallel preﬁx to compute the sequence of maximums
              x1 , max{x1 , x2 }, max{x1 , x2 , x3 }, . . . , max{x1 , x2 , . . . , xn } for the
              sequence S = x1 , x2 , . . . , xn .

         2.4. Let S = x1 , x2 , . . . , xn  be a sequence of integers. Give an algorithm
              to rearrange the elements of S so that all negative integers precede
              all positive integers. For example, if S = 3, −2, 1, −5, 4, −6, 7, the
              result should be −2, −5, −6, 3, 1, 4, 7.
May 7, 2022   11:14      Parallel Algorithms           9in x 6in     b4591-ch02        page 70




        70                                     Parallel Algorithms

         2.5. Give an algorithm to broadcast an item x stored in processor P0 to
              all other processors in the EREW PRAM with n = 2k processors.
              What is the running time of your algorithm?

         2.6. Consider Algorithm parquicksort in Section 2.5.2 for parallel
              quicksort. What is the cost of the algorithm on average? How about
              in the worst case?

         2.7. What is the number of parallel steps in Algorithm parsearch for
              parallel search discussed in Section 2.6?

         2.8. Apply Algorithm parsearch for parallel search using two processors
              on the sequence

                             S = 1, 2, 5, 7, 8, 11, 12, 15, 19 and x = 8.

                 How many parallel steps are there?

         2.9. Illustrate the operation of Algorithm parrank in Section 2.9.1 for
              computing the ranks of B in A on the input:

                       A = 1, 4, 7, 10, 12, 14, 19, 20 and B = 5, 11, 15, 18.

        2.10. Illustrate the operation of Algorithm oddevenmerge in Section 2.11
              for odd–even merging on the input:

                                 A = 2, 5, 6, 8 and B = 1, 3, 7, 9.

        2.11. Do Exercise 2.10 with the following modiﬁcation. Merge Aodd with
              Bodd and Aeven with Beven . (See Exercise 2.46).

        2.12. Let A, B, C, D and E be as deﬁned in Algorithm oddevenmerge
              discussed in Section 2.11, and assume the elements in A ∪ B are
              distinct. Given a sequence X and an element x, recall that rank(x, X)
              is the number of elements in X less than x. Express rank(x, C) and
              rank(x, D) in terms of rank(x, A) and rank(x, B).

        2.13. Use the result of Exercise 2.12 to show that for c ∈ C, either c is in
              its correct position in E or to the left of it.
May 7, 2022   11:14     Parallel Algorithms    9in x 6in      b4591-ch02                  page 71




                             Shared-memory Computers (PRAM)                         71

        2.14. Use the result of Exercise 2.12 to show that for d ∈ D, either d is in
              its correct position in E or to the right of it.

        2.15. Illustrate the operation of the bitonic sort network shown in Fig. 2.21
              on the input sequence 6, 7, 1, 4, 2, 5, 8, 3.

        2.16. Give an example of a bitonic sequence with one local maximum and
              one local minimum.

        2.17. In Algorithm parselect for selection discussed in Section 2.14, show
              that in each iteration, the median of medians m is greater than and
              smaller than at most 3|A|/4 elements.

        2.18. Consider Algorithm parmultiselect1 for multiselection discussed
              in Section 2.15. Compare the algorithm given with direct application
              of Algorithm parselect given in Section 2.14.

        2.19. Repeat Exercise 2.18 with the second algorithm for multiselection for
              the PRAM, Algorithm parmultiselect2.

        2.20. Suggest an algorithm for sorting using multisession. What is the time
              complexity of your algorithm?

        2.21. Consider the algorithm for matrix multiplication discussed in Sec-
              tion 2.16. What is the cost of the algorithm? What modiﬁcation
              should be done in order to make the total cost O(n3 )?

        2.22. Let P be a simple polygon (that is not necessarily convex) with n
              vertices, and let x be a point. Assume that there are n processors,
              each assigned to one edge. Give an eﬃcient parallel algorithm to
              decide whether x is in the interior of P . (Hint : Draw a horizontal
              line L such that x lies on L. Count how many times L intersects
              with the edges of P ).

        2.23. Let x1 , x2 , . . . , xn be n Boolean variables. Show how to ﬁnd the log-
              ical OR of these variables in O(1) time on the COMMON CRCW
              PRAM with n processors.
May 7, 2022   11:14        Parallel Algorithms           9in x 6in     b4591-ch02            page 72




        72                                       Parallel Algorithms


        2.24. Let x1 , x2 , . . . , xn  be a sequence of n distinct numbers. Show how
              to ﬁnd the maximum of these numbers in O(1) time on the CRCW
              PRAM with n2 processors.

        2.25. Let x1 , x2 , . . . , xn  be a sequence of n distinct numbers. Show how
              to ﬁnd the maximum of these numbers in O(log log n) time on the
                                                                                     √
              CRCW PRAM with n processors. Hint : Partition the input into n
              parts and recursively ﬁnd the maximum in each part. Use Exer-
              cise 2.24.

        2.26. Let S be a sequence of n distinct numbers and x ∈ S. The rank
              of x in S is the number of elements in S less than x. Show how to
              compute the rank of x in S in O(log n) time on the CREW PRAM
              with n processors.

        2.27. Let S be a sequence of n integers, and x an integer. Show how to
              compute rank(x, S) and the rank of x in S, in O(log n) time on the
              EREW PRAM using O(n) operations.

        2.28. Let S = {x1 , x2 , . . . , xn } be n numbers and k an integer, 1 ≤ k ≤ n.
              Show how to ﬁnd the kth smallest element in S in O(log n) time on
              the CREW PRAM with n2 processors.

        2.29. Let S = x1 , x2 , . . . , xn  be a sequence of n numbers. Consider
              the simple recursive algorithm for parallel preﬁx that divides the
              sequence S into two halves: S1 = x1 , x2 , . . . , xn/2  and S2 =
              xn/2+1 , xn/2+2 , . . . , xn , and then calls the algorithm recursively on
              each of S1 and S2 .
                  (a)   Write down the detailed algorithm.
                  (b)   Will the algorithm work on the EREW PRAM?
                  (c)   What is the total work done by the algorithm?
                  (d)   Will Brent’s Theorem (Theorem 2.1) help in reducing the
                        number of processors without increasing the running time
                        complexity?

        2.30. Let x1 , x2 , . . . , xn  be a sequence of n numbers. The preﬁx minima
              is to compute for each i, 1 ≤ i ≤ n, the minimum among the elements
              {x1 , x2 , . . . , xi }. Develop an algorithm to compute the preﬁx minima
              that runs in time O(log n) on the EREW PRAM.
May 7, 2022   11:14     Parallel Algorithms             9in x 6in           b4591-ch02        page 73




                             Shared-memory Computers (PRAM)                              73

        2.31. Do Exercise 2.30 using suﬃx minima instead, that is, compute
              for each i, 1 ≤ i ≤ n, the minimum among the elements
              {xi , xi+1 , . . . , xn }.

        2.32. Let x1 , x2 , . . . , xn  be a sequence of n numbers. The suﬃx compu-
              tation problem is to compute the suﬃxes xn , xn−1 ◦ xn , . . . , x1 ◦ x2 ◦
              · · · ◦ xn . Give an O(log n) time algorithm to solve this problem on
              the CREW PRAM with n processors.

        2.33. Do Exercise 2.32 for the case of EREW PRAM.

        2.34. Let T1 , T2 , . . . , Tm be m directed and rooted binary trees on n ver-
              tices. Each node has a pointer to its parent, except the root which
              points to itself. Design a parallel algorithm to allow each vertex to
              know the identity of the tree to which it belongs (The trees are iden-
              tiﬁed by their roots. The roots are numbered 1, 2, . . . , m).

        2.35. Compute the next and succ functions as describe in Table 2.1 (page
              25) for all vertices in the tree shown in Fig. 2.32. Use the obtained
              values to derive an Euler tour.

        2.36. Use the Euler tour technique to direct the tree shown in Fig. 2.32,
              where vertex 1 is to be set as the root.

        2.37. Use the Euler tour technique to assign levels to the vertices in the
              tree shown in Fig. 2.32.

        2.38. In a postorder traversal of a tree T at the root r, the subtrees of r
              are traversed from left to right in postorder followed by r. Develop
              an algorithm to determine the postorder numbering of the vertices
              in a rooted tree. What is the time complexity of your algorithm?



                                              2                     7

                                       4          1         6           5

                                              3                 8

                                           Fig. 2.32.     A tree.
May 7, 2022   11:14     Parallel Algorithms               9in x 6in       b4591-ch02      page 74




        74                                    Parallel Algorithms


                                                  2                   7

                                              4       1          6

                                       5          3                   8

                                     Fig. 2.33.       A rooted tree.

        2.39. Apply the algorithm developed in Exercise 2.38 on the tree shown in
              Fig. 2.33.

        2.40. Parallelize Horner’s rule to evaluate a polynomial of degree n under
              the EREW PRAM in time O(log n).

        2.41. Let x1 , x2 , . . . , xn  be a sequence of n distinct numbers. Design a
              parallel algorithm for the CREW PRAM to sort this sequence in
              time O(log n). Assume an unlimited number of processors.

        2.42. Let n be a positive integer. Consider the problem of computing the
              polynomials xi = xi , for 1 ≤ i ≤ n. Show how to compute the xi ’s in
              O(log n) time. Specify the PRAM model used.

        2.43. Consider Algorithm parquicksort presented in Section 2.5.2. Sup-
              pose we always select the median as the pivot (see Section 2.14).
              What will be the running time of the algorithm?

        2.44. Let A and B be two sequences of distinct number sorted in ascend-
              ing order, where |A| = |B| = n. Design an O(1) time algorithm to
              merge A and B on the CREW PRAM. Assume an unlimited number
              of processors.

        2.45. Apply Brent’s theorem on Algorithm parmerge presented in Sec-
              tion 2.9.2.

        2.46. In Algorithm oddevenmerge in Section 2.11, Aeven is merged with
              Bodd and Aodd is merged with Beven . Rewrite the algorithm with the
              modiﬁcation so that it merges Aodd with Bodd and Aeven with Beven .
              It is important to know that this will change the step of traversing
              the shuﬄe of C and D.

        2.47. Let G = (V, E) be an undirected graph with n vertices. Give an algo-
              rithm to decide whether G contains a triangle, that is, three mutually
May 7, 2022   11:14      Parallel Algorithms                            9in x 6in                 b4591-ch02        page 75




                              Shared-memory Computers (PRAM)                                                   75

                 adjacent vertices. Assume that G is represented by its adjacency
                 matrix. Your algorithm should run in O(log n) time on the CRCW
                 PRAM with n3 processors.

        2.48. Prove Theorem 2.6.

        2.49. Show that the reduced adjacency matrix in the minimum spanning
              tree algorithm of Section 2.18 can be constructed in time O(log n)
              using O(n2 ) processors on the CREW PRAM.

        2.50. Show the steps of computing a minimum spanning tree on the graph
              shown in Fig. 2.34.

        2.51. Show the steps of computing a minimum spanning tree on the graph
              shown in Fig. 2.35.

        2.52. Let G = (V, E) be an undirected graph. G is bipartite if and only
              if V can be partitioned into two parts V1 and V2 , such that every
              edge connects a vertex in V1 with a vertex in V2 . Equivalently, G is
              bipartite if and only if it contains no odd-length cycles. Develop an
              algorithm to test whether G is bipartite.

        2.53. Illustrate the operation of the bitonic sort network shown in Fig. 2.21
              on the input 6, 7, 1, 4, 2, 5, 8, 3.


                                                       1        7           2        5        4
                                           2                                4        8       1
                                                       3
                                          6        6            3               7             5
                                                                        12               9
                                                            14                  13
                                                       11                            10
                                                                        8

                                 Fig. 2.34.                An undirected graph.

                                                                    13
                                                            8                1
                                                       4                       9
                                                                    8       1
                                                   7                                     2
                                                           12
                                               3                                          7
                                                   6        5                            3
                                                   11 5                      4 10
                                                                        2

                                 Fig. 2.35.                An undirected graph.
May 7, 2022    11:14        Parallel Algorithms           9in x 6in     b4591-ch02         page 76




        76                                        Parallel Algorithms


        2.54. Let A, A , C and C  be sorted sequences such that C is a 3-cover
              for A and C  is a 3-cover for A . Is C ∪ C  necessarily a 3-cover for
              A ∪ A ? See Section 2.13 for the deﬁnition of 3-cover.

        2.55. Illustrate the operation of the pipelined mergesort algorithm on the
              input 6, 7, 1, 4, 2, 5, 8, 3.

        2.56. Prove Observation 2.1.

        2.57. Let W, X and Y be three sorted sequences such that Y = W ∪X, and
              W ∩X = φ. Assume that R(S, S) is known for any sequence S, where
              R(A, B) is the cross ranks of A in B as deﬁned in Section 2.13. Show
              how to compute R(W, X) and R(X, W ) in O(1) time using O(|Y |)
              processors.

        2.58. Parallelize the Θ(n) time sequential algorithm for selection using
              n/ log n processors on the PRAM. Analyze your algorithm.


          Algorithm 2.26 select
          Input: An array A[1..n] of n elements and an integer k, 1 ≤ k ≤ n.
          Output: The kth smallest element in A.
              1. select(A, k)

          Procedure select(A, k)
           1. n ← |A|
           2. if n < 44 then sort A and return (A[k])
           3. Let q = n/5 . Divide A into q groups of 5 elements each. If 5 does not
              divide p, then discard the remaining elements.
           4. Sort each of the q groups individually and extract its median. Let the set
              of medians be M .
           5. mm ← select(M, q/2)       {mm is the median of medians}
           6. Partition A into three arrays:
              A1 = {a | a < mm}
              A2 = {a | a = mm}
              A3 = {a | a > mm}
           7. case
              |A1 | ≥ k: return select(A1 , k)
              |A1 | + |A2 | ≥ k: return mm
              |A1 | + |A2 | < k: return select(A3 , k − |A1 | − |A2 |)
           8. end case
May 7, 2022   11:14       Parallel Algorithms       9in x 6in       b4591-ch02                     page 77




                               Shared-memory Computers (PRAM)                                77


        2.59. Let A = a1 , a2 , . . . , an  be a sequence of numbers and let k be a given
              integer between 1 and n. Design and analyze a parallel algorithm to
              ﬁnd all k smallest items in A. Do not use multiselection. What model
              of computation did you use?



        2.23      Solutions

         2.1. Give a parallel algorithm to compute the maximum of n numbers
              in the sequence x1 , x2 , . . . , xn  on the EREW PRAM. What is the
              running time of your algorithm?

                 Similar to Algorithm paraddition for parallel addition discussed in
                 Section 2.2.

         2.2. Consider Algorithm sortingcrew presented in Section 2.4.1. Sup-
              pose we change the outer loop in Line 3 to sequential and change the
              inner loop in Line 4 to parallel, will the algorithm still work on the
              CREW PRAM? Explain.

                 No, since there will be concurrent writes. For instance, comparing
                 A[1] with A[2] and comparing A[1] with A[3] will take place simul-
                 taneously, and hence the statement r[1]← r[1] + 1 may be executed
                 at least twice at the same time.

         2.3. Use parallel preﬁx to compute the sequence of maximums
              x1 , max{x1 , x2 }, max{x1 , x2 , x3 }, . . . , max{x1 , x2 , . . . , xn } for the
              sequence S = x1 , x2 , . . . , xn .

                 Similar to Algorithm parprefix for parallel preﬁx discussed in Sec-
                 tion 2.5.

         2.4. Let S = x1 , x2 , . . . , xn  be a sequence of integers. Give an algorithm
              to rearrange the elements of S so that all negative integers precede
              all positive integers. For example, if S = 3, −2, 1, −5, 4, −6, 7, the
              result should be −2, −5, −6, 3, 1, 4, 7.

                 Use array packing; — similar to Example 2.2.
May 7, 2022   11:14       Parallel Algorithms           9in x 6in         b4591-ch02    page 78




        78                                      Parallel Algorithms

         2.5. Give an algorithm to broadcast an item x stored in processor P0 to
              all other processors in the EREW PRAM with n = 2k processors.
              What is the running time of your algorithm?
                 First, P0 writes x to global memory, and P1 reads x. P0 and P1 then
                 broadcast x to P2 and P3 simultaneously. P0 , P1 , P2 and P3 then
                 broadcast x to P4 , P5 , P6 and P7 , and so on. The running time is
                 Θ(log n).

         2.6. Consider Algorithm parquicksort in Section 2.5.2 for parallel
              quicksort. What is the cost of the algorithm on average? How about
              in the worst case?
                 The cost of the algorithm on average is Θ(n log2 n), and Θ(n2 log n)
                 in the worst case.

         2.7. What is the number of parallel steps in Algorithm parsearch for
              parallel search discussed in Section 2.6?
                 The number of parallel steps is at most logp+1 n + 1.

         2.8. Apply Algorithm parsearch for parallel search using two processors
              on the sequence
                              S = 1, 2, 5, 7, 8, 11, 12, 15, 19 and x = 8.
                 How many parallel steps are there?
                 Initially, the algorithm divides S into three subsequences
                                     1, 2, 5,      7, 8, 11,    12, 15, 19.
                 The two processors compare x with elements at the internal bound-
                 aries, that is, 5 and 11. Since 8 > 5 and 8 < 11, the search area is
                 reduced to 7, 8. Finally, the two processors perform two compar-
                 isons simultaneously and one of them returns 3 + 2 = 5 . The number
                 of parallel steps is 2.

         2.9. Illustrate the operation of Algorithm parrank in Section 2.9.1 for
              computing the ranks of B in A on the input:
                       A = 1, 4, 7, 10, 12, 14, 19, 20 and B = 5, 11, 15, 18.

                 Similar to Examples 2.9 and 2.10.
May 7, 2022   11:14       Parallel Algorithms            9in x 6in               b4591-ch02               page 79




                               Shared-memory Computers (PRAM)                                       79

        2.10. Illustrate the operation of Algorithm oddevenmerge in Section 2.11
              for odd–even merging on the input:

                                  A = 2, 5, 6, 8 and B = 1, 3, 7, 9.

                 Similar to Example 2.11.

        2.11. Do Exercise 2.10 with the following modiﬁcation. Merge Aodd with
              Bodd and Aeven with Beven . (See Exercise 2.46).
                 Similar to Exercise 2.10.

        2.12. Let A, B, C, D and E be as deﬁned in Algorithm oddevenmerge
              discussed in Section 2.11, and assume the elements in A ∪ B are
              distinct. Given a sequence X and an element x, recall that rank(x, X)
              is the number of elements in X less than x. Express rank(x, C) and
              rank(x, D) in terms of rank(x, A) and rank(x, B).
                 Let x ∈ A ∪ B. Then,
                                                                 
                                          rank(x, A)     rank(x, B)
                             rank(x, C) =             +               ,
                                              2              2
                 and
                                                                 
                                          rank(x, A)     rank(x, B)
                             rank(x, D) =             +              .
                                              2              2

        2.13. Use the result of Exercise 2.12 to show that for c ∈ C, either c is in
              its correct position in E or to the left of it.
                 For x ∈ X, let pos(x, X) be the position of x in the sequence X, where
                 pos(x, X) ≥ 0. For c ∈ C, let r1 = rank(c, A) and r2 = rank(c, B),
                 and rc = r1 + r2 . Either c ∈ A or c ∈ B. If c ∈ A, then r1 is even
                 since pos(c, A) is even, and it follows that the position of c in E is
                                                                      
                               pos(c, E) = 2 rank(c, C) = 2 r21 + 2 r22
                                                ≤ r1 + (r2 ) since r1 is even
                                                = rc .
                                                           r1          r2 
                 Since rc − 1 = r1 + (r2 − 1) ≤ 2           2      +2     2      = pos(c, E), we have

                                            rc − 1 ≤ pos(c, E) ≤ rc .                             (2.5)
May 7, 2022   11:14        Parallel Algorithms           9in x 6in            b4591-ch02                   page 80




        80                                       Parallel Algorithms


                 Thus, either pos(c, E) = rc − 1 or pos(c, E) = rc . That is, either c is
                 in its correct position in E or to the left of it.
                 On the other hand, if c ∈ B, then r2 is odd since pos(c, B) is odd,
                 and we get the same inequalities.

        2.14. Use the result of Exercise 2.12 to show that for d ∈ D, either d is in
              its correct position in E or to the right of it.
                 For x ∈ X, let pos(x, X) be the position of x in the sequence X,
                 where pos(x, X) ≥ 0. For d ∈ D, let r3 = rank(d, A), r4 = rank(d, B)
                 and rd = r3 + r4 . If d ∈ A then r3 is odd since pos(d, A) is odd. It
                 follows that if d ∈ A, then the position of d in E is
                                                                    
                           pos(d, E) = 2 rank(d, D) + 1 = 2 r23 + 2 r24 + 1
                                        ≤ (r3 − 1) + (r4 + 1) + 1 since r3 is odd
                                        = rd + 1.
                                                               r3          r4 
                 Since rd = (r3 − 1) + (r4 ) + 1 ≤ 2            2      +2     2      + 1 = pos(d, E), we
                 have
                                             rd ≤ pos(d, E) ≤ rd + 1.                              (2.6)
                 Thus, either pos(d, E) = rd or pos(d, E) = rd + 1. That is, either d
                 is in its correct position in E or to the right of it.
                 If d ∈ B, then r4 is even, and we get the same inequalities.

        2.15. Illustrate the operation of the bitonic sort network shown in Fig. 2.21
              on the input sequence 6, 7, 1, 4, 2, 5, 8, 3.
                 Similar to Fig. 2.21.

        2.16. Give an example of a bitonic sequence with one local maximum and
              one local minimum.
                 The sequence 2, 1, 4, 3 is such an example.

        2.17. In Algorithm parselect for selection discussed in Section 2.14, show
              that in each iteration, the median of medians m is greater than and
              smaller than at most 3|A|/4 elements.
                 Let r = |A|/ log |A| be the number of groups, and s = log |A| be
                 the size of each group. Let the groups be g1 , g2 , . . . , gr with medians
May 7, 2022   11:14        Parallel Algorithms            9in x 6in      b4591-ch02               page 81




                                Shared-memory Computers (PRAM)                              81

                 m1 , m2 , . . . mr , where mi ≤ mi+1 , 1 ≤ i ≤ r − 1. Then, the median of
                 medians m ≥ mi for 1 ≤ i ≤ r/2. Hence, m is greater than or equal
                 to at least 2s elements in groups g1 , g2 , . . . , gr/2 . Thus, m ≥ at least
                              |A|                                                3|A|
                 2 × 2 = 4 elements. It follows that m ≤ at most
                 r    s
                                                                                   4  elements.
                                             3|A|
                 Similarly, m ≥ at most 4 elements.

        2.18. Consider Algorithm parmultiselect1 for multiselection discussed
              in Section 2.15. Compare the algorithm given with direct application
              of Algorithm parselect given in Section 2.14.

                 Direct application of Algorithm parselect r times takes

                                r × O(log n log log n) = O(r log n log log n),

                 using n/ log n processors. On the other hand, Algorithm parmulti-
                 select1 takes

                                                 O(log r log n log log n).

                 which is less than direct application for any r that is asymmetrically
                 more than constant.

        2.19. Repeat Exercise 2.18 with the second algorithm for multiselection for
              the PRAM, Algorithm parmultiselect2.

                 Direct application of Algorithm parselect r times takes

                                r × O(log n log log n) = O(r log n log log n),

                 using n/ log n processors. On the other hand, Algorithm parmulti-
                 select2 takes

                                           O(log n(log r + log log n)).

                 which is less than direct application for any r that is asymmetrically
                 more than constant.

        2.20. Suggest an algorithm for sorting using multisession. What is the time
              complexity of your algorithm?
May 7, 2022   11:14       Parallel Algorithms           9in x 6in     b4591-ch02           page 82




        82                                      Parallel Algorithms


                 Use Algorithm parmultiselect2 on the PRAM with n/ log n pro-
                 cessors. Setting r = n, its running time becomes

                                 O(log n(log r + log log n)) = O(log2 n),

                 which is cost optimal.

        2.21. Consider the algorithm for matrix multiplication discussed in Sec-
              tion 2.16. What is the cost of the algorithm? What modiﬁcation
              should be done in order to make the total cost O(n3 ).

                 The cost is Θ(n3 log n). To make the total cost O(n3 ), reduce the
                 number of processors to O(n3 / log n).

        2.22. Let P be a simple polygon (that is not necessarily convex) with n
              vertices, and let x be a point. Assume that there are n processors,
              each assigned to one edge. Give an eﬃcient parallel algorithm to
              decide whether x is in the interior of P . (Hint: Draw a horizontal
              line L such that x lies on L. Count how many times L intersects
              with the edges of P ).

                 As suggested by the hint. Assign one edge of the polygon to each
                 processor. Each processor stores a 1 if its assigned edge intersects
                 the line L and 0 otherwise. Finally, perform the sum of these stored
                 values and test whether it is even or odd. The total time is Θ(log n).

        2.23. Let x1 , x2 , . . . , xn be n Boolean variables. Show how to ﬁnd the log-
              ical OR of these variables in O(1) time on the COMMON CRCW
              PRAM with n processors.

                 Let y hold the output. Initially, set y = 0. Each processor Pi executes
                 the command: if xi = 1 then y = 1. Then all processors Pj with
                 xj = 1 will write the same value. Hence, the output is y = 1 using
                 the COMMON PRAM if and only if at least one xi is 1.

        2.24. Let x1 , x2 , . . . , xn  be a sequence of n distinct numbers. Show how
              to ﬁnd the maximum of these numbers in O(1) time on the CRCW
              PRAM with n2 processors.
May 7, 2022   11:14         Parallel Algorithms         9in x 6in        b4591-ch02                       page 83




                                 Shared-memory Computers (PRAM)                                     83


                 Label the n2 processors as Pi,j , 1 ≤ i, j ≤ n. Let processors
                 Pi,1 , Pi,2 , . . . , Pi,n deﬁne group i, 1 ≤ i ≤ n. Then, group i will com-
                 pute yi , which is the OR of xi,1 , xi,2 , . . . , xi,n , where xi,j = (xi < xj ),
                 as shown in the solution of Exercise 2.23. Clearly, yi is 0 if and only
                 if xi is the maximum. Each processor Pi executes the command:
                 if yi = 0 then output xi . Only one processor will succeed and out-
                 put its element. The reason concurrent writes are needed is the com-
                 putation of the OR’s.

        2.25. Let x1 , x2 , . . . , xn  be a sequence of n distinct numbers. Show how
              to ﬁnd the maximum of these numbers in O(log log n) time on the
                                                                                     √
              CRCW PRAM with n processors. Hint: Partition the input into n
              parts and recursively ﬁnd the maximum in each part. Use Exer-
              cise 2.24.

                                               √
                 Partition the input into n parts and recursively ﬁnd the maximum
                                                            √
                 in each part. Each part is assigned n processors to ﬁnd the maxi-
                 mum recursively (number of elements equals number of processors).
                 Let the maximums be x1 , x2 , . . . , x√n . Use Exercise 2.24 to ﬁnd the
                 maximum of x1 , x2 , . . . , x√n using n processors in O(1) time. The
                                                                          √
                 running time is given by the recurrence T (n) = T ( n) + O(1) whose
                 solution is T (n) = O(log log n).

        2.26. Let S be a sequence of n distinct numbers and x ∈ S. The rank
              of x in S is the number of elements in S less than x. Show how to
              compute the rank of x in S in O(log n) time on the CREW PRAM
              with n processors.

                 Let S = a1 , a2 , . . . , an . Compute A[i] = (ai < x) for 1 ≤ i ≤ n.
                 Let r be the sum of 1’s in array A. Output r: r can be found by
                 addition or parallel preﬁx in O(log n) time. The reason concurrent
                 reads are required is so that all processors read x at the same time.

        2.27. Let S be a sequence of n integers, and x an integer. Show how to
              compute rank(x, S), the rank of x in S, in O(log n) time on the
              EREW PRAM using O(n) operations.
May 7, 2022   11:14        Parallel Algorithms           9in x 6in     b4591-ch02                page 84




        84                                       Parallel Algorithms

                 To adapt the solution of Exercise 2.26 to the EREW PRAM, ﬁrst
                 broadcast x to all processors, say B[i] = x for 1 ≤ i ≤ n, then
                 compute (A[i] < B[i]) for 1 ≤ i ≤ n. To broadcast x, ﬁrst P1 copies
                 B[1] = x to B[2]. Next, P1 and P2 copy B[1] and B[2] to B[3] and
                 B[4], respectively. Next, P1 , P2 , P3 and P4 copy B[1], B[2], B[3] and
                 B[4] to B[5], B[6], B[7] and B[8], respectively, and so on. The number
                 of writes is equal to 1 + 2 + 4 + · · · + 2k = 2n − 1, where k = log n.
                 The number of comparisons (A[i] < B[i]) is n, which is equal to
                 the number of assignments. Hence, the total number of operations
                 is Θ(n).

        2.28. Let S = {x1 , x2 , . . . , xn } be n numbers and k an integer, 1 ≤ k ≤ n.
              Show how to ﬁnd the kth smallest element in S in O(log n) time on
              the CREW PRAM with n2 processors.
                 Assume the xi ’s are distinct. Label the n2 processors as Pi,j , 1 ≤
                 i, j ≤ n. Let processors Pi,1 , Pi,2 , . . . , Pi,n deﬁne group i, 1 ≤ i ≤ n.
                 For i = 1, 2, . . . , n, we use Exercise 2.26 to ﬁnd the rank of xi in
                 group i, and store it in B[i], 1 ≤ i ≤ n. Now, for i = 1, 2, . . . , n,
                 processor Pi,1 outputs xi if its rank B[i] is equal to k − 1. Note that
                 exactly one processor will output the kth smallest element, so there
                 are no concurrent writes. The running time is O(log n) and the fact
                 that it runs on the CREW PRAM follows from Exercise 2.26.

        2.29. Let S = x1 , x2 , . . . , xn  be a sequence of n numbers. Consider
              the simple recursive algorithm for parallel preﬁx that divides the
              sequence S into two halves: S1 = x1 , x2 , . . . , xn/2  and S2 =
              xn/2+1 , xn/2+2 , . . . , xn , and then calls the algorithm recursively on
              each of S1 and S2 .
                  (a)   Write down the detailed algorithm.
                  (b)   Will the algorithm work on the EREW PRAM?
                  (c)   What is the total work done by the algorithm?
                  (d)   Will Brent’s Theorem (Theorem 2.1) help in reducing the
                        number of processors without increasing the running time
                        complexity?

                  (a) The algorithm is shown as Algorithm parprefix2.
                  (b) The algorithm will not work on the EREW PRAM. There are
                      concurrent reads of sn/2 .
May 7, 2022   11:14       Parallel Algorithms       9in x 6in         b4591-ch02              page 85




                               Shared-memory Computers (PRAM)                            85


          Algorithm 2.27 parprefix2
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers, where n = 2k .
          Output: S = s1 , s2 , . . . , sn , the preﬁx sums of X.
           1. if n = 1 then return x1
           2. else do
           3.     X1 = x1 , x2 , . . . , xn/2 
           4.     X2 = xn/2+1 xn/2+2 , . . . , xn 
           5.     S1 ← parprefix2(X1 )
           6.     S2 ← parprefix2(X2 )
           7.     for j ← (n/2 + 1) to n do in parallel
           8.         sj ← sj + sn/2
           9.     end for
          10. return S1 ∪ S2



                  (c) The total number of operations (additions) done by the algo-
                      rithm is given by the recurrence W (n) = 2W (n/2) + n/2, whose
                      solution is W (n) = Θ(n log n), which is the total work per-
                      formed by the algorithm.
                  (d) Brent’s Theorem does not help in reducing the number of pro-
                      cessors, since the total number of operations is Θ(n log n).

        2.30. Let x1 , x2 , . . . , xn  be a sequence of n numbers. The preﬁx minima
              is to compute for each i, 1 ≤ i ≤ n, the minimum among the elements
              {x1 , x2 , . . . , xi }. Develop an algorithm to compute the preﬁx minima
              that runs in time O(log n) on the EREW PRAM.
                 This is the parallel preﬁx problem using the associative binary oper-
                 ation MIN.

        2.31. Do Exercise 2.30 using suﬃx minima instead, that is, compute
              for each i, 1 ≤ i ≤ n, the minimum among the elements
              {xi , xi+1 , . . . , xn }.
                 The algorithm is similar to Algorithm parprefixrec. It is shown as
                 Algorithm psminima.

        2.32. Let x1 , x2 , . . . , xn  be a sequence of n numbers. The suﬃx compu-
              tation problem is to compute the suﬃxes xn , xn−1 ◦ xn , . . . , x1 ◦ x2 ◦
              · · · ◦ xn . Give an O(log n) time algorithm to solve this problem on
              the CREW PRAM with n processors.
May 7, 2022   11:14         Parallel Algorithms           9in x 6in       b4591-ch02                  page 86




        86                                        Parallel Algorithms


          Algorithm 2.28 psminima
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers, where n = 2k .
          Output: S = s1 , s2 , . . . , sn , where si = min{xi , xi+1 , . . . , xn } are the suﬃx
                    minima.
           1. sn ← xn
           2. if n = 1 then return S = xn 
           3. for i ← 1 to n/2 do in parallel
           4.     x2i−1 ← min{x2i−1 , x2i }
           5. end for
           6. Recursively compute the preﬁx minima of x1 , x3 , . . . , xn−1  and store
              them in s1 , s3 , . . . , sn−1 
           7. for i ← 1 to (n/2) − 1 do in parallel
           8.     s2i ← min{x2i , s2i+1 }
           9. end for
          10. return S = s1 , s2 , . . . , sn 



                 The algorithm is similar to Algorithm parprefix2 in the solution of
                 Exercise 2.29. It is shown as Algorithm parsuffix.

          Algorithm 2.29 parsuffix
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers, where n = 2k .
          Output: S = s1 , s2 , . . . , sn , where si = xi ◦ xi+1 ◦ · · · ◦ xn .
           1. if n = 1 then return x1
           2. else do
           3.     X1 = x1 , x2 , . . . , xn/2 
           4.     X2 = xn/2+1 xn/2+2 , . . . , xn 
           5.     S1 ← parsuffix(X1 )
           6.     S2 ← parsuffix(X2 )
           7.     for j ← 1 to n/2 do in parallel
           8.         sj ← sj ◦ s(n/2)+1
           9.     end for
          10. return S1 ∪ S2




        2.33. Do Exercise 2.32 for the case of EREW PRAM.
                 The algorithm is a generalization of Algorithm psminima in the solu-
                 tion of Exercise 2.31. Replace the MIN operator with ◦.

        2.34. Let T1 , T2 , . . . , Tm be m directed and rooted binary trees on n ver-
              tices. Each node has a pointer to its parent, except the root which
May 7, 2022   11:14       Parallel Algorithms             9in x 6in           b4591-ch02        page 87




                               Shared-memory Computers (PRAM)                              87


                                                2                     7

                                         4          1         6           5

                                                3                 8

                                             Fig. 2.36.     A tree.

                 points to itself. Design a parallel algorithm to allow each vertex to
                 know the identity of the tree to which it belongs. The trees are iden-
                 tiﬁed by their roots. The roots are numbered 1, 2, . . . , m.
                 Use pointer jumping to let each node point to its root. Then assign
                 root(s) ← succ(s) for all nodes s.

        2.35. Compute the next and succ functions as describe in Table 2.1 (page
              25) for all vertices in the tree shown in Fig. 2.36. Use the obtained
              values to derive an Euler tour.
                 Similar to Example 2.6.

        2.36. Use the Euler tour technique to direct the tree shown in Fig. 2.36,
              where vertex 1 is to be set as the root.
                 Similar to Example 2.7.

        2.37. Use the Euler tour technique to assign levels to the vertices in the
              tree shown in Fig. 2.36.
                 Similar to Example 2.8.

        2.38. In a postorder traversal of a tree T at the root r, the subtrees of r
              are traversed from left to right in postorder followed by r. Develop
              an algorithm to determine the postorder numbering of the vertices
              in a rooted tree. What is the time complexity of your algorithm?
                 Construct an Euler tour τ . Then, τ visits each vertex v several times,
                 and we only need to record the last visit, which happens when the
                 edge (v, p(v)) is visited. The detailed algorithm is given as Algorithm
                 treepostorder.
                 The time complexity is dominated by computing the preﬁx sums,
                 which is Θ(log n) using O(n) processors on the EREW PRAM.
May 7, 2022      11:14                Parallel Algorithms                         9in x 6in                b4591-ch02                                page 88




        88                                                    Parallel Algorithms


            Algorithm 2.30 treepostorder
            Input: A tree T on n vertices rooted at r.
            Output: Assign postorder numbers to all vertices in T .
                1.   Find an Euler tour τ for the tree T .
                2.   Assign the weights w(p(v), v) = 0 and w(v, p(v)) = 1, v = r.
                3.   Apply parallel preﬁx on the set of edges of τ .
                4.   Set postorder(v) to the preﬁx sum of the edge (v, p(v)).
                5.   Set postorder(r) to n.




                                                                  2                                    7

                                                          4               1                   6

                                                  5               3                                    8

                                                   Fig. 2.37.                 A rooted tree.


         (a)                                          (b)                                                  (c)
                      2                       7                       2                            7                    1 2                    5 7
                              0          1                                    0           5
                     11           0                               31               4                            3                  8
                                              0                                                    4
                 4        1             6                     4           1              6                          4          1           6
                     00                      0                    13                              5
        1                         1                   2                              7                                                 7
                 0        1              1                    1           4               6                2
            5         3                      8            5           3                           8         5            3 4               6 8

                      Fig. 2.38.         Postorder numbering of the vertices in a rooted tree.


        2.39. Apply the algorithm developed in Exercise 2.38 on the tree shown in
              Fig. 2.37.

                     See Fig. 2.38. The edges in the tour are assigned 0 and 1 in
                     Fig. 2.38(a). Parallel preﬁx is applied in Fig. 2.38(b), and the pos-
                     torder numbers are shown in Fig. 2.38(c).

        2.40. Parallelize Horner’s rule to evaluate a polynomial of degree n under
              the EREW PRAM in time O(log n).

                          f (x) = a0 + a1 x + a2 x2 + · · · + an−1 xn−1
                                      = a0 + x(a1 + x(a2 + x((. . . x(an−2 + an−1 x) . . .))))
May 7, 2022   11:14        Parallel Algorithms       9in x 6in      b4591-ch02                    page 89




                                Shared-memory Computers (PRAM)                              89


                           = a0 + x(a1 + x(a2 + x((. . . x(an/2−2 + an/2−1 x) . . .))))
                              + xn/2 (an/2 + x(an/2+1 + x(an/2+2 + x((. . . x(an−2
                              + an−1 x) . . .)))).

                 Thus, recursively compute the two halves, and multiply the right
                 half by xn/2 , which is computed by doubling in each recursive call:
                 x← x ∗ x.

        2.41. Let x1 , x2 , . . . , xn  be a sequence of n distinct numbers. Design a
              parallel algorithm for the CREW PRAM to sort this sequence in
              time O(log n). Assume an unlimited number of processors.

                 Use n groups of processors. Each group gk , 1 ≤ k ≤ n, consists of n2
                 processors, and uses Exercise 2.28 to ﬁnd the kth smallest element
                 on the CREW PRAM.

        2.42. Let n be a positive integer. Consider the problem of computing the
              polynomials yi = xi , for 1 ≤ i ≤ n. Show how to compute the yi ’s in
              O(log n) time.

                 Use parallel preﬁx.

        2.43. Consider Algorithm parquicksort presented in Section 2.5.2. Sup-
              pose we always select the median as the pivot (see Section 2.14).
              What will be the running time of the algorithm?

                 We will use n/ log n processors. The running time for ﬁnding the
                 median is that for selection, which is O(log n log log n). Since there
                 are log n levels, the overall running time is O(log2 n log log n).

        2.44. Let A and B be two sequences of distinct number sorted in ascend-
              ing order, where |A| = |B| = n. Design an O(1) time algorithm to
              merge A and B on the CREW PRAM. Assume an unlimited number
              of processors.

                 Let C be the array that will hold the merge of A and B. We will
                 use n-ary search (parallel search using n processors for each element-
                 search). Associate n processors with each element of A and B. Let
                 Pi,1 , Pi,2 , . . . , Pi,n be the n processors associated with A[i]. Processor
May 7, 2022   11:14        Parallel Algorithms           9in x 6in     b4591-ch02              page 90




        90                                       Parallel Algorithms


                 Pi,j tests whether B[j] < A[i] and B[j + 1] > A[i]. If this is the case,
                 then rank(A[i], B) = j, and we set C[i + j] = A[i]. This is done for
                 each element of A. We repeat the procedure for array B. The total
                 number of processors needed is 2n2 .

        2.45. Apply Brent’s theorem on Algorithm parmerge presented in Sec-
              tion 2.9.2.
                 The amount of work done by Algorithm parmerge is O(n log log n),
                 assuming n = m. Since the work is equal to the cost, Brent’s
                 theorem is of no help in reducing the cost by reducing the number
                 of processors.

        2.46. In Algorithm oddevenmerge in Section 2.11, Aeven is merged with
              Bodd and Aodd is merged with Beven . Rewrite the algorithm with the
              modiﬁcation that it merges Aodd with Bodd and Aeven with Beven . It
              is important to know that this will change the step of traversing the
              shuﬄe of C and D.
                 In this case, we traverse E starting from d0 . Thus, we compare d0
                 with c1 , d1 with c2 , and so on.

        2.47. Let G = (V, E) be an undirected graph with n vertices. Give an algo-
              rithm to decide whether G contains a triangle, that is, three mutu-
              ally adjacent vertices. Assume that G is represented by its adjacency
              matrix. Your algorithm should run in O(log n) time on the CRCW
              PRAM with n3 processors.
                 Let A be the n × n adjacency matrix. There is a triangle in G if and
                 only if there is a 1 in the diagonal of A3 . Thus, to test for the presence
                 of a triangle, compute A3 in Θ(log n) time, and test its diagonal for
                 the occurrence of 1 by taking the OR of the diagonal elements in
                 O(1) time as explained in Exercise 2.23.

        2.48. Prove Theorem 2.6.
                 Let T be a spanning tree, and let {V1 , V2 } be a partition of the
                 vertices. Let e be an edge connecting V1 and V2 in T . Suppose there
                 is another edge e connecting V1 and V2 in G such that w(e ) < w(e).
                 Consider the tree T  obtained from T by replacing edge e by edge e ,
                 that is, T  = T − {e} ∪ {e }. Then, the total cost of T  is less than
                 that of T .
May 7, 2022   11:14        Parallel Algorithms                 9in x 6in                   b4591-ch02           page 91




                                Shared-memory Computers (PRAM)                                             91


                                                 r                                s


                                                 i                                    j



        Fig. 2.39.    Connections between two rooted stars. Some of the weights may be ∞.


                                                     1     7    2        5        4
                                             2                  4        8       1
                                                     3
                                            6        6     3        7             5
                                                               12            9
                                                          14        13
                                                     11                  10
                                                               8

                                   Fig. 2.40.            An undirected graph.

        2.49. Show that the reduced adjacency matrix in the minimum spanning
              tree algorithm of Section 2.18 can be constructed in time O(log n)
              using O(n2 ) processors on the CREW PRAM.
                 If r and s are the roots of two stars, then the (r, s) entry of the
                 reduced matrix W  is computed as

                         W  (r, s) = min{W (i, j) | C(i) = r                             and C(j) = s}.

                 See Fig. 2.39. Let n1 and n2 be the number of nodes in stars r
                 and s, respectively. The edge of minimum weight can be determined
                 in time O(log(n1 + n2 )) = O(log n) using O(n1 n2 ) processors by
                 computing n1 minima using parallel preﬁx, and then computing the
                 minimum of these minima. The total number of processors used is
                 
                    n1 n2 , which is less than or equal to the total number of edges =
                 O(n2 ). Since all W  (r, s)’s can be computed in parallel, the con-
                 struction of the matrix W  from W takes O(log n) time using O(n2 )
                 processors on the CREW PRAM.

        2.50. Show the steps of computing a minimum spanning tree on the graph
              shown in Fig. 2.40.
                 Similar to Examples 2.15 and 2.16.

        2.51. Show the steps of computing a minimum spanning tree on the graph
              shown in Fig. 2.41.
                 Similar to Examples 2.15 and 2.16.
May 7, 2022   11:14        Parallel Algorithms                        9in x 6in       b4591-ch02   page 92




        92                                       Parallel Algorithms

                                                                  13
                                                              8            1
                                                         4                   9
                                                                  8       1
                                                     7                           2
                                                             12
                                                 3                                7
                                                     6        5                  3
                                                     11 5                 4 10
                                                                      2

                                   Fig. 2.41.                An undirected graph.


        2.52. Let G = (V, E) be an undirected graph. G is bipartite if and only
              if V can be partitioned into two parts V1 and V2 such that every
              edge connects a vertex in V1 with a vertex in V2 . Equivalently, G is
              bipartite if and only if it contains no odd-length cycles. Develop an
              algorithm to test whether G is bipartite.

                 First, ﬁnd a spanning tree T for G. Next, make T directed, and ﬁnd
                 the level of each vertex. Let V1 be the set of vertices at even levels,
                 and let V2 be the set of vertices at odd levels. Test whether two
                 adjacent vertices (in G) are both in V1 or in V2 . If there exists an
                 edge (u, v) in E such that u and v are both in V1 or both in V2 ,
                 then G is not bipartite.

        2.53. Illustrate the operation of the bitonic sort network shown in Fig. 2.21
              on the input 6, 7, 1, 4, 2, 5, 8, 3.

                 Similar to the example shown in the Fig. 2.21.

        2.54. Let A, A , C and C  be sorted sequences such that C is a 3-cover
              for A and C  is a 3-cover for A . Is C ∪ C  necessarily a 3-cover for
              A ∪ A ? See Section 2.13 for the deﬁnition of 3-cover.

                 No, as evident from the following counterexample: Let A = 2, 5, 6, 7,
                 A = 1, 3, 4, 8, C = 2, 7 and C  = 1, 8. Then, C ∪ C  = 1, 2, 7, 8
                 and A ∪ A = 1, 2, 3, 4, 5, 6, 7, 8. There are 5 elements in A ∪ A
                 between 2 and 7.

        2.55. Illustrate the operation of the pipelined mergesort algorithm on the
              input 6, 7, 1, 4, 2, 5, 8, 3.

                 Similar to the example shown in Fig. 2.22.
May 7, 2022   11:14        Parallel Algorithms     9in x 6in      b4591-ch02                   page 93




                                Shared-memory Computers (PRAM)                           93

        2.56. Prove Observation 2.1.
                 Let C = c1 , c2 , . . . and D = d1 , d2 , . . .. Let di and di+1 be two
                 adjacent elements in D, and assume that there are no elements in C
                 between them. Let cj be the element in C immediately before di , and
                 cj+1 the element in C immediately following di+1 (including −∞ and
                 +∞). Since C is a 3-cover for A, there are at most three elements
                 in A between ci and ci+1 . It follows that there are at most three
                 elements in A between di and di+1 . The other case where there are
                 elements in C between di and di+1 is similar.

        2.57. Let W, X and Y be three sorted sequences such that Y = W ∪X, and
              W ∩X = φ. Assume that R(S, S) is known for any sequence S, where
              R(A, B) is the cross ranks of A in B as deﬁned in Section 2.13. Show
              how to compute R(W, X) and R(X, W ) in O(1) time using O(|Y |)
              processors.
                 For any a ∈ X, r(a, X) = r(a, Y ) − r(a, W ), where r(a, W ) is the
                 rank of a in W . This takes care of R(W, X). Computing R(X, W ) is
                 similar.

        2.58. Parallelize the Θ(n) time sequential algorithm for selection using
              n/ log n processors on the PRAM. Analyze your algorithm.
                 Each step of the sequential algorithm is done in parallel using the
                 available processors. Dividing the inputs into groups of 5 elements
                 will meaning unclear. Sorting the log n-element groups takes Θ(log n)
                 sequential time (each group is assigned one processor). Computing
                 A1 , A2 and A3 takes Θ(log n) time using parallel preﬁx and packing
                 as explained in the parallel quicksort algorithm in Section 2.5.2. The
                 recursive calls take T (n/ log n) and T (3n/4). Hence the running time
                 is given by the recurrence T (n) ≤ T (3n/4) + T (n/ log n) + Θ(log n),
                 whose solution is T (n) = O(log2 n).

        2.59. Let A = a1 , a2 , . . . , an  be a sequence of numbers and let k be a given
              integer between 1 and n. Design and analyze a parallel algorithm to
              ﬁnd all k smallest items in A. Do not use multiselection. What model
              of computation did you use?
                 Use Exercise 2.28 to ﬁnd the kth smallest element on the CREW
                 PRAM with n2 processors and call it x. For 1 ≤ i ≤ n, let B[i] = 1
May 7, 2022   11:14       Parallel Algorithms           9in x 6in     b4591-ch02         page 94




        94                                      Parallel Algorithms


                 if ai ≤ x and B[i] = 0 otherwise. Now use parallel preﬁx and pack-
                 ing to move the k smallest elements to the beginning of A or to any
                 other location. The model used is the CREW. To solve this prob-
                 lem more eﬃciently, use the parallel selection algorithm discussed in
                 Section 2.14 on the EREW PRAM using n/ log n processors only.
May 7, 2022   11:14     Parallel Algorithms       9in x 6in   b4591-ch03                  page 95




                                              Chapter 3


                                 The Hypercube



        3.1     Introduction

        The hypercube is one of the most popular, versatile and eﬃcient topological
        structures of interconnection networks. It has many excellent features, and
        thus became the ﬁrst choice of topological structure in parallel processing
        and computing systems. Let d ≥ 0. The d-dimensional hypercube Hd has
        n = 2d nodes and d2d−1 edges. Each node corresponds to a d-bit binary
        string, and two nodes are linked by an edge if and only if their binary strings
        diﬀer in precisely one bit. Each node is incident to d = log n other nodes,
        one for each bit position. Figure 3.1 shows the d-dimensional hypercubes
        for d = 1, 2, 3.
            An edge in the hypercube is called a dimension k edge if it links two
        nodes that diﬀer in their kth bit position.
            In the d-dimensional hypercube Hd , for any k ≤ d, the removal of the
        dimension k edges leaves two disjoint copies of a (d − 1)-dimensional hyper-
        cube. Conversely, a d-dimensional hypercube Hd can be constructed from
        two (d−1)-dimensional hypercubes Hd−1 by simply connecting the ith node
        of one Hd−1 to the ith node of the other Hd−1 . Thus, a hypercube has a
        simple recursive structure. For example, see Fig. 3.2. The d-dimensional
        hypercube Hd has a diameter d, which is low, and a high bisection width
        of 2d−1 .
            Let G1 = (V1 , E1 ) and G2 = (V2 , E2 ) be two undirected graphs. The
        Cartesian product of G1 and G2 is an undirected graph, denoted by G1 ×G2 ,
        where V (G1 × G2 ) = V1 × V2 . There are two distinct vertices x1 x2 and


                                                 95
May 7, 2022   11:14          Parallel Algorithms           9in x 6in            b4591-ch03            page 96




        96                                         Parallel Algorithms




                        Fig. 3.1.       d-dimensional hypercube for d = 1, 2, 3.




                                     1110          1111                  0110        0111

                                                                0100
                         1100               1101                                     0101


                                    1010            1011               0010           0011



                      1000                    1001          0000                 0001

                         Fig. 3.2.      The construction of H4 from two H3 ’s.

        y1 y2 , where x1 , y1 ∈ V (G1 ) and x2 , y2 ∈ V (G2 ), are linked by an edge in
        G1 × G2 if and only if either x1 = y1 and (x2 , y2 ) ∈ E(G2 ), or x2 = y2 and
        (x1 , y1 ) ∈ E(G1 ). Examples of Cartesian products are shown in Figs. 3.1
        and 3.2, where H2 = H1 × H1 , H3 = H2 × H1 and H4 = H3 × H1 . Let K2
        be the complete graph on two vertices. Then, Hd can be deﬁned recursively
        as follows:
                 H1 = K 2 ,      Hd = Hd−1 × H1 = H1 × H1 × · · · × H1 ,                     d ≥ 2.
                                                                  
                                                                         d


        3.2     The Butterfly

        The butterﬂy interconnection network is closely related to the hypercube.
        Th d-dimensional butterﬂy Bd consists of n = (d + 1)2d processors and
May 7, 2022   11:14        Parallel Algorithms          9in x 6in   b4591-ch03               page 97




                                                 The Hypercube                         97




                      B1                     B2                             B3

                           Fig. 3.3.   d-dimensional butterﬂy for d = 1, 2, 3.


        d2d+1 links. Each processor in Bd is represented by the pair (u, i), where i
        is the level or dimension of the processor, 0 ≤ i ≤ d, and u is a d-bit binary
        number that denotes the row of the processor. Two processors (u, i) and
        (v, j) are connected by a link if and only if j = i + 1 and either u and
        v are identical, or u and v diﬀer in exactly the jth bit. Figure 3.3 shows
        the d-dimensional butterﬂy for d = 1, 2, 3. If u and v are identical, the
        link is said to be a straight link, otherwise it is called a cross link. Edges
        connecting processors on levels i and i + 1 are called level i + 1 edges.
            There are structural similarities between the hypercube and the butter-
        ﬂy. In particular, the ith node of Hd corresponds naturally to the ith row
        of Bd , and the ith dimension edge (u, v) of Hd corresponds to cross edges
        ((u, i − 1), (v, i)) and ((v, i − 1), (u, i)) in level i of Bd . We can obtain the
        hypercube Hd from the butterﬂy Bd by merging all nodes in the same row
        in Bd , and then removing the extra copy of each edge.
            The butterﬂy has a simple recursive structure. Figure 3.4 shows a
        3-dimensional butterﬂy with level 3 nodes removed. The result is two
        2-dimensional butterﬂies, one consisting of even rows (solid edges), and
May 7, 2022   11:14     Parallel Algorithms           9in x 6in     b4591-ch03           page 98




        98                                    Parallel Algorithms




                        Fig. 3.4.    Recursive structure of the butterﬂy.

        the other of odd rows (dashed edges). Alternatively, we could remove the
        level 0 nodes of Bd to obtain two identical Bd−1 ’s.
            A useful property of the d-dimensional butterﬂy is that the level 0 pro-
        cessor in any row u is linked to the level d processor in any row v by a
        unique path of length d. The path traverses each level exactly once, using
        the cross edge from level i to level i + 1 if and only if u and v diﬀer in the
        (i + 1)th bit. We will call this path the greedy path. Figure 3.5(a) shows
        the greedy path from (000, 0) to (110, 3). It follows that the diameter of the
        d-dimensional butterﬂy is 2d = Θ(log n). Figure 3.5(b) shows a 2d -leaf com-
        plete binary tree contained within the d-dimensional butterﬂy. The leaves
        of the tree are the level d nodes of the butterﬂy.
            An algorithm that runs on the butterﬂy is called a normal butterfly algo-
        rithm if no two processors at diﬀerent levels are active at the same time.
        That is, at any given time, only processors in the same level are participat-
        ing in the computation. A single step of a normal butterﬂy algorithm can
        be simulated in one step of the hypercube.
May 7, 2022   11:14     Parallel Algorithms          9in x 6in   b4591-ch03              page 99




                                              The Hypercube                        99




                          (a)                                          (b)

        Fig. 3.5. (a) The greedy path from (000, 0) to (110, 3). (b) A complete binary
        tree contained within B3 .



        3.3     Embeddings of the Hypercube

        There is an ever-growing interest in the portability of algorithms developed
        for architectures based on other topologies, such as linear arrays, rings,
        two-dimensional meshes, and complete binary trees, into the hypercube.
        Let G = (Vg , Eg ) and H = (Vh , Eh ) be two undirected graphs, called the
        guest and host graphs, respectively. An embedding of G into H is deﬁned
        by two mappings: φ : Vg → Vh from the set of vertices of G to the set
        of vertices of H, and ψ : Eg → Π(H) from the set of edges of G to the
        set of paths in H. Note that a path may consist of one edge, so in some
        embeddings, the mapping is ψ : Eg → Eh in which edges in G are mapped
        to edges in H.
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch03             page 100




        100                                      Parallel Algorithms

              There are some important properties associated with an embedding:

        • Dilation. The dilation of an embedding is the maximum length of a path
          in Π(H) mapped to by one single edge of G. It measures how much an
          edge in G is stretched in H.
        • Congestion. The congestion of an embedding is the maximum number
          of edges in G mapped to one single edge in H. This counts the maximum
          number of paths in the image of ψ that pass through one particular edge
          in H.
                                                                     |V |
        • Expansion. The expansion of an embedding is deﬁned by h .
                                                                     |Vg |
        • Load. This is the maximum number of nodes in G that are mapped to
          one single node in H.

        Example 3.1 Consider the two graphs G and H shown in Fig. 3.6.
        Deﬁne the embedding functions φ and ψ by: φ(a) = w, φ(b) = x, φ(c) = z,
        ψ((a, b)) = w, x, ψ((b, c)) = x, z, and ψ((a, c)) = w, y, z. Since the edge
        (a, c) is mapped to the path w, y, z, the dilation is 2. All edges of H are
        used at most once, and hence the congestion is 1. The expansion is 4/3.
        The load is 1.                                                           



        3.3.1         Gray codes
        A Gray code is an ordering of all possible d-bit binary sequences so that
        for all k ≥ 0, k and k + 1 diﬀer in exactly one bit. The sequence of 3-bit
        numbers corresponding to 0, 1, . . . , 7 is 000, 001, 011, 010, 110, 111, 101, 100.
        The Gray code of d bits is denoted by Gd , which is deﬁned recursively as

                             G1 = {0, 1} and Gk+1 = {0Gk , 1GR
                                                             k },




                               Fig. 3.6.     Example of graph embedding.
May 7, 2022   11:14          Parallel Algorithms               9in x 6in   b4591-ch03          page 101




                                               The Hypercube                             101




                                       Fig. 3.7.        Construction of G3 .




                        Fig. 3.8.   Pictorial illustration of the construction of G3 .

        where 0Gk and 1Gk denote preﬁxing each element in the sequence Gk with 0
        and 1, respectively, and GR
                                  k denotes Gk in reverse order. Thus, for example,
        to construct the sequence G3 , we do the following steps (see Fig. 3.7):

        (1) Write down the sequence for G1 columnwise, that is 01 .
                                  0G1
        (2) Next, construct G2 as 1G R.
                                                   1
                                                       0G2
        (3) Repeat step 2 to get G3 as                 1GR .
                                                         2


            Figure 3.8 shows the recursive construction of G3 pictorially. Note that
        this is a Hamiltonian cycle in H3 .

        3.3.2         Embedding of a linear array into the hypercube
        The embedding of a linear array with n = 2d processors into Hd is straight-
        forward (see Fig. 3.9). As we saw above, renumbering the hypercube proces-
        sors using the Gray code induces a Hamiltonian cycle. Hence, a linear array
        or a ring with n = 2d processors can be embedded into Hd with dilation 1
        and congestion 1.
May 7, 2022   11:14          Parallel Algorithms          9in x 6in      b4591-ch03          page 102




        102                                        Parallel Algorithms




              000 001 010 011 100 101 110 111




                       Fig. 3.9.   Embedding of a linear array into the hypercube.




                         Fig. 3.10.    Embedding of a mesh into the hypercube.



        3.3.3         Embedding of a mesh into the hypercube
        The linear array is really a 1-dimensional mesh. Although the word mesh
        usually refers to the 2-dimensional mesh, there are d-dimesional meshes
        in general with dimensions r1 , r2 , . . . , rd . A d-dimesional mesh is the cross
        product (Cartesian product) of d arrays. This is similar to the hypercube
        in which a d-dimesional hypercube is the cross product of d hypercubes
        of dimension 2. A 2-dimesional mesh can be embedded by extending the
        idea discussed above for the case of linear arrays to two dimensions. Let M
        be a mesh with 2r rows and 2c columns. We treat each row independently
        as a linear array. Next, we generate the numbers 0, 1, . . . , 2c − 1 in Gray
        code and preﬁx each processor number in row j with the number j in
        Gray code. Figure 3.10 provides an example of embedding a mesh with
        21 × 22 nodes into H3 . First, label each node in row 0 with the numbers
May 7, 2022   11:14             Parallel Algorithms                 9in x 6in         b4591-ch03               page 103




                                                  The Hypercube                                          103


                                                                                110                111
                                 011 a
                      001                                 101
                            b                         c               010
                                                                                             011

                  d             e            f                  g
                                                                                100
                 000            010         100             110                                    101



                                                                         000               001

        Fig. 3.11. An example of embedding of a binary tree with 7 nodes into the
        hypercube with 8 nodes.

        0, 1, 2, 3(00, 01, 11, 10) using G2 code. Do the same for row 1. Finally, preﬁx
        each node label in rows 0 and 1 with 0 and 1, respectively.


        3.3.4         Embedding of a binary tree into the hypercube
        There are several embeddings of binary trees into hypercubes.

        Example 3.2 Consider the embedding of a complete binary tree with
        7 nodes into a hypercube with 8 nodes shown in Fig. 3.11. The embedding
        shown is inorder since the nodes of the binary tree are labeled inorder.
        Since the edge (a, c) is mapped to the path 011, 111, 101, whose length is 2
        (which is maximum) the dilation is 2. In fact, the dilation can be found
        from the binary labels on the tree by computing the Hamming distance
        between adjacent nodes in the binary tree. For instance, the Hamming
        distance between 001 and 010 in the tree is 2. All edges of the hypercube
        are used at most twice, and hence the congestion is 2. The expansion is
        8/7, and the load is 1.                                                   


        Theorem 3.1 It is impossible to embed a complete binary tree T with
        n − 1 nodes into a hypercube H with n ≥ 8 nodes with dilation 1.

        Proof. Assume n = 2d . Since T has n − 1 nodes, the number of leaves in
        T is n/2. Suppose for the sake of contradiction that a complete binary tree
        with n − 1 nodes is a subgraph of the d-dimensional hypercube Hd . A node
        in Hd has even parity if the number of ones in its binary string is even;
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch03              page 104




        104                                    Parallel Algorithms




        Fig. 3.12. One possible embedding of a binary tree with n leaves into the hyper-
        cube with n nodes with dilation 1.


        otherwise it has odd parity. It is easy to see that the number of nodes of even
        parity is n/2, and the number of nodes of odd parity is n/2. Assume without
        loss of generality that the hypercube node that contains the root of T has
        even parity. Since the neighbors of this node have odd parity, the children
        of the root of T are contained in odd parity hypercube nodes. Similarly,
        the grandchildren of the root of T are contained in even parity hypercube
        nodes, and so on. Hence, the leaves and their grandparents, which account
        for n/2 + n/8 = 5n/8 nodes, must all be contained in hypercube nodes of
        the same parity. This is impossible, as there are only n/2 nodes with the
        same parity in Hd . It follows that T is not a subgraph of Hd .              

            It is possible, however, to embed a complete binary tree T with n leaves
        into a hypercube H with n nodes with dilation 1. Note that the tree has
        a total of 2n-1 nodes. In this embedding, the ith leaf of the binary tree T
        is mapped to the ith node of the hypercube, and each internal node of T
        is mapped to the same hypercube node as its leftmost descendant leaf. See
        Fig. 3.12.


        3.4     Broadcasting in the Hypercube

        Broadcasting a datum x from processor P0 to all other processors in the
        d-dimensional hypercube can be achieved as follows. In the ﬁrst step, P0
        sends x to P1 . In step 2, P0 and P1 send in parallel x to P2 and P3 . In
        step 3, P0 , P1 , P2 and P3 send in parallel x to P4 , P5 , P6 and P7 . The formal
        algorithm is shown as Algorithm hcbroadcast. The notation j (i) denotes
        j with the ith bit complemented, 0 ≤ i ≤ d − 1. For example, 1012 = 001.
        The total number of steps in the algorithm is d.
May 7, 2022    11:14       Parallel Algorithms      9in x 6in       b4591-ch03                   page 105




                                             The Hypercube                                 105


          Algorithm 3.1 hcbroadcast
          Input: x.
          Output: Broadcast x from P0 to all other processors.
              1. for i ← 0 to d − 1 do
              2.     for all j < 2i and j < j (i) do in parallel
              3.         Processor Pj sends x to processor Pj (i)
              4.     end for
              5. end for



        3.5      Semigroup Operations

        The hypercube is ideal for semigroup operations, e.g., addition and ﬁnd-
        ing the maximum. Assume that n numbers are distributed, one per pro-
        cessor. Then, in order to compute a semigroup operation over this set of
        numbers, the technique of reduction is used as shown in Algorithm hcsum
        for the case of the binary operation of addition. The notation i(l) means i
        with the lth bit complemented. After d steps, the ﬁnal result will be known
        to processor P0 . The instruction A[i]← A[i] + A[i(l) ] involves two substeps;
        in the ﬁrst substep, A[i] is copied from processor Pi to processor Pi(l) , and
        in the second substep, the addition operation is performed. Clearly, the
        number of parallel steps in the algorithm is d = Θ(log n).

          Algorithm 3.2 hcsum
          Input: A sequence of numbers A[j], 0 ≤ j ≤ n − 1, stored in P0 , P1 , . . . , Pn−1 .
                 where n = 2d .
                  
          Output: n−1j=0 A[j] stored in P0 .

              1. for l ← d − 1 downto 0 do
              2.     for all i, 0 ≤ i ≤ 2l − 1 do in parallel
              3.         A[i] ← A[i] + A[i(l) ]
              4.     end for
              5. end for




        3.6      Permutation Routing on the Hypercube

        Consider the problem of routing in the d-dimensional hypercube Hd with
        n = 2d processors. We consider this the problem of permutation routing in
        which every processor tries to send to a diﬀerent destination. Processor i
May 7, 2022   11:14         Parallel Algorithms             9in x 6in     b4591-ch03      page 106




        106                                       Parallel Algorithms


        wants to send a packet vi to destination δ(i). We also assume oblivious
        routing, in which the route taken by packet vi depends only on the destina-
        tion δ(i), and not on any other packet’s destination δ(j). A collision occurs
        when two packets arrive at the same processor at the same time, and try
        to leave along the same link. To deal with collisions, every processor has
        a queue and a prioritizing scheme for each incoming packet. If incoming
        packets try to leave along the same link, they are placed in a queue and
        then sent oﬀ in diﬀerent time steps.


        3.6.1         The greedy algorithm
        A straightforward method for oblivious routing is called bit fixing, which
        works by taking the bit address of the source processor and changing one
        bit at a time to the address of the destination processor. Each time a bit
        is changed, the packet is forwarded to a neighboring processor. Clearly, bit
        ﬁxing is an optimal routing scheme for a single packet. If the source i and
        destination j diﬀer by k bits, then the packet must traverse at least k links
        in the hypercube to get to its destination. Bit ﬁxing takes exactly k steps.
                                                        √
        However, the queue size can be as large as O( n), as is evident from the
        following theorem.


        Theorem 3.2 The maximum queue size of the greedy algorithm for
                                                                √
        permutation routing on the d-dimensional hypercube is O( n).

        Proof.      Notice that during bit ﬁxing routing, an intermediate address
        is always of the form z = y1 · · · yk xk+1 · · · xd , where xi is a bit of the
        source address, and yj is a bit of the destination address. If two packets
        collide, that means their destination addresses agree in their ﬁrst k bits,
        and their source addresses agree in their last d − k bits, where 1 ≤ k ≤ d.
        There are 2k packets with source addresses agreeing on xk+1 · · · xd , and 2d−k
        packets with destination addresses agreeing on y1 . . . yk that may end up
        at processor Pz . Therefore, if we let S be the set of packets that collide
                                    d
        at processor Pz , then |S| ≤ k=1 min{2k , 2d−k }, since k ranges between 1
        and d. Assume without loss of generality that d is even. Then,

                                                  
                                                  d
                                       |S| ≤            min{2k , 2d−k }
                                                  k=1
May 7, 2022   11:14        Parallel Algorithms            9in x 6in               b4591-ch03         page 107




                                             The Hypercube                                     107


                                                 
                                                 d/2
                                                                
                                                                d
                                           =           2k +                2d−k
                                                 k=1          k=d/2+1

                                                 
                                                 d/2
                                                              
                                                              d/2−1
                                           =           2k +           2k
                                                 k=1          k=0

                                           = 3 × 2d/2 − 3.
                                           = O(2d/2 )
                                               √
                                           = O( n).
                                                    √
        It follows that the maximum queue size is O( n).                                        


        3.6.2         The randomized algorithm
        If we have to route many packets, bit ﬁxing can cause many collisions, as
        shown in Theorem 3.2. In fact, so can any deterministic oblivious routing
        strategy. We have the following theorem, which is quite general:


        Theorem 3.3 Any deterministic oblivious permutation routing scheme
        for
           a parallel machine with n processors, each with d outward links requires
        Ω( n/d) steps.

            Luckily, we can avoid this bad case by using a randomized routing
        scheme. In fact, most permutations cause very few collisions. So, the idea
        is to ﬁrst route all the packets using a random permutation, and then from
        there to their ﬁnal destination. That is,

        (a) Phase 1. Choose a random permutation σ of {1, 2, . . . , n}. Route
            packet vi to destination σ(i) using bit ﬁxing.
        (b) Phase 2. Route packet vi from σ(i) to destination δ(i) using bit ﬁxing.

        The following observation about bit ﬁxing during one of the two phases
        above is important.


        Observation 3.1 Two packets can come together along a route and then
        separate, but only once. That is, a pair of routes can look like Fig 3.13(a),
        but part (b) of the ﬁgure is impossible.
May 7, 2022   11:14         Parallel Algorithms          9in x 6in          b4591-ch03      page 108




        108                                       Parallel Algorithms

                      (a)                                      (b)




                                        Fig. 3.13.     Packets collision.

            To see this, notice that during bit ﬁxing routing, an intermediate address
        is always of the form y1 . . . yk xk+1 . . . xd , where xi is a bit of the source
        address, and yj is a bit of the destination address. If two routes collide at
        the kth step, that means their destination addresses agree in their ﬁrst k
        bits, and the source addresses agree in their d−k bits. At each time step, we
        add one more bit of the destination address, which means we increment k.
        Eventually, the destination bits must disagree since the destinations are
        diﬀerent. Let k0 be the value of k at which this happens. Then, the yk0
        destination bit is diﬀerent for the two packets. At this point, the two packets
        separate, and they will never collide again, because all the later intermediate
        destinations will include the yk0 bit. Observation 3.1 is the crux of the proof
        of the following theorem:

        Theorem 3.4 Let S be the set of packets whose routes intersect vi ’s
        route. Then, the delay of packet vi is ≤ |S|.

            Notice that whenever the routes of two packets intersect, one of the
        packets may be delayed by one time step. Once that packet is delayed by
        one time step at the ﬁrst shared node, it will ﬂow along the shared route
        behind the other packet, and will not be delayed any more by that packet.
        If the same route intersects other routes, each of them may add a delay
        of one time step. This happens either because another packet collides with
        the current packet along a shared route, or because another packet collides
        with a packet that is ahead of the current packet along a part of the route
        which is shared by all three. In either case, an extra delay of at most one
        results.
            To get the running time of this scheme, we compute the expected value
        of the size of the set S above. Deﬁne the indicator random variable Xi,j
        which is 1 when the routes of packets vi and vj share at least one edge,
        and Xi,j is 0 otherwise. Then, by the above theorem, the expected delay
                                                            n
        of packet vi is the expected size of S, which is E    j=1 Xi,j . It is rather
May 7, 2022    11:14       Parallel Algorithms       9in x 6in   b4591-ch03                   page 109




                                              The Hypercube                            109


        diﬃcult to get an estimate of this quantity. It is easier to think of Y (e),
        which is the number of routes that pass through a given edge e. Now,
        suppose the route of packet vi consists of the edges (e1 , e2 , . . . , ek ). Then,
                            
        we have nj=1 Xi,j ≤ kl=1 Y (el ). Hence,
                                     ⎡        ⎤      k        
                                      n             
                                    E⎣   Xi,j ⎦ ≤ E     Y (el ) .
                                         j=1              l=1


        To use this bound, we next compute E[Y (e)]. Notice that

              E[Y (e)] = (sum of lengths of all routes)/(total edges in the network).

        The sum of lengths of all routes is the expected length of a route times n (the
        number of all routes). The average length of a route is d/2 because a d-bit
        source diﬀers from a random destination address in d/2 bits on average. So,
        the sum of route lengths is nd/2. The total number of edges in the network
        is the number of nodes times the number of outbound links, which is nd.
        So, E[Y (e)] = (nd/2)/(nd) = 1/2. Thus, if the path for packet vi has k
        edges along it, then
                  ⎡         ⎤       k         
                    n                            k
                                                                   k         1   d
            μ=E   ⎣         ⎦
                        Xi,j ≤ E        Y (el ) =     E[Y (el )] = ≤ d × = .
                    j=1
                                                                   2         2   2
                                          l=1            l=1


        Now, we can apply Chernoﬀ bound in Theorem A.3 to the probability
        of there being a substantial number of paths intersecting vi ’s path. The
        Chernoﬀ bound is
                               ⎡                   ⎤
                                n
                            Pr ⎣    Xi,j > (1 + δ)μ⎦ < 2−δμ .
                                        j=1


        We now compute the probability that vi is delayed at least 3d steps. So,
        we require that (1 + δ)μ = 3d. Notice that we do not actually know what μ
        is, but we have a bound for it of μ ≤ d/2. It follows that μδ ≥ 2.5d. Thus,
        the probability that vi is delayed by at least 3d steps is bounded above by
        2−2.5d .
            This is a bound for the probability that a given packet is delayed more
        than 3d steps. But we want to get a bound for the probability that no
May 7, 2022   11:14         Parallel Algorithms          9in x 6in      b4591-ch03               page 110




        110                                       Parallel Algorithms

        packet gets delayed more than 3d steps. For that it is enough to use Boole’s
        inequality for probabilities as a bound:

        Boole’s inequality: For any ﬁnite sequence of events E1 , E2 , . . . , En ,

                      Pr[E1 ∪ E2 ∪ · · · ∪ En ] ≤ Pr[E1 ] + Pr[E2 ] + · · · + Pr[En ].   (3.1)

           There are n = 2d routes in total, and the probability that one of these
        takes more than 3d steps is bounded above by 2d 2−2.5d = 2−1.5d . So we
        can make the following assertion: With probability at least 1 − 2−1.5d every
        packet reaches its destination σ(i) in 4d or fewer steps. The 4d comes from
        the delay time 3d plus the time for bit ﬁxing steps, which is ≤ d. Notice
        that all of this applies to just one phase of the algorithm. So, the full
        algorithm(two phases) routes all packets to their destinations with high
        probability in 8d or fewer steps.


        3.7     Permutation Routing on the Butterfly

        Consider the problem of sending packets from level 0 to level d in the
        d-dimensional butterﬂy Bd . Processor (i, 0) in level 0 wants to send a
        packet vi to destination (δ(i), d) in level d. We consider the problem of
        permutation routing in which every processor in level 0 tries to send to a
        diﬀerent destination in level d. That is, the function δ(i) is a permutation.
           A simple process for routing a single packet obliviously is called bit
        ﬁxing. For deﬁnitions of bit ﬁxing, its lower bound, collision and oblivious
        routing, see Section 3.6. Next we discuss in detail a randomized routing
        scheme for the butterﬂy. This scheme consists of three phases.

        (a) Phase 1. Choose a random permutation σ of {1, 2, . . . , 2d }. Route
            packet vi to destination (σ(i), d) using the greedy path.
        (b) Phase 2. Route packet vi from (σ(i), d) to destination row but in level 0
            (δ(i), 0) using the greedy path.
        (c) Phase 3. Route packet vi from (δ(i), 0) in level 0 to (δ(i), d) in level d
            through direct links.

           In what follows, we analyze Phase 1; Phase 2 is the reverse of Phase 1,
        and Phase 3 takes d steps.
           Let S be the set of packets whose routes intersect vi ’s route. Deﬁne the
        indicator random variable Xi,j which is 1 when the routes of packets vi
May 7, 2022   11:14     Parallel Algorithms         9in x 6in         b4591-ch03            page 111




                                           The Hypercube                             111

        and vj share at least one edge, and Xi,j is 0 otherwise. Then, by Theo-
        rem 3.4, the expected delay of packet vi is the expected size of S, which is
          2d
        E    j=1 Xi,j . It is rather diﬃcult to get an estimate of this quantity. It
        is easier to think of Y (e), which is the number of routes that pass through
        a given edge e. Now, suppose the route of packet vi consists of the edges
                                              2d        d
        (e1 , e2 , . . . , ed ). Then, we have j=1 Xi,j ≤ l=1 Y (el ). Hence,
                                  ⎡ d      ⎤      d        
                                   2             
                                 E⎣   Xi,j ⎦ ≤ E     Y (el ) .
                                      j=1                l=1


        To use this bound, we next compute E[Y (el )]. Consider the link el at level l,
        which connects level l − 1 node to level l node. The number of packets that
        can potentially go through el is 2l−1 since there are only 2l−1 processors
        at level 0 for which there are greedy paths through this link. In fact, if
        el = ((u, l − 1), (v, l)), then u is the root of a complete binary tree with
        2l−1 leaves in level 0. Now, we compute the probability that packet vi will
        go through link el . Consider what happens to packet vi in level 0, when
        it wants to move to level 1. There are two links to choose from to go to
        level 1, either the direct link or the cross link. Thus, it takes one of these
        two links with probability 1/2. It follows that in order for packet vi to go
        through link el , it has to go through l links with probability (1/2)l . Clearly,
        Y (el ) has the binomial distribution with parameters 2l−1 and (1/2)l (see
        Section A.4.3). Hence, E[Y (el )] = 2l−1 × (1/2)l = 1/2. Thus,
                        ⎡ d        ⎤      d         
                          2                            d
                                                                        d
                      E ⎣          ⎦
                              Xi,j ≤ E        Y (el ) =     E[Y (el )] = .
                          j=1
                                                                        2
                                              l=1               l=1


        Now, we can apply Chernoﬀ bound in Theorem A.3 to the probability
        of there being a substantial number of paths intersecting vi ’s path. The
        Chernoﬀ bound is
                               ⎡ d                 ⎤
                                2
                            Pr ⎣    Xi,j > (1 + δ)μ⎦ < 2−δμ .
                                     j=1


        We compute the probability that vi is delayed at least 3d steps. So, we
        require that (1 + δ)μ = 3d. Notice that we do not actually know what μ
        is, but we have a bound for it of μ ≤ d/2. It follows that μδ ≥ 2.5d. Thus,
May 7, 2022    11:14        Parallel Algorithms          9in x 6in      b4591-ch03             page 112




        112                                       Parallel Algorithms

        the probability that vi is delayed by at least 3d steps is bounded above by
        2−2.5d .
            This is a bound for the probability that a given packet is delayed more
        than 3d steps. But we want to get a bound for the probability that no
        packet gets delayed more than 3d steps. For that, it is enough to use Boole’s
        inequality for probabilities as a bound (Eq. (3.1)): There are 2d routes
        in total, and the probability that one of these takes more than 3d steps
        is bounded above by 2d 2−2.5d = 2−1.5d . So we can make the following
        assertion: With probability at least 1 − 2−1.5d , every packet vi reaches its
        phase 1 destination (σ(i), d) in 4d or fewer steps. The 4d comes from the
        delay time 3d plus the time for bit ﬁxing steps, which is d. Notice that all of
        this applies to just one phase of the algorithm. So, the full algorithm(three
        phases) routes all packets to their destinations with high probability in
        4d + 4d + d = 9d or fewer steps.


        3.8      Computing Parallel Prefix on the Hypercube

        The parallel preﬁx problem was deﬁned in Section 2.5. In this section,
        we show how to compute it on the hypercube. Let Hd be a d-dimensional
        hypercube, where each processor Pi contains item xi , 0 ≤ i ≤ n−1 = 2d −1.
        Assume that each processor has two registers: s and z. The algorithm is
        shown as Algorithm hcparprefix. The notation j (i) means j with the ith
        bit complemented, 0 ≤ i ≤ d − 1, where i = 0 corresponds to the rightmost
        least signiﬁcant binary digit. For example, 1001 = 110. sj computes the
        sum x0 ◦ x2 ◦ · · · ◦ xj , and zj is a temporary variable. Initially, si = zi = xi ,
        0 ≤ i ≤ n − 1.

          Algorithm 3.3 hcparprefix
          Input: X = x0 , x1 , . . . , xn−1 , a sequences of n numbers, where n = 2d .
          Output: S = s0 , s1 , . . . , sn−1 , the preﬁx sums of X.
              1. for i ← 0 to d − 1 do
              2.     for all j < j (i) do in parallel
              3.         zj (i) ← zj (i) ◦ zj
              4.         sj (i) ← sj (i) ◦ zj
              5.         zj ← zj (i)
              6.     end for
              7. end for
May 7, 2022   11:14          Parallel Algorithms      9in x 6in   b4591-ch03                  page 113




                                               The Hypercube                           113


                  (a)                                     (b)




                  (c)                                     (d)




        Fig. 3.14.      Example of computing parallel preﬁx on the 3-dimensional hypercube.



            Figure 3.14 illustrates the operation of the algorithm on the
        3-dimensional hypercube. For clarity, the intermediate calculations have
        been shown with indices of the form si−j , which is equal to xi ◦xi+1 ◦· · ·◦xj ,
        0 ≤ i ≤ j ≤ n − 1. The same thing applies to zi−j . Figure 3.14(b) shows the
        contents of registers after the computations in the ﬁrst iteration (i = 0).
        Parts (c) and (d) show the contents of registers after the computations in
        the second and third iterations (i = 1 and 2). There are d = log n iterations
        in the algorithm, each takes Θ(1) time. Hence, its running time is Θ(log n).


        3.9     Hyperquicksort

        Quicksort is a very popular sorting algorithm. There have been numerous
        attempts to parallelize it for a variety of machines and models of com-
        putation; see Section 2.5.2 for an example. One attempt is hyperquick-
        sort, which is targeted for the case of hypercubes with p < n, where n is
        the number of items and p is the number of processors. The algorithm is
        shown as Algorithm hchyperquicksort. Initially, it is assumed that the
May 7, 2022    11:14       Parallel Algorithms          9in x 6in      b4591-ch03              page 114




        114                                      Parallel Algorithms


        n elements are evenly distributed among the p = 2d processors, so that
        every processor contains n/p elements.

          Algorithm 3.4 hchyperquicksort
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers.
          Output: X sorted in ascending order.
              1. Each processor sorts its n/p items using a sequential sorting algorithm.
              2. Processor P0 determines the median m of its elements and broadcasts it
                 to all other processors.
              3. Every processor Pi partitions its items into X of items ≤ m and Y of
                 items > m.
              4. Let the two subcubes of size 2d−1 each be L and U . Every processor Pi
                 in L sends its set Y to its adjacent processor Pj in U . Likewise, Pj sends
                 its set X to Pi .
              5. Every processor merges the elements that it already has with those it
                 received from its adjacent processor.
              6. Repeat Steps 2–5 to recursively sort L and U in parallel until the subcubes
                 consist of one processor.



            Clearly, Algorithm hchyperquicksort sorts its input. What remains
        is to ﬁnd its running time. Assume that the data is balanced, so that after
        Step 5 is executed, each processor has Θ(n/p) elements. In this case, the
        recursion depth is O(log p) = O(d). Step 1 takes Θ((n/p) log(n/p)) time.
        Determining the median in Step 2 takes Θ(1) time since the items in each
        processor are sorted. Broadcasting m takes Θ(d) time in one recursive call
        for a total of d + (d − 1) + (d − 2) + · · · = d(d+1)
                                                          2   = Θ(d2 ) in all recursive
        calls. Step 3 takes Θ(n/p) time. Step 4 of data transmission takes Θ(n/p)
        time. By the end of this step, every element in L is ≤ every element in U .
        Step 5 of merging the two sets takes Θ(n/p) time.
            It can be shown that if the data is initially distributed in a random
        fashion, the expected running time of the algorithm is
                                                             
                              Θ (n/p) log(n/p) + d2 + dn/p .

        The (n/p) log(n/p) term represents the sorting step. The d2 term repre-
        sents broadcasting as stated above, and the dn/p term represents the time
        required for exchanging and merging sets of elements in all recursive calls.
        One disadvantage of the algorithm is that the elements may not be evenly
        distributed after the algorithm terminates.
May 7, 2022   11:14       Parallel Algorithms            9in x 6in        b4591-ch03                   page 115




                                            The Hypercube                                        115

        3.10      Sample Sort

        Sample sort is a generalization of quicksort, in which a sample of size s is
        selected, and the input is partitioned into s + 1 parts, where all elements
        in one part are less than all elements in the next part. Each part is then
        sorted separately. Let n be the number of elements, and p the number
        of processors, where n ≥ p2 . Let S = {a1 , a2 , . . . , an } be the sequence of
        elements to be sorted, and assume they are distinct.
            Parallel sample sort consists of the following steps: In the beginning, it is
        assumed that each processor has a list of w = np items, which it sorts using a
        sequential sorting algorithm. Deﬁne a regular sample X = X0 ∪X1 ∪· · · Xp−1
        to be a set of p(p − 1) elements, where

              Xj = {a(w/p)+jw , a(2w/p)+jw , . . . , a((p−1)w/p)+jw },          0 ≤ j ≤ p − 1.

        In other words, from each of the p lists, p − 1 samples are chosen, evenly
        spaced throughout the list. Next, X is sorted using a sequential sorting
        algorithm. This can be achieved by letting each processor send its sample
        of p − 1 elements to processor P0 , which then sorts the whole sample of
        p(p − 1) elements (Exercise 3.9). Let this ordered sample be

                                            b1 , b2 , . . . , bp(p−1) .

        Next, choose

                               Y = b(p/2) , bp+(p/2) , . . . , b(p−2)p+(p/2)

        as the p − 1 pivots for partitioning S, which we will refer to as

                                            y1 , y2 , . . . , y(p−1) .

        In other words, the p(p − 1) samples are sorted and p − 1 elements evenly
        spaced throughout the sorted list, are chosen to be the pivots.
           The partitioning of S is accomplished as follows. Each processor ﬁnds
        where each of the p − 1 pivots divides its list using binary search, after
        which each of the p sorted lists of S have been divided into p sorted sublists
        with the property that every item in every list’s ith sorted sublist is greater
        than any item in any list’s (i − 1)th sorted sublist, for 2 ≤ i ≤ p.
           Finally, each processor Pi , 1 ≤ i ≤ p, performs a p-way mergesort to
        merge all the ith sorted sublists of p lists. Note that, unlike in the ﬁrst step,
        in which each processor sorts a contiguous block of items, each processor
May 7, 2022    11:14        Parallel Algorithms           9in x 6in        b4591-ch03               page 116




        116                                       Parallel Algorithms

        merges p sublists stored in p diﬀerent areas. Because of the demarcations
        established before, their merges are completely independent of each other.
        The above description is summarized in Algorithm samplesort.

          Algorithm 3.5 samplesort
          Input: S = a1 , a2 , . . . , an , a sequences of n numbers.
          Output: S sorted in ascending order.
              1. Set w ← np .
              2. Each processor sorts its list of size w.
              3. Each processor chooses evenly spaced p − 1 samples from its list. Let X
                 be the set of p(p − 1) samples.
              4. Sort X using a sequential sorting algorithm. Let this ordered sample be
                 b1 , b2 , . . . , bp(p−1) .
              5. Choose Y = b(p/2) , bp+(p/2) , . . . , b(p−2)p+(p/2) as the p − 1 pivots.
              6. Each processor Pi ﬁnds where each of the p −1 pivots divides its list using
                 binary search, and divides its list into p sublists.
              7. Each processor Pi , 1 ≤ i ≤ p, performs a p-way mergesort to merge all
                 the ith sorted sublists of p lists.



        Example 3.3 Figure 3.15 provides an illustration of Algorithm sample-
        sort for the case n = 24 and p = 3. The input is given in Fig. 3.15(a),
        which is divided into three parts, one part per processor. The set X of

              (a)
                    12 17 15 21 8 3 14 9        18 4 10 5 23 16 24 19      7 20 1 11 6 2 13 22

              (b)
                    3 8 9 12 14 15 17 21          4 5 10 16 18 19 23 24    1   2 6 7 11 13 20 22


                                          (c)
                                                  9 15 10 19 6 13
                                          (d)
                                                  6 9 10 13 15 19

              (e)
                    3 8 9 4 5 1 2 6 7              12 14 15 10 11 13   17 21 16 18 19 23 24 20 22

              (f)
                    1 2 3 4 5 6 7 8 9              10 11 12 13 14 15   16 17 18 19 20 21 22 23 24

                           Fig. 3.15.    Illustration of Algorithm samplesort.
May 7, 2022   11:14       Parallel Algorithms      9in x 6in   b4591-ch03                   page 117




                                            The Hypercube                             117


        sample elements is shown as the shaded items in Fig. 3.15(b). These items
        are shown in Fig. 3.15(c). Sorting this sample and choosing p − 1 = 2 pivots
        is shown in Fig. 3.15(d). Figure 3.15(e) shows the contents of each processor
        after merging the sublists, and Fig. 3.15(f) shows the sorted items.       

        Theorem 3.5 In the last step of Algorithm samplesort, each processor
        merges less than or equal to 2w = 2n
                                           p elements.


        Proof.        Consider any processor Pi , 1 ≤ i ≤ p. There are three cases.
        Case 1: i = 1. All the items to be merged by processor P1 must be ≤ y1 .
        Since there are p2 − p − p2 samples which are > y1 , there are at least
        (p2 − p − p2 ) wp elements of S which are > y1 . In other words, there are at
        most n − (p2 − p − p2 ) wp = (p + p2 ) wp < 2w elements of S which are ≤ y1 .
        Case 2: i = p. All the items to be merged by processor Pp must be > yp−1 .
        Since there are (p − 2)p + p2 samples which are ≤ yp−1 , there are at least
        (p2 − 2p + p2 ) wp elements of S which are ≤ yp−1 . In other words, there are at
        most n − (p2 − 2p + p2 ) wp = (2p − p2 ) wp < 2w elements of S which are > yp−1 .
        Case 3: 1 < i < p. All the items to be merged by processor Pi must
        be > yi−1 and ≤ yi . Since there are (i − 2)p + p2 samples which are ≤ yi−1 ,
        there are at least ((i − 2)p + p2 ) wp elements of S which are ≤ yi−1 . On the
        other hand, there are (p − i)p − p2 samples which are > yi . Thus, there are
        at least ((p − i)p − p2 ) wp elements of S which are > yi . Combining these two
        inequalities, there are at most
                                    p w            p w     w
                      n − (i − 2)p +     − (p − i)p −      = 2p = 2w
                                     2 p              2 p      p

        elements of S for processor Pi to merge.
                                                                2n
        It follows that no processor merges more than 2w =       p   elements in the last
        step of the algorithm.                                                         


            Now, we analyze the running time of the algorithm. Step 2 of sequential
        sorting takes Θ(w log w). Step 4 of sorting the sample takes Θ(p2 log p2 ). In
        Step 5, each processor performs p − 1 binary searches in O(p log w) time.
        By Theorem 3.5, in the last step of merging the sublists, the size of data
        to be merged by any processor is less than or equal to 2w, and hence the
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch03               page 118




        118                                    Parallel Algorithms


        time needed by this step is O(w log p). Hence, the total running time is

        T (n, p) = Θ(w log w + w log p + p log w + p2 log p2 ) = Θ(w log w + p2 log p2 ),

        since n ≥ p2 . When n ≥ p3 , the running time becomes
                                                            
                                                     n     n
                           T (n) = Θ(w log w) = Θ      log     .
                                                     p     p


        3.11      Selection on the Hypercube

        Recall the problem of selection discussed in Section 2.14: Given a
        sequence A = a1 , a2 , . . . , an of n elements and a positive integer k, 1 ≤
        k ≤ n, ﬁnd the kth smallest element in A. In this section, we develop an
        algorithm that runs on the hypercube with p < n processors. The algorithm
        is shown as Algorithm hcselect.

          Algorithm 3.6 hcselect
          Input: A sequence A = a1 , . . . , an  of elements and an integer k, 1 ≤ k ≤ n.
          Output: The kth smallest element in A.
           1. if |A| ≤ p then sort A and return the kth smallest element.
           2. for i ← 0 to p − 1 do in parallel
           3.     Processor Pi computes the median mi of its local n/p elements
                  using an optimal sequential algorithm for selection. Let the set of
                  medians be M .
           4. end for
           5. Sort M and ﬁnd its median m.
           6. Broadcast m to all p processors.
           7. Partition A into three sequences:
              A1 = {a | a < m}
              A2 = {a | a = m}
              A3 = {a | a > m}
           8. case
           9.     |A1 | ≥ k:
          10.          Distribute A1 evenly over all processors
          11.          hcselect(A1 , k)
          12.     |A1 | + |A2 | ≥ k: return m
          13.     |A1 | + |A2 | < k:
          14.          Distribute A3 evenly over all processors
          15.          k ← k − |A1 | − |A2 |
          16.          hcselect(A3 , k)
          17. end case
May 7, 2022   11:14      Parallel Algorithms       9in x 6in       b4591-ch03              page 119




                                           The Hypercube                            119

            The time complexity of the algorithm can be computed as follows: Step 3
        of the algorithm takes O(n/p) time using an optimal sequential algorithm
        for selection. The sorting step in Line 4 takes ts (p, p) time, which is the
        time needed to sort p elements using p processors. Broadcasting m in Step 5
        requires O(log p) time. Partitioning A into A1 , A2 and A3 can be done by
        ﬁrst each processor splitting its data, and then computing the global sizes
        of A1 , A2 and A3 . This takes O(n/p + log p) time using parallel preﬁx and
        compaction. The load balancing problem (see Section 3.13) is to redistribute
        data items stored in a hypercube such that the number of items in diﬀerent
        processors diﬀer by at most one after the redistribution. We use a load
        balancing algorithm that has a time complexity of O(M + log p) where M
        is the maximum number of items at any processor before the redistribution.
        Thus, data distribution in Steps 10 and 14 takes O(n/p + log p) time.
            The median of medians m is smaller than (and greater than) at least
        (|A|/2p)(p/2) = |A|/4 elements. That is, it is greater than (and smaller
        than) at most 3|A|/4 elements (Exercise 2.17). Hence, the recursive call
        takes at most T (3n/4). This implies the following recurrence for the running
        time:
                            ⎧
                            ⎨O(n)                               if p = 1
                 T (n, p) = O(ts (p, p))                        if p ≥ n
                            ⎩
                              T (3n/4, p) + O(n/p + ts (p, p)) if 1 < p < n.

        The recursion depth is log n − log p, since the recursion ceases when n
        becomes less than p. The solution to this recurrence is

          T (n, p) = O(n/p + ts (p, p)(log n − log p)) = O(n/p + ts (p, p) log(n/p)).

        If, for example, we let p = n/ log n and use the O(log p log log p) time sorting
        algorithm, then the time complexity becomes

                      T (n, n/ log n) = O(log n + log p log log p log(log n))
                                        = O(log n + log n(log log n)2 )
                                        = O(log n(log log n)2 ).


        3.12      Multiselection on the Hypercube

        Let A = a1 , a2 , . . . , an be a sequence of n distinct elements drawn from a
        linearly ordered set, and let K = k1 , k2 , . . . , kr be a sequence of positive
May 7, 2022   11:14     Parallel Algorithms          9in x 6in      b4591-ch03             page 120




        120                                   Parallel Algorithms

        integers between 1 and n. The multiselection problem is to select the ki th
        smallest element for all values of i, 1 ≤ i ≤ r. The hypercube structure
        is ideal for parallel execution of balanced divide-and-conquer algorithms.
        This leads to the following idea of the multiselection algorithm: First, use
        Algorithm hcselect to ﬁnd the median m of A. Use m as a splitter to
        partition A into A1 of items smaller than or equal to m, and A2 of items
        larger than m. This induces a bipartition of B into two subsequences —
        B1 of items less than k = n/2 , and B2 of items greater than k. The
        algorithm is then recursively called in parallel with (A1 , B1 ) and (A2 , B2 ).
        Note that since the elements are distinct, |A1 | = n/2 and |A2 | = n/2.
        Following this idea, the algorithm is shown as Algorithm hcmultiselect.
        In cube(s, d), s is the starting address of the cube and d is its dimension.
        Initially, the algorithm is called with hcmultiselect(A, B, cube(0, log p)).
            In Step 13, A1 is discarded, since |B1 | = 0. Similarly, in Step 16, A2
        is discarded, since |B2 | = 0. Let Q denote our hypercube with p = 2d
        processors. Q can be divided into two disjoint halves L and U , where L
        consists of processors with addresses 0x, and U consists of processors with
        addresses 1x. Now, we show how to move the elements in A1 and A2 to L
        and U , respectively, as stated in Steps 19 and 20. Every processor P in Q
        logically partitions its local set of data into two groups X and Y , where X
        contains those elements less than or equal to the median m, and Y con-
        tains those elements greater than m. This requires O(n/p) sequential time.
        Now, each processor P0x in L sends its set Y to its adjacent processor P1x
        in U . Likewise, each processor P1x in U sends its set X to its adjacent
        processor P0x in L. Notice that when this step is complete, all elements
        less than or equal to m are in L, while all elements greater than m are
        in U . This step requires O(n/p) time for the transmission of data. It is
        followed by load balancing (see Section 3.13). The load balancing problem
        is to redistribute data items stored in a hypercube such that the number of
        items in diﬀerent processors diﬀer by at most one after the redistribution.
        The load balancing algorithm that we will use has a time complexity of
May 7, 2022   11:14      Parallel Algorithms      9in x 6in      b4591-ch03                   page 121




                                           The Hypercube                                121




          Algorithm 3.7 hcmultiselect
          Input: A sequence A = a1 , . . . , an  of elements and a sequence of positive
                 integers B = k1 , k2 , . . . , kr , 1 ≤ ki ≤ n. cube(s, d), the starting
                 address of the cube s and its dimension d.
          Output: The ki th smallest element in A, 1 ≤ i ≤ r.
           1. p ← 2d .
           2. if p = 1 then use a sequential multiselection algorithm and exit.
           3. else if |A| ≤ p then sort A and return the ki th smallest element,
              1 ≤ i ≤ r.
           4. else if |B| = 1 then use Algorithm hcselect to ﬁnd the k1 th smallest
              element.
           5. else do Steps 6 to 24
           6. Use Algorithm hcselect to ﬁnd the median element m
           7. Broadcast m to all p processors.
           8. k ← |A|/2 .
           9. Partition A into A1 and A2 , where A1 (resp. A2 ) is the set of elements in
              A less than or equal to (resp. greater than) m.
          10. Partition B into B1 and B2 , where B1 (resp. B2 ) is the set of elements
              in B less than or equal to (resp. greater than) k. Subtract k from each
              item in B2 .
          11. case
          12.     |B1 | = 0:
          13.          Distribute A2 evenly over all processors
          14.          hcmultiselect(A2 , B2 , s, d)
          15.     |B2 | = 0:
          16.          Distribute A1 evenly over all processors
          17.          hcmultiselect(A1 , B1 , s, d)
          18.     |B1 | > 0 and |B2 | > 0:
          19.          Distribute A1 evenly over all processors in
                       L = cube(, s) d-1
          20.          Distribute A2 evenly over all processors in
                       U = cube(, s + 2d−1 ) d-1
          21.          do in parallel
          22.             hcmultiselect(A1 , B1 , cube(s, d − 1))
          23.             hcmultiselect(A2 , B2 , cube(s + 2d−1 , d − 1))
          24. end case
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch03               page 122




        122                                      Parallel Algorithms


        O(M + log p) where M is the maximum number of items at any processor
        before the redistribution. Thus, it runs in time O(n/p + log p).
             The time complexity of the algorithm can be computed as follows:
        ﬁnding the median m by Algorithm hcselect in Step 6 requires O(n/p +
        ts (p, p) log(n/p)) time, where ts (p, p) is the time needed to sort p ele-
        ments using p processors. Partitioning A into A1 and A2 can be done by
        each processor splitting its data in time O(n/p). Data redistribution takes
        O(n/p + log p) time. This implies the following recurrence for the running
        time:
                        ⎧
                        ⎪
                        ⎪
                          O(n log r)                                         if p = 1
                        ⎪
                        ⎪
                        ⎪
                        ⎪
                        ⎨ O( np + ts (p, p) log( np ))                       if r = 1
          T (n, r, p) ≤
                        ⎪
                        ⎪                                                    if p ≥ n
                        ⎪
                        ⎪
                          O(ts (p, p))
                        ⎪
                        ⎪
                        ⎩ n
                          T ( 2 , r − 1, p2 ) + O( np + ts (p, p) log( np )) if 1 < p, r < n.

        In the worst case, the recursion depth is min{r, log p, log n} = min{r, log p}
        since p < n. It follows that the solution to this recurrence is

                      T (n, r, p) = O((n/p + ts (p, p) log(n/p)) min{r, log p}).

        If we use the O(log p log log p) time sorting algorithm, then the time com-
        plexity becomes

                  T (n, r, p) = O((n/p + log p log log p log(n/p)) min{r, log p}).

        If we let p = n1− for 0 <  < 1, then there is always a constant n0 such
        that n/p = n > log p log log p log(n/p) holds for all n > n0 . This shows
        that T (n, r, n1− ) = O(n min{r, log n1− }).


        3.13      Load Balancing on the Hypercube

        The load balancing problem on the hypercube is to redistribute data items
        stored in a hypercube such that the number of items in diﬀerent processors
        diﬀer by at most one after the redistribution. In this section, we present a
        simple load balancing algorithm on the hypercube. Assume that it takes one
        time unit to move one item from one processor to a neighboring processor.
May 7, 2022   11:14      Parallel Algorithms      9in x 6in      b4591-ch03                    page 123




                                           The Hypercube                                123

        Thus, moving k units of load from one processor to a neighboring processor
        takes k units of time. Suppose we have a hypercube with n = 2d processors
        such that each processor Pi has Li units of load. In the load balancing
        problem, it is requited to redistribute the load so that if Li is the load
        on processor Pi after the redistribution, then | Li − Lj | ≤ 1 for every pair
        of processors Pi and Pj . We are interested in balancing load as well as
        minimizing the load transfer time. The load can be balanced such that
        | Li − Lj | ≤ 1 for every pair of processors Pi and Pj by balancing across
        each of the d dimensions of the hypercube.
             Now, we describe the algorithm in connection with the example shown
        in Fig. 3.16. Consider an eight-processor hypercube with the initial load
        distribution shown in Fig. 3.16(a). We consider the dimensions of the
        hypercube in the order 2, 1, 0. When considering dimension 2, we ensure
        that the total loads of the two subhypercubes of size 4 diﬀer by at most
        one. The total load in the subhypercube {P0 , P1 , P2 , P3 } is 56, while that
        in the subhypercube {P4 , P5 , P6 , P7 } is 48. After balancing across dimen-
        sion 2, each of these subhypercubes will have a total load of 52. We use
        the embedding of a hypercube into a binary tree, as shown in Fig. 3.12.
        Thus, the processors will communicate using the binary tree of Fig. 3.16(b).
        Note that the tree levels are numbered 0, 1, 2, 3 such that the root is at
        level 0.
             First, we perform an upward pass starting from level 2 (the level above
        the leaves) up to the root. During this pass, level 2 and level 1 nodes com-
        pute the total load in the leaves of the subtrees of which they are the
        root. This gives the numbers next to each node at levels 2 and 1. Now,
        at the root, we compute the load diﬀerence δ between the two subhyper-
        cubes of size 4. Since δ = 8, 4 units of load have to be transferred from
        the hypercube {P0 , P1 , P2 , P3 } to the hypercube {P4 , P5 , P6 , P7 }. To get the
        actual processor-to-processor load transfer, we make a pass down the tree as
        shown by the downward arrows in Fig. 3.16(b). P0 on level 1 knows that its
        hypercube has to transfer 4 units of load to hypercube {P4 , P5 , P6 , P7 }. It
        attempts to do this by having each of its size-2 hypercubes transfer 4/2 = 2
        units. This is possible as one size-2 hypercube has 30 units and the other
        has 26. On level 2, P0 has to allocate a 2-unit data transfer from {P0 , P1 }
        and processor P2 has to allocate a 2-unit transfer from {P2 , P3 }. This is
        accomplished by having each of the processors {P0 , P1 , P2 , P3 } transfer 1
May 7, 2022   11:14              Parallel Algorithms                                                    9in x 6in                               b4591-ch03            page 124




        124                                                                        Parallel Algorithms

                       (a)
                                 P                   P                P                    P            P                 P                P              P
                                 0                    1                2                    3            4                 5               6              7
                                10               20                   14                   12           15                17               8              8
                                                                                                                                                              Level
                                                                                                         δ=8
                       (b)                                                                      P                                                               0
                                                                               4                0


                                                     56                                                                                                         1
                                                              P                                                           48       P
                                                 2            0            2                                                       4
                                                                                       26
                                 30                                                                                                                             2
                                         P                                     P                        32       P                         16       P
                                             0                                     2                             4                                  6
                                 1                    1               1                     1

                                 P                   P                 P                    P            P                 P               P              P     3
                                 0                    1                2                    3            4                 5               6              7
                                10                   20               14                   12           15                17               8              8


                                                               δ=4                                                                     δ = 16
                       (c)                               P                                                                     P
                                         2                0                                                      8             4
                                                                               24                                                                   18
                            28                                                                  34
                                     P                                    P                              P                                     P
                                     0                                     2                                 4                                  6
                        1                        1                                              4                     4

                            P                P                 P                    P               P                P             P                P
                            0                    1                2                    3            4                 5                6             7
                        9                19                   13                   11           16               18                9                 9


                      (d)
                                             δ = 10                                δ=2                               δ=2                             δ=0
                                     P                                    P                              P                                     P
                                     0                                     2                                 4                                  6
                                                 5            1                                                       1

                            P                P                 P                    P               P                P             P                P
                            0                    1                2                    3            4                 5                6             7
                        8                18                   14                   12           12               14                13                13

                      (e)
                        P                    P                P                    P            P                P                 P                P
                            0                1                 2                    3               4                5             6                7
                       13                13                   13                   13           13               13                13               13

              Fig. 3.16.         Load balancing algorithm on the 3-dimensional hypercube.


        unit of load to their neighboring processors across dimension 2, i.e, proces-
        sors P0 , P1 , P2 and P3 transfer 1 unit each to processors P4 , P5 , P6 and P7 ,
        respectively. During the load allocation downward pass, we are repeatedly
        in the situation shown in Fig. 3.17. Here, a and b are hypercube loads com-
        puted in the upward pass, w is the load to be allocated at this level by Pi ,
May 7, 2022   11:14     Parallel Algorithms               9in x 6in     b4591-ch03         page 125




                                          The Hypercube                              125


                                                            w


                                                      P
                                                      i
                                              x                     y

                                              P                 P
                                                  i             j
                                              a                 b

                          Fig. 3.17.     Load allocation downward pass.

                                                            w

                                                      P
                                                      i
                                                      a

                      Fig. 3.18.   The last iteration of the downward pass.

        and x + y = w. From the nature of the downward pass, we have w ≤ a + b.
        We would like to have x ≈ y, i.e., x = w/2 and y = w/2. This is pos-
        sible only if a ≥ w/2 and b ≥ w/2. In case a < w/2 , we set x = a,
        and y = w − x. In case b < w/2, we set y = b, and x = w − b. In the last
        iteration of the downward pass, the situation is shown in Fig. 3.18. Here, a
        is the current load in processor Pi , and w ≤ a. Processor Pi is to transfer
        w load units to its neighbor along the balancing dimension. Following this,
        the load in processor Pi is a − w.
            Now, in our example, the numbers below the leaf nodes of Fig. 3.16(c)
        give the load in each processor following the dimension 2 balancing. Next,
        we balance across dimension 1. For this, pairs of hypercubes with two pro-
        cessors each are considered. The hypercubes {P0 , P1 } and {P2 , P3 } balance
        load as do the hypercubes {P4 , P5 } and {P6 , P7 }. This is done in parallel.
        Figure 3.16(c) shows the two pass process. Processors P0 and P1 are to
        transfer one unit each to processors P2 and P3 , respectively, and processors
        P4 and P5 are to transfer 4 units each to processors P6 and P7 , respectively.
        The load in each processor after this load transfer is given below the leaves
        of Fig. 3.16(d). In the third and ﬁnal load balancing iteration, load is bal-
        anced in pairs of processors that diﬀer in bit 0. Figure 3.16(d) shows the
        computation. After the required load redistribution, each processor has 13
        units of load (Fig. 3.16(e)).
May 7, 2022    11:14       Parallel Algorithms          9in x 6in      b4591-ch03               page 126




        126                                      Parallel Algorithms

           The load balancing algorithm for general d is summarized in Algorithm
        hcloadbalance. There are d − 1 iterations, and each upward and down-
        ward pass of a tree of height r takes Θ(r) = O(log d) time. Hence, the
                                                                               d−1
        overall running time of the algorithm is O(d2 + m). Here, m = i=0 mi ,
        where mi is the maximum load transferred between a pair of processors
        when balancing along dimension i. From the above discussion, it follows
        that | Li − Lj | ≤ 1 for all i and j. Finally, we note that the load balancing
        problem can be solved in time O(log p + M ), where p is the number of
        processors and M is the maximum number of items at any processor before
        the distribution (see the bibliographic notes).

          Algorithm 3.8 hcloadbalance
          Input: A d-dimensional hypercube with loads Li , 0 ≤ i ≤ 2d .
          Output: Perform load balancing on the hypercube.
              1. for r ← d − 1 downto 0 do
              2.     Perform an upward pass computing the sum of loads in the
                     subtree leaves.
              3.     Perform a downward pass to compute the load to be transferred.
              4.     Transfer load.
              5. end for




        3.14       Computing Parallel Prefix on the Butterfly

        The parallel preﬁx problem was deﬁned in Section 2.5. In this section, we
        show how to compute this problem on the butterﬂy. For simplicity, we will
        assume addition as the binary operation. Recall from Section 3.2 that a
        complete binary tree with 2d leaves corresponding to level 0 processors, and
        rooted at (0, d) is a subgraph of the d-dimensional butterﬂy (see Fig. 3.5(b)).
        Assume that each processor has two registers: s and z. Register s at node v,
        denoted by s(v), contains the sum of all items at the leaves of the subtree
        rooted at v, and z(v) contains the sum of all items at the leaves of the
        subtree rooted at the left child of node v. Initially, the items x1 , x2 , . . . , xn
        are input to the n = 2d processors at level 0 in registers z and s. The
        algorithm consists of two passes: Bottom-up and top-down. It is shown as
        Algorithm bfparprefix.
           Obtaining the running time is straightforward; it is Θ(d) in both the
        bottom-up phase and the top-down phase.
May 7, 2022   11:14         Parallel Algorithms      9in x 6in        b4591-ch03                  page 127




                                              The Hypercube                                 127


                      (a)                               (b)




                      (c)                               (d)




        Fig. 3.19.    Example of computing parallel preﬁx on the 2-dimensional butterﬂy.


          Algorithm 3.9 bfparprefix
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers, where n = 2d .
          Output: S = s1 , s2 , . . . , sn , the preﬁx sums of X.
               (a) Bottom-up phase. See Fig. 3.19(a). Each leaf node l sends its item
                s(l) to its parent. Each internal node v upon receipt of two s-values s(x)
                and s(y) from its children x and y computes their sum and stores it in
                register s(v). It also stores s(x), the left child sum, in register z(v).
               (b) Top-down phase. See Figs. 3.19(b-d). Initially, the root sends 0 to its
                left child and z to its right child. Each node v upon receipt of value y
                from its parent does the following: If v is a leaf, it sets s(v) = s(v) + y;
                otherwise it sends y to its left child, and sends y + z(v) to its right child.
                At the end, the s value at the ith leaf contains si = x1 + x2 + · · · + xi ,
                1 ≤ i ≤ n.




        3.15      Odd–Even Merging and Sorting on the Butterfly

        In this section, we implement odd–even merging and sorting on the
        d-dimensional butterﬂy, where n = 2d ; odd–even merging and sorting on
        the PRAM were discussed in Section 2.11. Let A = a0 , a1 , . . . , an/2−1 and
        B = b0 , b1 , . . . , bn/2−1 be two sorted sequences of n/2 elements each. Ini-
        tially, A and B are input into level d of the d-dimensional butterﬂy, where
May 7, 2022   11:14              Parallel Algorithms          9in x 6in         b4591-ch03            page 128




        128                                            Parallel Algorithms


                      (a) level 3 level 2 level 1 level 0      (b) level 3 level 2 level 1 level 0
                            a0                                   000 a0      a0


                            a1                                            a1   a1
                                                                 001

                            a2                                            a2   a2
                                                                 010

                            a3                                   011 a3        a3


                            b0                                   100 b0        b1


                            b1                                   101 b1        b0


                            b2                                   110 b2        b3

                            b3                                   111 b3        b2


                      (c) level 3 level 2 level 1 level 0       (d) level 3 level 2 level 1 level 0
                                       c0                        000 e0        c0


                                       d0                                 e1   d0
                                                                 001

                                       c1                                 e2   c1
                                                                 010

                                       d1                        011 e3        d1


                                       c2                        100 e4        c2


                                       d2                        101 e5        d2


                                       c3                        110 e6        c3

                                       d3                        111 e7        d3

                                 Fig. 3.20.     Odd–even merging on the butterﬂy.


        the ai ’s are input to the lower half, and the bi ’s are input to the upper
        half (see Fig. 3.20(a)). The odd–even merging method is outlined in Algo-
        rithm bfoddevenmerge. It is important to note that the butterﬂy has
        a recursive structure; the even rows of the d-dimensional butterﬂy and the
May 7, 2022    11:14          Parallel Algorithms         9in x 6in        b4591-ch03                      page 129




                                                The Hypercube                                        129


          Algorithm 3.10 bfoddevenmerge
          Input: Two sorted sequences A = a0 , a1 , . . . , an/2−1  and
                 B = b0 , b1 , . . . , bn/2−1  of n/2 elements each, where n = 2d . A and B
                 are stored in level d of the butterﬂy.
          Output: The elements in S = A ∪ B in sorted order.
              1. if n = 2 then merge the two elements and exit. e Move the ai ’s to level
                 d−1 along the straight edges and the bi ’s to level d−1 along the cross edge
                 as shown in Fig. 3.20(b). This is equivalent to partitioning the input into
                 Aeven , Aodd , Beven , and Bodd , and storing them in the (d − 1)-dimensional
                 butterﬂies.
              2. Recursively merge Aeven and Bodd to obtain C = c0 , c1 , . . . , cn/2−1  using
                 the even (d − 1)-dimensional butterﬂy.
              3. Recursively merge Aodd and Beven to obtain D = d0 , d1 , . . . , dn/2−1 
                 using the odd (d − 1)-dimensional butterﬂy.
              4. Let E  be the shuﬄe of C and D, that is,
                 E  = c0 , d0 , c1 , d1 , . . . , cn/2−1 , dn/2−1 . Starting from c0 in E  , compare
                 each ci with the following di , and switch them if they are out of order to
                 obtain the sorted sequence E = e0 , e1 , . . . , en−1 .
              5. return S = E.



        odd rows contain a (d − 1)-dimensional butterﬂy each (refer to Fig. 3.4).
        These two (d − 1)-dimensional butterﬂies will henceforth be referred to as
        the even and odd butterﬂies. In Fig. 3.20(b), the even (d − 1)-dimensional
        butterﬂy and the odd (d − 1)-dimensional butterﬂy are shown with thick
        and dashed lines, respectively.
            The ﬁrst step in the algorithm is to move the ai ’s to level d− 1 along the
        straight edges and the bi ’s to level d − 1 along the cross edges in one step
        as shown in Fig. 3.20(b). This is equivalent to partitioning the input into
        Aeven , Aodd , Beven , and Bodd and storing them in the (d − 1)-dimensional
        butterﬂies. Next, the algorithm recursively merges Aeven with Bodd to pro-
        duce C, and recursively merges Beven with Aodd to produce D using the
        even and odd (d − 1)-dimensional butterﬂies, respectively (see Figs. 3.20(b)
        and (c)). Here C = c0 , c1 , . . . , cn/2−1 and D = d0 , d1 , . . . , dn/2−1 . Let E 
        be the shuﬄe of C and D, that is, E  = c0 , d0 , c1 , d1 , . . . , cn/2−1 , dn/2−1
        (see Fig. 3.20(c)). E  is scanned from left to right (in one step) for pairs
        that are out of order, which are ordered, if necessary. In other words,
        starting from c0 , compare each ci with the following di , and switch them
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch03              page 130




        130                                    Parallel Algorithms

        if they are out of order. This is accomplished by letting the even rows
        at level d compute min{ci , di } and the odd rows at level d compute
        max{ci , di }. The result of the comparisons and exchanges, which is the
        sequence E = e0 , e1 , . . . , en−1 is then stored in level d of the d-dimensional
        butterﬂy as the desired sorted sequence (see Fig. 3.20(d)).
            The analysis of the algorithm is straightforward. Step 1 takes Θ(1)
        time. Steps 2 and 3 take T (n/2) time each. Step 4 takes Θ(1) time.
        Hence, the running time of the algorithm is governed by the recurrence
        T (n) = T (n/2) + Θ(1), whose solution is T (n) = Θ(log n). The proof of
        correctness is given by Theorem 2.2 in Section 2.11.

        Example 3.4 Consider merging the two sorted sequences A = 1, 3, 5, 8
        and B = 2, 4, 6, 7 on the 3-dimensional butterﬂy shown in Fig. 3.21(a).
        The ai ’s are ﬁrst moved to level 2 along the straight edges and the bi ’s
        are moved to level 2 along the cross edge as shown in Fig. 3.20(b). This
        is equivalent to partitioning the input into Aeven , Aodd , Beven and Bodd
        and storing them in the 2-dimensional butterﬂies. Thus, Aeven = {1, 5},
        Aodd = {3, 8}, Beven = {2, 6} and Bodd = {4, 7}. Aeven and Bodd are
        merged recursively as well as Aodd and Beven in the 2-dimensional butter-
        ﬂies, and the two sequences C = 1, 4, 5, 7 and D = 2, 3, 6, 8 are formed as
        shown in Figs. 3.21(b) and (c). Finally, the elements in each pair (ci , di ) in
        the sequence E  = 1, 2, 4, 3, 5, 6, 7, 8 , which is the shuﬄe of C and D, are
        compared and exchanged if they are out of order as shown in Fig. 3.21(d).
        The pair (4, 3) is out of order, so 4 and 3 are interchanged. The sorted
        sequence is E = 1, 2, 3, 4, 5, 6, 7, 8 as shown in Fig. 3.21(d).              

           The algorithm for sorting is given as Algorithm bfoddevenmerge-
        sort. It is similar to Algorithm oddevenmergesort for the PRAM in
        Section 2.11 Initially, the input sequence is input to level d of the butterﬂy.
May 7, 2022   11:14              Parallel Algorithms        9in x 6in           b4591-ch03                page 131




                                                   The Hypercube                                    131


                      (a) level 3 level 2 level 1 level 0     (b) level 3 level 2 level 1 level 0
                            1                                       1        1

                             3                                          3   3

                             5                                      5       5

                             8                                          8   8


                             2                                      2       4

                             4                                      4       2

                             6                                      6       7

                             7                                          7   6

                      (c) level 3 level 2 level 1 level 0     (d) level 3 level 2 level 1 level 0
                       000           1                         000 1         1

                                       2
                       001                                     001 2        2


                       010             4                       010 3        4

                       011             3                       011 4        3


                       100             5                       100 5        5

                       101             6                       101 6        6

                       110             7                       110 7        7

                       111             8                       111 8        8

                      Fig. 3.21.        Example of odd–even merging on the butterﬂy.
May 7, 2022    11:14          Parallel Algorithms          9in x 6in      b4591-ch03   page 132




        132                                         Parallel Algorithms

        First, the algorithm recursively sorts each half separately using the two
        (d − 1)-dimensional butterﬂies in Steps 3 and 4. Next, the two sorted halves
        are merged using Algorithm bfoddevenmerge in Step 5.

          Algorithm 3.11 bfoddevenmergesort
          Input: A sequence S = a0 , a1 , . . . , an−1  where n = 2d .
          Output: The elements of S in sorted order.
              1.   S1 ← a0 , a1 , . . . , an/2−1 .
              2.   S2 ← an/2 , an/2+1 , . . . , an−1 .
              3.   S1 ← bfoddevenmergesort(S1 )
              4.   S2 ← bfoddevenmergesort(S2 )
              5.   S ← bfoddevenmerge(S1 , S2 )
              6.   return S



            The running time of the algorithm is governed by the recurrence T (n) =
        T (n/2) + Θ(log n), whose solution is T (n) = Θ(log2 n).


        3.16        Matrix Multiplication on the Hypercube

        Consider the problem of matrix multiplication on the hypercube: Given two
        square matrices A and B of order n × n, ﬁnd their product C = A × B.
        Note that the matrices are indexed from 0 to n − 1. Thus, the matrix A
        has the form:
                             ⎡                                 ⎤
                                a0,0    a0,1    ...    a0,n−1
                             ⎢                                 ⎥
                             ⎢ a1,0     a1,1    ...    a1,n−1 ⎥
                             ⎢                                 ⎥
                        A=⎢  ⎢ ..        ..               ..
                                                               ⎥.
                                                               ⎥
                             ⎢ .          .                .   ⎥
                             ⎣                                 ⎦
                               an−1,0 an−1,1 . . . an−1,n−1
May 7, 2022   11:14         Parallel Algorithms            9in x 6in          b4591-ch03                    page 133




                                                The Hypercube                                        133


            Assume that there are n3 = 23r processors P0 , P1 , . . . , Pn3 −1 . The pro-
        cessors will also be referred to by the triple (l, i, j), 0 ≤ l, i, j ≤ n − 1. So,
        if the index of a processor has the binary representation b3r−1 b3r−2 . . . b0 ,
        then the binary representations of l, i and j are

                      b3r−1 b3r−2 . . . b2r ,     b2r−1 b2r−2 . . . br ,     br−1 br−2 . . . b0 ,

        respectively. In particular, if we ﬁx any index l, i, or j, and vary the remain-
        ing indices over all its possible values, we obtain a subcube of dimension 2r,
        and if we ﬁx any pair of indices l, i, and j, and vary the remaining index
        over all its possible values, we obtain a subcube of dimension r.
            Initially, the input elements of A and B are distributed over the n2
        processors P (0, i, j), 0 ≤ i, j ≤ n − 1, so that A(0, i, j) = ai,j and
        B(0, i, j) = bi,j . There are three registers associated with every processor
        P (l, i, j), namely A(l, i, j), B(l, i, j) and C(l, i, j). The desired ﬁnal conﬁg-
        uration is

                                   C(0, i, j) = ci,j ,         0 ≤ i, j ≤ n − 1,

        where

                                                         
                                                         n−1
                                                ci,j =         ai,l bl,j .                          (3.2)
                                                         l=0

        The algorithm computes the product matrix C by directly making use
        of (3.2). The algorithm has three phases. In the ﬁrst phase, elements of A
        and B are redistributed over the n3 processors so that we have A(l, i, j) =
        ai,l and B(l, i, j) = bl,j . In the second phase, the products C(l, i, j) =
        A(l, i, j) × B(l, i, j) = ai,l bl,j are computed. Finally, in the third phase, the
               n−1
        sums l=0 Cl,i,j are computed. An outline of the algorithm is shown as
        follows.
May 7, 2022    11:14        Parallel Algorithms          9in x 6in      b4591-ch03                 page 134




        134                                       Parallel Algorithms


              1. For 0 ≤ l ≤ n − 1, set A(l, i, j) ← A(0, i, j) and B(l, i, j) ← B(0, i, j).
              2. Set A(l, i, j) ← A(l, i, l), 0 ≤ j ≤ n − 1.
              3. Set B(l, i, j) ← B(l, l, j), 0 ≤ i ≤ n − 1.
              4. For each 0 ≤ i, j ≤ n − 1, processor P (l, i, j) computes the product
                 C(l, i, j) ← A(l, i, j) × B(l, i, j).
              5. For each 0 ≤ i, j ≤ n − 1, processors
                                                              P (l, i, j), where 0 ≤ l ≤ n − 1,
                 compute the sum C(0, i, j) = n−1      l=0 C(l, i, j).



            In Step 1, the contents of registers A and B in the processors of subcube
        l = 0 are broadcast to all other processors. In Step 2, a copy of the contents
        of register A of each processor in column l is sent to all processors in the
        same row, and in Step 3, a copy of the contents of register B of each proces-
        sor in row l is sent to all processors in the same column. Thus, after Step 2
        is completed, A(l, i, j) = ai,l , and after Step 3 is completed, B(l, i, j) = bl,j .
                                     
        Step 5 performs the sum n−1      l=0 C(l, i, j). This is a typical hypercube sum
        operation for each pair (i, j) applied on subcubes with processors P (l, i, j),
        0 ≤ l ≤ n − 1 (see Algorithm hcsum in Section 3.5).

        Example 3.5 Consider multiplying the two 4×4 matrices on a hypercube
        with n = 26 = 64 processors, where
               ⎡                   ⎤          ⎡                        ⎤
                  1 2 3 4                        −1 −2 −3 −4
               ⎢ 5 6 7 8 ⎥                    ⎢ −5 −6 −7 −8 ⎥
           A=⎢                     ⎥          ⎢
               ⎣ 9 10 11 12 ⎦ and B = ⎣ −9 −10 −11 −12 ⎦ .
                                                                       ⎥

                 13 14 15 16                    −13 −14 −15 −16
            Figure 3.22(a) shows the initial input stored in registers A and B (Only
        the ﬁrst 16 processors are shown in the ﬁgure). Figure 3.22(b) shows the
        result of applying Step 2 of the algorithm’s outline. As shown in the ﬁgure,
        the A registers in the ﬁrst 16 processors contain the ﬁrst column of matrix A,
        that is, A(0, i, j) = ai,0 . Figure 3.22(c) shows the result of applying Step 3
        of the algorithm’s outline. As shown in the ﬁgure, the B registers in the ﬁrst
        16 processors contain the ﬁrst row of matrix A, that is, B(0, i, j) = b0,j . 

           The details of the algorithm are given as Algorithm hcmatrixmult. In
        the algorithm, S(bk = d) denotes the set of processor labels t, 0 ≤ t ≤ n3 −1,
May 7, 2022   11:14           Parallel Algorithms             9in x 6in            b4591-ch03             page 135




                                                    The Hypercube                                   135


                       (a)
                                  15                16                     7                8
                                 -15 001110 001111 -16                    -7 000110 000111 -8

                        13                     14                5     000100          6
                       -13             001101 -14               -5                    -6   000101
                              001100
                                  11                12                     3                   4
                                 -11 001010 001011 -12                    -3           000011 -4
                                                                                 000010

                         9                     10                1                      2
                        -9                    -10               -1                     -2
                      001000                001001            000000                 000001

                       (b)

                                 13                      13               5                     5
                                       001110 001111                            000110 000111

                       13 001100               13               5    000100           5    000101
                                      001101

                                  9                      9                1                     1
                                       001010 001011                                  000011
                                                                                000010

                        9                       9               1                      1
                      001000                001001            000000                000001

                       (c)


                                  -3 001110 001111 -4                     -3 000110 000111 -4

                                                                       000100
                        -1             001101 -2                -1                    -2   000101
                              001100

                                   -3 001010 001011 -4                    -3           000011 -4
                                                                                 000010


                        -1                      -2              -1                    -2
                      001000                   001001         000000                 000001

                 Fig. 3.22.     Illustration of matrix multiplication on the hypercube.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch03            page 136




        136                                     Parallel Algorithms

        whose binary representation is b3r−1 . . . bk+1 dbk−1 . . . b0 . For instance, in
        the algorithm, S(bk = 0) means all labels t with binary representation
        b3r−1 . . . bk+1 0bk−1 . . . b0 . The notation t(k) means t with the kth bit com-
        plemented. For example, if t = 001011, then t(4) = 011011.

          Algorithm 3.12 hcmatrixmult
          Input: Two n × n matrices A and B.
          Output: The product C = A × B.
           1.   for k ← 3r − 1 downto 2r do
           2.       for all t ∈ S(bk = 0) do in parallel
           3.           At(k) ← At
           4.           Bt(k) ← Bt
           5.       end for
           6.   end for
           7.   for k ← r − 1 downto 0 do
           8.       for all t ∈ S(bk = b2r+k ) do in parallel
           9.           At(k) ← At
          10.       end for
          11.   end for
          12.   for k ← 2r − 1 downto r do
          13.       for all t ∈ S(bk = br+k ) do in parallel
          14.           Bt(k) ← Bt
          15.       end for
          16.   end for
          17.   for t ← 0 to n3 − 1 do in parallel
          18.       Ct ← At × Bt
          19.   end for
          20.   for k ← 2r to 3r − 1 do
          21.       for all t ∈ S(bk = 0) do in parallel
          22.           Ct ← Ct + Ct(k)
          23.       end for
          24.   end for



           The running time is computed as follows: the for loops in Steps 1, 7,
        and 12 are iterated Θ(r) = Θ(log n) times. Steps 17–19 take Θ(1) time, and
        Steps 20–24 take Θ(log n) time. Hence, the overall running time is Θ(log n).
        The total cost of the algorithm is n3 × Θ(log n) = Θ(n3 log n), which is not
        optimal.
May 7, 2022   11:14             Parallel Algorithms             9in x 6in             b4591-ch03                         page 137




                                                  The Hypercube                                                    137


                 (a)      110                             111     (b)       110   3                      4   111
                                                                                  7                      8

                                               101                      1                   2      101
              100                                               100
                                                                        5                   6


                                 3                    4   011                     3                      4   011
                          010                                               010
                                 7                    8                           7                      8

                      1                    2                            1                    2
              000                               001             000                                001
                      5                    6                            5                    6


                 (c)      110    4                    4   111     (d)       110   4                      4   111
                                 7                    8                           7                      8

              100     2                    2   101                      2                   2      101
                                           6                    100
                      5                                                 7                   8


                                 3                    3   011                     3                      3
                          010                                               010                              011
                                 7                    8                           5                      6
                      1                    1                            1                    1
              000                               001             000                                001
                      5                    6                            5                    6


                 (e)      110   28                    32 111      (f)       110                              111


              100 14                      16 101                100                                101



                          010   15                    18 011                010 43                       50 011


              000     5                    6    001             000 19                      22 001


                    Fig. 3.23.       Example of matrix multiplication on the hypercube.



        Example 3.6 Figure 3.23 shows an example of running Algorithm hcma-
        trixmult on the two matrices
                                                    
                               1 2                5 6
                          A=           and B =           .
                               3 4                7 8
May 7, 2022   11:14     Parallel Algorithms          9in x 6in      b4591-ch03        page 138




        138                                   Parallel Algorithms


        There are n = 23 = 8 processors. Figure 3.23(a) shows the initial input.
        Figures. 3.23(b)–(d) show the results of applying Steps 1–14 of the algo-
        rithm. The products of A and B registers are shown in Fig. 3.23(e), and
        the sum of these products is shown in Fig. 3.23(f).                    


        3.17      Bibliographic Notes

        There are a number of books that cover parallel algorithms on the hyper-
        cube. These include Akl [4], Akl [5], Cosnard and Trystram [29], Grama,
        Gupta, Karypis and Kumar [39], Horowitz, Sahni and Rajasekaran [43],
        Lakshmivarahan and Dhall [52], Leighton [57], and Miller and Boxer [66].
        An assortment of matrix problems can be found/is embedded in
        Lakshmivarahan and Dhall [52]. For a survey of parallel sorting and selec-
        tion algorithms, see Rajasekaran [75]. Parallel algorithms for many prob-
        lems, including problems in computational geometry on the mesh can be
        found in Leighton [57]. Randomized routing in the hypercube and the but-
        terﬂy are based on Valiant[100] and Valiant and Brebner[101]. Selection on
        the hypercube is from Chandran and Rosenfeld [20]. Hyperquicksort is due
        to Wagar[93]. Sample sort is from Shi and Schaeﬀer [86]. Multiselection
        on the hypercube is from Shen [83] and Shen [84]. The O(log n log log n)
        time algorithm for sorting on the hypercube can be found in Cypher and
        Plaxton [30]. The load balancing algorithm is from Woo and Sahni [99].
        The load balancing problem can be solved in time O(log p + M ), where p
        is the number of processors and M is the maximum number of items at
        any processor before the distribution. For more on load balancing, see Jan
        and Huang [45], JáJá and Ryu [46], and Plaxton [74]. Parallel matrix mul-
        tiplication on the hypercube is due to Dekel, Nassimi and Sahni [33]. For
        more references on parallel algorithms on the hypercube interconnection
        network, see for instance Leighton [57].

        3.18      Exercises

         3.1. Give an O(d) time algorithm for broadcasting in the d-dimensional
              hypercube Hd if the origin of the message is an arbitrary processor.

         3.2. Design a recursive algorithm to compute the sum of n numbers on
              the hypercube with n = 2d processors. What is the time complexity
              of your algorithm?
May 7, 2022   11:14      Parallel Algorithms       9in x 6in        b4591-ch03                    page 139




                                           The Hypercube                                    139

         3.3. Design a recursive algorithm to compute the preﬁx sums of n num-
              bers on the hypercube with n = 2d processors. What is the time
              complexity of your algorithm?

         3.4. Describe how to implement the odd–even merge sort on a hypercube
              of dimension d.

         3.5. Design an algorithm to rearrange a sequence of n numbers dis-
              tributed one number per processor in a d-dimensional hypercube,
              where n = 2d , so that all numbers smaller than or equal to the aver-
              age precede all numbers greater than the average. Your algorithm
              should run in Θ(log n) time.

         3.6. Explain how to compute the preﬁx sums of n numbers on a hypercube
              with p processors, where p < n. What is the running time of your
              algorithm?

         3.7. Explain how to run the algorithm for quicksort designed for the
              PRAM and discussed in Section 2.5.2 on the hypercube with n
              processors.

         3.8. Illustrate the operation of Algorithm samplesort discussed in
              Section 3.10 on the input

                       18, 12, 23, 14, 15, 16, 7, 21, 20, 19, 11, 2, 24, 14, 5, 6, 17, 1,

                 where n = 18 and p = 3.

         3.9. In Algorithm samplesort discussed in Section 3.10, each processor
              sends its sample of (p − 1) elements to P0 , which in turn collects a
              sample of p(p − 1) elements. Explain how this data transmission can
              be achieved, and analyze its cost.

        3.10. A sorting method known as bucketsort works as follows. Let S
              be a sequence of n numbers within a reasonable range, say all num-
              bers are between 1 and m, where m is not too large compared to n.
              The numbers are distributed into k buckets, with the ﬁrst bucket
              containing those numbers between 1 and m/k, the second bucket
              containing those numbers between m/k + 1 to 2m/k, and so on.
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch03         page 140




        140                                    Parallel Algorithms

                 The numbers in each bucket are then sorted using an optimal sorting
                 algorithm. Show how to parallelize the algorithm.

        3.11. Analyze the running time of Algorithm bucketsort in the solution
              to Exercise 3.10.

        3.12. Consider the algorithm for permutation routing in the hypercube
              discussed in Section 3.6. What is the probability that the algorithm
              will route all packets to their destinations in 8d steps or fewer?

        3.13. Consider Algorithm hcselect for selection on the hypercube dis-
              cussed in Section 3.11. For what values of p is the algorithm optimal?

        3.14. Consider Algorithm parselect for selection on the EREW PRAM
              presented in Section 2.14. Suppose we simulate this algorithm to
              run on the hypercube with n/ log n processors. What will be the
              running time of the algorithm? Compare this with that of Algorithm
              hcselect for selection on the hypercube presented in Section 3.11.
              (See Exercise 3.21).

        3.15. Outline an algorithm to ﬁnd all the kth largest elements in a hyper-
              cube with p < n processors. What is the running time of your
              algorithm?

        3.16. Consider Algorithm hcmultiselect for multiselection on the hyper-
              cube discussed in Section 3.12. For what values of r is the algorithm
              cost optimal when the number of processors is n1− ?

        3.17. Consider Algorithm hcmultiselect for multiselection on the hyper-
              cube discussed in Section 3.12. Compare the algorithm given with
              direct application of Algorithm hcselect given in Section 3.11.

        3.18. Construct the Gray code sequence G4 .

        3.19. Consider the two graphs shown in Fig. 3.24. Find an embedding of G
              into H. What are the dilation, congestion, expansion and load of your
              embedding?

        3.20. Give an embedding similar to the one given in Example 3.2, except
              that it is postorder, that is, the nodes of the binary tree are labeled
May 7, 2022   11:14       Parallel Algorithms        9in x 6in           b4591-ch03         page 141




                                            The Hypercube                             141


                                                                     y
                                   d            c


                                                                 z



                                  a             b                        x
                                                             w
                                        G                        H

                                        Fig. 3.24.   Exercise 3.19.


                 in postorder traversal. What are the dilation, congestion, expansion
                 and load of the embedding?

        3.21. Explain how to simulate an EREW PRAM on a hypercube with n
              processors.

        3.22. Compute the bisection width of the d-dimensional butterﬂy Bd .

        3.23. Design an algorithm to compute the sum of n numbers on the hyper-
              cube with p processors, 1 ≤ p < n. Is your algorithm always optimal?

        3.24. Explain how to compute the maximum of 2d−1 numbers distributed
              arbitrarily in a hypercube with 2d processors. What is the running
              time of your algorithm?

        3.25. Consider the partial permutation routing problem on the hypercube
              in which every processor is the source of at most one packet and the
              destination of at most one packet. Will Theorem 3.2 hold for this
              routing problem?

        3.26. Consider the many-to-many routing problem on the hypercube in
              which every processor is the source of r packets and the destination
              of r packets. Suppose we run the greedy algorithm for routing on the
              hypercube to solve this problem. What will be the maximum queue
              size?

        3.27. Give an O(nd) time algorithm for the problem of routing in the
              d-dimensional hypercube if every processor has a packet to be sent
              to every other processor, where n = 2d . Hint : Use randomized routing
              n times.
May 7, 2022   11:14      Parallel Algorithms              9in x 6in       b4591-ch03         page 142




        142                                    Parallel Algorithms


        3.28. Give an O(n) time algorithm for the problem in Exercise 3.27.

        3.29. Apply Algorithm hcparprefix for computing parallel preﬁx on the
              hypercube on the input sequence 1, 2, 3, 4, 5, 6, 7, 8 . Assume a hyper-
              cube with 8 processors.

        3.30. Give an algorithm to evaluate the polynomial an−1 xn−1 +an−2 xn−2 +
              · · · + a1 x + a0 at the point x0 on the d-dimensional hypercube Hd
              with n = 2d processors. Assume that each ai is stored in processor Pi ,
              0 ≤ i ≤ n − 1.

        3.31. Consider a hypercube with four processors {P0 , P1 , P2 , P3 } with ini-
              tial loads 8, 2, 6, 4. Perform load balancing on the hypercube so that,
              at the end, each processor has the same load.

        3.32. Redo Exercise 3.31 using the algorithm presented in Section 3.13.

        3.33. Consider a hypercube with eight processors {P0 , P1 , . . . , P7 } with ini-
              tial loads 8, 5, 6, 4, 7, 2, 5, 3. Perform load balancing on the hypercube
              using the algorithm described in Section 3.13 so that, at the end, each
              processor has approximately the same load.

        3.34. Suggest a heuristic to improve the performance of the load balancing
              algorithm discussed in Section 3.13.

        3.35. Illustrate the operation of Algorithm bfoddevenmerge for merging
              on the butterﬂy to merge the two sorted sequences A = 1, 4, 6, 9
              and B = 2, 5, 7, 8 on the 3-dimensional butterﬂy.

        3.36. Use the matrix multiplication algorithm on the hypercube discussed
              in Section 3.16 to compute the product C = A × B of the two 2 × 2
              matrices A and B shown below. Assume a hypercube with n = 23 = 8
              processors.
                                                                    
                                       1          3                2 1
                                    A=                     and B =       .
                                       2          4                4 3

        3.37. Suggest an algorithm for computing the transitive closure of an adja-
              cency matrix A on the hypercube. What is the running time of the
              algorithm?
May 7, 2022   11:14       Parallel Algorithms             9in x 6in              b4591-ch03         page 143




                                              The Hypercube                                   143

        3.38. Suggest an algorithm for computing the shortest paths in a directed
              graph G represented by its adjacency matrix A on the hypercube.
              What is the running time of the algorithm?

        3.39. Let S = x0 , x1 , . . . , xn−1 be a sequence of numbers stored in a
              hypercube with n processors where xi is stored in Pi , 0 ≤ i < n,
              and let y be stored in P0 . Give an algorithm to count the number of
              elements in S that are larger than y.

        3.40. Following the example shown in Fig. 3.19, show how to compute the
              preﬁx sums of the sequence 1, 2, 3, 4 on the 2-dimensional butterﬂy.


        3.41. The d-dimensional cube-connected cycles (CCC) is constructed from
              the d-dimensional hypercube by replacing each node with a cycle of
              length d (see Fig. 3.25). The nodes in the cycle corresponding to
              node x in the hypercube are labeled as (x, 1), (x, 2), . . . , (x, d). Node
              (x, i) is connected to node (y, j) if and only if x = y and | i − j | = 1
              (mod d) or i = j and x and y are connected in the corresponding
              hypercube. The CCC has d2d nodes. Derive an algorithm to ﬁnd
              the sum of n = d2d numbers stored in the CCC, one number per
              processor. The resulting sum should be stored in processor P(0d ,1) .


                                                     (110,1)        (111,1)
                                        (110,2)                                    (111,3)
                                                                   (111,2)
                                                     (110,3)
                              (100,2)     (100,1)                     (101,2)
                                                     (101,1)
                           (100,3)        (010,3)                   (101,3)        (011,3)

                                        (010,2)                    (011,1)
                                                      (010,1)                    (011,2)

                                           (000,2)     (001,3)
                           (000,3)                                     (001,2)

                                      (000,1)            (001,1)

                      Fig. 3.25.     3-dimensional cube-connected cycles (CCC).
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch03          page 144




        144                                     Parallel Algorithms

        3.42. What are the degree and diameter of the d-dimensional cube-
              connected cycles described in Exercise 3.41?

        3.43. What is the bisection width of the d-dimensional cube-connected
              cycles described in Exercise 3.41?

        3.44. Give an algorithm for computing the preﬁx sums on the
              d-dimensional cube-connected cycles described in Exercise 3.41. Your
              algorithm should run in O(d) = O(log n) time.

        3.45. Give an embedding function from the d-dimensional hypercube to
              the d-dimensional cube-connected cycles(CCC) network described in
              Exercise 3.41. What is the dilation of the embedding?

        3.46. What is the congestion of the embedding in Exercise 3.45?

        3.47. Explain how to simulate a hypercube with 2d processors on the cube-
              connected cycles with d2d processors described in Exercise 3.41.


        3.19      Solutions

         3.1. Give an O(d) time algorithm for broadcasting in the d-dimensional
              hypercube Hd if the origin of the message is an arbitrary processor.

                 Let Pi be the origin of broadcasting datum x. First, transfer x from
                 Pi to P0 using bit ﬁxing in O(d) steps, then broadcast it to all other
                 processors in O(d) time as shown in Algorithm hcbroadcast.

         3.2. Design a recursive algorithm to compute the sum of n numbers on
              the hypercube with n = 2d processors. What is the time complexity
              of your algorithm?

                 Let the two halves of the hypercube be 0Hd−1 and 1Hd−1 , where
                 0Hd−1 is the subcube with 0 leading binary digits in its labels, and
                 1Hd−1 is the subcube with 1 leading binary digits in its labels. The
                 idea is to store the sum in all processors. Every processor has a
                 register t for storing the (partial) sums. See Algorithm hcsumrec.
                 The running time is given by the recurrence T (n) = T (n/2)+ Θ(1) =
                 Θ(log n).
May 7, 2022    11:14        Parallel Algorithms      9in x 6in      b4591-ch03                    page 145




                                              The Hypercube                                 145


          Algorithm 3.13 hcsumrec
          Input: n numbers x1 , x2 , . . . , xn stored in Hd , one element per processor.
          Output: The sum of the numbers stored in all processors of Hd .
              1. if n = 1 then set the t register to the x-value and exit
              2. Recursively ﬁnd the sum in each subcube
              3. Each processor with label 0y in 0Hd−1 adds the content of its t register
                 to the t register of processor with label 1y in subcube 1Hd−1 .
              4. Processor with label 1y in subcube 1Hd−1 copies the content of its t
                 register back to the t register of processor with label 0y in subcube 0Hd−1 .



         3.3. Design a recursive algorithm to compute the preﬁx sums of n num-
              bers on the hypercube with n = 2d processors. What is the time
              complexity of your algorithm?

                  The idea is similar to that in the solution to Exercise 3.2. We compute
                  both the preﬁx sums and the sum of the numbers simultaneously.
                  Deﬁne 0Hd−1 and 1Hd−1 as in the solution of Exercise 3.2. Every
                  processor Pi has two registers si for storing the preﬁx sums and ti for
                  storing the (partial) sums. See Algorithm hcprefixsumrec. After
                  Step 4, all processors have the preﬁx sums and the same total. The
                  running time is given by the recurrence T (n) = T (n/2) + Θ(1) =
                  Θ(log n).



          Algorithm 3.14 hcprefixsumrec
          Input: n numbers x1 , x2 , . . . , xn stored in Hd , one element per processor.
          Output: The preﬁx sums of s1 , s2 , . . . , sn .
              1. if n = 1 then copy the x-value to registers s and t and exit
              2. Recursively ﬁnd the preﬁx sums in each subcube
              3. Each processor Pi with label 0y adds the content of its ti register to
                 register sj of processor Pj with label 1y.
              4. Each processor Pi with label 0y adds the content of its ti register to
                 register tj of processor Pj with label 1y.
              5. Processor 1y in subcube 1Hd−1 copies the content of its t register back
                 to the t register of processor 0y in subcube 0Hd−1 .
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch03               page 146




        146                                      Parallel Algorithms

         3.4. Describe how to implement the odd–even merge sort on a hypercube
              of dimension d.
                 We adapt Algorithm bfoddevenmerge for the butterﬂy discussed
                 in Section 3.15. The algorithm is normal for the butterﬂy since at any
                 given time, only processors in the same level are participating in the
                 computation, which means a single step of the butterﬂy algorithm
                 can be simulated in one step of the hypercube. Hence, the algo-
                 rithm can be implemented to run on the hypercube in time Θ(d2 ) =
                 Θ(log2 n).

         3.5. Design an algorithm to rearrange a sequence of n numbers distributed
              one number per processor in a d-dimensional hypercube, where n =
              2d , so that all numbers smaller than or equal to the average precede
              all numbers greater than the average. Your algorithm should run in
              Θ(log n) time.
                 This is a direct application of parallel preﬁx. First, ﬁnd the sum,
                 divide it by n to obtain the average v. Next, broadcast v to all pro-
                 cessors in the hypercube. Label all elements ≤ v with 1 and the
                 others with 0. Finally, apply packing and route each element to its
                 proper location.

         3.6. Explain how to compute the preﬁx sums of n numbers on a hypercube
              with p processors, where p < n. What is the running time of your
              algorithm?
                 Divide the input into p groups of n/p elements each. Find the preﬁx
                 sums individually and sequentially in each group in Θ(n/p) time. Let
                 the ﬁnal preﬁx sums (the totals of all groups) be S = s1 , s2 , . . . , sp .
                 Apply parallel preﬁx on the sequence S in Θ(log p) time. Finally,
                 update the preﬁx sums in all groups sequentially in Θ(n/p) time.
                 The overall running time is Θ(n/p + log p) time.

         3.7. Explain how to run the algorithm for quicksort designed for the
              PRAM and discussed in Section 2.5.2 on the hypercube with n
              processors.
                 The algorithm runs on the hypercube with no modiﬁcations.
May 7, 2022   11:14       Parallel Algorithms       9in x 6in        b4591-ch03                    page 147




                                            The Hypercube                                    147

         3.8. Illustrate the operation of Algorithm samplesort discussed in
              Section 3.10 on the input

                        18, 12, 23, 14, 15, 16, 7, 21, 20, 19, 11, 2, 24, 14, 5, 6, 17, 1,

                 where n = 18 and p = 3.
                 Similar to Example 3.3.

         3.9. In Algorithm samplesort discussed in Section 3.10, each processor
              sends its sample of (p − 1) elements to P0 , which in turn collects a
              sample of p(p − 1) elements. Explain how this data transmission can
              be achieved, and analyze its cost.
                 Each odd-numbered processor sends its sample to its (even-
                 numbered) neighbor. Next, each even-numbered processor combines
                 the sample it has received from its neighbor with its own sam-
                 ple. This process of sending to neighbors continues until proces-
                 sor P0 receives all the p(p − 1) samples. The total time taken is
                 (p − 1) + 2(p − 1) + 4(p − 1) + · · · + 2log p (p − 1) = Θ(p2 ).

        3.10. A sorting method known as bucketsort works as follows. Let S
              be a sequence of n numbers within a reasonable range, say all num-
              bers are between 1 and m, where m is not too large compared to n.
              The numbers are distributed into k buckets, with the ﬁrst bucket
              containing those numbers between 1 and m/k, the second bucket
              containing those numbers between m/k + 1 to 2m/k, and so on.
              The numbers in each bucket are then sorted using an optimal sorting
              algorithm. Show how to parallelize the algorithm.
                 Let the number of processors be p, and set the number of buckets
                 k = p. Assign n/p elements to each processor. Each processor par-
                 titions its assigned elements into p partitions, one for each of the p
                 buckets. Next, each processor sends each part of its bucket to the
                 appropriate processor, and retains its part. Each processor then com-
                 bines the p − 1 parts received from the other p − 1 processors with
                 its retained elements. Finally, each processor sorts its items using
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch03            page 148




        148                                     Parallel Algorithms

                 an optimal sequential sorting algorithm. Note that we have assumed
                 that the processors know the interval [1..m].

        3.11. Analyze the running time of Algorithm bucketsort in the solution
              to Exercise 3.10.

                 Initially, assume that each processor has n/p elements stored in its
                 local memory. Partitioning the items in each bucket into p blocks
                 takes O( np log m) time using binary search. Sending data to their
                 processors can be achieved in O( np log p) time. The sorting step takes
                 Θ( np log np ) time. Hence, the total running time is
                                                                 
                                             n    n n       n
                                     Θ         log + log m + log p .
                                             p    p p       p

                 If m = O(n), then the running time becomes Θ( np log n), since p < n.

        3.12. Consider the algorithm for permutation routing in the hypercube
              discussed in Section 3.6. What is the probability that the algorithm
              will route all packets to their destinations in 8d steps or fewer?

                 With probability at least 1 − 2−1.5d , every packet vi reaches its des-
                 tination σ(i) in 4d or fewer steps. So, the full algorithm (two phases)
                 routes all packets to their destinations in 8d or fewer steps with prob-
                 ability (1 − 2−1.5d ) × (1 − 2−1.5d ) = (1 − 2−1.5d )2 .

        3.13. Consider Algorithm hcselect for selection on the hypercube dis-
              cussed in Section 3.11. For what values of p is the algorithm optimal?

                 Since the lower bound for any sequential selection algorithm is Ω(n),
                 the lower bound for the parallel version is Ω(n/p). Therefore, the
                 algorithm is optimal for all values of p = n , 0 <  < 1. In this case,
                 the running time of the algorithm is Θ(n1− ).

        3.14. Consider Algorithm parselect for selection on the EREW PRAM
              presented in Section 2.14. Suppose we simulate this algorithm to
              run on the hypercube with n/ log n processors. What will be the
              running time of the algorithm? Compare this with that of Algorithm
              hcselect for selection on the hypercube presented in Section 3.11.
              (See Exercise 3.21).
May 7, 2022   11:14       Parallel Algorithms      9in x 6in      b4591-ch03               page 149




                                            The Hypercube                            149

                 The running time will be

                              O(log n log log n log p) = O(log2 n log log n).

                 This is much slower than Algorithm hcselect, which runs in time
                 O(log n(log log n)2 ).

        3.15. Outline an algorithm to ﬁnd all the kth largest elements in a hyper-
              cube with p < n processors. What is the running time of your
              algorithm?

                 First, ﬁnd the kth smallest element x using the algorithm for selec-
                 tion. Next, broadcast x to all processors. Finally, each processor out-
                 puts all elements greater than or equal to x. The running time is
                 O(n/p + Ts (n, p) + log p) = O(n/p + Ts (n, p)), where Ts (n, p) is the
                 time required by the selection algorithm.

        3.16. Consider Algorithm hcmultiselect for multiselection on the hyper-
              cube discussed in Section 3.12. For what values of r is the algorithm
              cost optimal when the number of processors is n1− ?

                 When p = n1− , the running time is

                                  T (n, r, n1− ) = O(n min{r, log n1− }).

                 Since the lower bound for any sequential multiselection algorithm is
                 Ω(n log r), the lower bound for the parallel version is Ω((n/p) log r).
                 Hence, the algorithm is cost optimal for r ≥ p = n1− .

        3.17. Consider Algorithm hcmultiselect for multiselection on the hyper-
              cube discussed in Section 3.12. Compare the algorithm given with
              direct application of Algorithm hcselect given in Section 3.11.

                 Direct application of Algorithm hcselect r times takes

                  r × O(n/p + ts (p, p)(log n − log p)) = O(rn/p + rts (p, p) log(n/p)).

                 On the other hand, Algorithm hcmultiselect takes

                                O((n/p + ts (p, p) log(n/p)) min{r, log p}),

                 which is less than direct application for r > log p.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in           b4591-ch03     page 150




        150                                     Parallel Algorithms


                                                                       y
                                   d               c


                                                                   z



                                  a                b                       x
                                                               w
                                        G                          H

                                        Fig. 3.26.     Exercise 3.19.

        3.18. Construct the Gray code sequence G4 .
                 Similar to Fig. 3.7.

        3.19. Consider the two graphs shown in Fig. 3.26. Find an embedding of G
              into H. What are the dilation, congestion, expansion and load of your
              embedding?
                 Deﬁne the embedding functions φ and ψ by: φ(a) = w, φ(b) =
                 x, φ(c) = y, φ(d) = z, ψ((a, b)) = w, x, ψ((b, c)) = x, y, ψ((c, d)) =
                 y, z and ψ((a, d)) = w, z. Since each edge of G is mapped to exactly
                 one edge of H, the dilation is 1. All edges of H are used at most
                 once, and hence the congestion is 1. The expansion is 3/3 = 1, and
                 the load is 1.

        3.20. Give an embedding similar to the one given in Example 3.2, except
              that it is postorder, that is, the nodes of the binary tree are labeled
              in postorder traversal. What are the dilation, congestion, expansion
              and load of the embedding?
                 Similar to Example 3.2.

        3.21. Explain how to simulate an EREW PRAM on a hypercube with n
              processors.
                 The simulation is done using routing. PRAM processor local com-
                 putation is done locally, while a read/write by PRAM processor i
                 to PRAM memory j can be simulated by a packet going through
                 the network from the node simulating i to the node simulating j.
                 Thus, simulating a PRAM with p processors on a hypercube with
May 7, 2022    11:14         Parallel Algorithms      9in x 6in    b4591-ch03              page 151




                                               The Hypercube                        151

                  the same number of processors is a packet routing problem. Since
                  the hypercube can route a packets in O(log p) steps, it can simulate
                  any EREW PRAM with p processors with an O(log p) factor delay.

        3.22. Compute the bisection width of the d-dimensional butterﬂy Bd .

                  Figure 3.3 shows the d-dimensional butterﬂy for d = 1, 2, 3. From
                  the ﬁgure, it is clear that for j > 1, Bj can be divided into two
                  halves with 2 × 2d−1 = 2d connections between them. To construct
                  a bisection width of this size, simply remove the cross edges from a
                  single level.

        3.23. Design an algorithm to compute the sum of n numbers on the hyper-
              cube with p processors, 1 ≤ p < n. Is your algorithm always optimal?

                  Assume that each processor Pi contains at least one number; if not
                  then let Pi contain 0. First, compute the sum of the numbers in each
                  processors. Next, compute the sum of the p = 2d resulting numbers
                  using the technique of reduction, which is a method similar to the
                  method used in broadcasting in the hypercube, but in reverse order.
                  This is shown in Algorithm hcsum. Here the notation j (i) means j
                  with the ith bit complemented, 0 ≤ i ≤ d − 1. If the numbers are
                  distributed evenly among the p processors, so that each processor
                  contains n/p numbers, then the running time is O(max{ np , d}), which
                  is optimal. Otherwise, the algorithm is not optimal, as the running
                  time may be as large as Θ(n). In this case, data redistribution may be
                  helpful if it takes o(m), where m is the maximum number of elements
                  in all processors.

          Algorithm 3.15 hcsum
          Input: x0 , x1 , . . . , x2d −1 .
          Output: The sum of the numbers x0 , x1 , . . . , x2d −1 stored in processors
                  P0 , P1 , . . . , P2d −1 of Hd .
              1. for i ← d − 1, d − 2, . . . , 1, 0 do
              2.     for all j < 2i and j < j (i) do in parallel
              3.         xj ← xj + xj (i)
              4.     end for
              5. end for
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch03        page 152




        152                                     Parallel Algorithms


        3.24. Explain how to compute the maximum of 2d−1 numbers distributed
              arbitrarily in a hypercube with 2d processors. What is the running
              time of your algorithm?
                 The ﬁrst step is to route these numbers so that they occupy one half
                 of the hypercube. This routing step takes Θ(log n) time. Next, the
                 maximum of these numbers is computed in Θ(log n) time. The total
                 running time is Θ(log n).
                   Another alternative is to use parallel preﬁx to pack the numbers
                 in the ﬁrst 2d−1 processors and then ﬁnd their sum using the lower
                 half of the hypercube.

        3.25. Consider the partial permutation routing problem on the hypercube
              in which every processor is the source of at most one packet and the
              destination of at most one packet. Will Theorem 3.2 hold for this
              routing problem?
                 Theorem 3.2 holds for partial permutation routing, and the proof
                 works with no modiﬁcations.

        3.26. Consider the many-to-many routing problems on the hypercube in
              which every processor is the source of r packets and the destination
              of r packets. Suppose we run the greedy algorithm for routing on the
              hypercube to solve this problem. What will be the maximum queue
              size?
                 Theorem 3.2 no longer holds for many-to-many routing. In the proof
                 of Theorem 3.2, a ≤ r2k − 1 and b ≤ r2d−k − 1, and hence the
                                                 √
                 maximum queue size will be O(r n).

        3.27. Give an O(nd) time algorithm for the problem of routing in the
              d-dimensional hypercube if every processor has a packet to be sent to
              every other processor, where n = 2d . Hint : Use randomized routing n
              times.
                 Use randomized routing sequentially n times. Each run takes O(d)
                 time for a total of O(nd).

        3.28. Give an O(n) time algorithm for the problem in Exercise 3.27.
                 Use randomized routing in parallel n times. This takes O(d) parallel
                 steps. However, there are queues that will expand the running time.
                 There are n(n − 1) paths, and hence each node of the hypercube is
May 7, 2022   11:14        Parallel Algorithms          9in x 6in        b4591-ch03              page 153




                                                 The Hypercube                            153


                 included in n(n−1)
                                n   = n − 1 paths. This results in a queue of size
                 O(n) at each node. This means the running time will be expanded
                 to O(n) + O(d) = O(n).

        3.29. Apply Algorithm hcparprefix for computing parallel preﬁx on the
              hypercube on the input sequence 1, 2, 3, 4, 5, 6, 7, 8 . Assume a hyper-
              cube with 8 processors.
                 Similar to the example in Fig. 3.14.

        3.30. Give an algorithm to evaluate the polynomial an−1 xn−1 +an−2 xn−2 +
              · · · + a1 x + a0 at the point x0 on the d-dimensional hypercube Hd
              with n = 2d processors. Assume that each ai is stored in processor Pi ,
              0 ≤ i ≤ n − 1.
                 First, use parallel preﬁx to compute 1, x0 , x20 , . . . , xn−1
                                                                             0   in processors
                 P0 , P1 , . . . , Pn−1 . Next, within each processor, multiply ai × xi0 , 0 ≤
                 i ≤ n − 1. Finally, use Algorithm hcsum in Exercise 3.23 above to
                 ﬁnd the desired sum. The running time is Θ(log n).

        3.31. Consider a hypercube with four processors {P0 , P1 , P2 , P3 } with ini-
              tial loads 8, 2, 6, 4. Perform load balancing on the hypercube so that,
              at the end, each processor has the same load.
                 Consider the hypercube with four processors shown in Fig. 3.27(a).
                 The number next to a processor is its initial load. The sum of the
                 initial loads is 20, and so, after balancing, each processor will have
                 5 units of load. One way to accomplish this is to have processor P0
                 send 3 units to processor P1 and to have processor P2 send, in parallel,
                 one unit to processor P3 . The time needed for this is 3 units as the

                          8                  2                      8           2
                                  3                                      4
                          P                  P                      P           P
                          0                  1                      0           1


                                                              1                       1


                          P                  P                      P           P
                          2                  3                      2           3
                                 1
                          6                  4                      6           4
                                     (a)                                 (b)

                                           Fig. 3.27.   Exercise 3.31.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch03             page 154




        154                                     Parallel Algorithms

                 transfer from processor P2 to processor P3 is overlapped with the
                 transfer from processor P0 to processor P1 .
                   Another possibility is shown in Fig. 3.27(b). In this scheme, pro-
                 cessor P0 sends 4 units to processor P1 . After this transmission is
                 completed, processor P2 sends one unit to processor P0 and proces-
                 sor P1 sends, in parallel, one unit to processor P3 . The total time is
                 5 units.

        3.32. Redo Exercise 3.31 using the algorithm presented in Section 3.13.
                 Similar to the example described in Section 3.13.

        3.33. Consider a hypercube with eight processors {P0 , P1 , . . . , P7 } with ini-
              tial loads 8, 5, 6, 4, 7, 2, 5, 3. Perform load balancing on the hypercube
              using the algorithm described in Section 3.13 so that, at the end, each
              processor has approximately the same load.
                 Similar to the example described in Section 3.13.

        3.34. Suggest a heuristic to improve the performance of the load balancing
              algorithm discussed in Section 3.13.
                 A simple heuristic is to select the next dimension to balance across,
                 as the dimension that maximizes si = max | Li − Lj | such that Pj is
                 a neighbor of Pi along an unselected dimension. So, ﬁrst, each pro-
                 cessor Pi computes ri and si such that si = max | Li − Lj |, where Pj
                 is a neighbor of Pi , along an unselected dimension. ri is such that
                 si = | Li − Lj |, where j is si ’s neighbor along dimension ri . Next,
                 the maximum of the si ’s is computed. If this maximum is sl , then
                 dimension rl is selected. The time required to select the next dimen-
                 sion is O(d), and the total time spent on determining the order of
                 dimensions is O(d2 ), which does not aﬀect the time complexity of
                 the algorithm.

        3.35. Illustrate the operation of Algorithm bfoddevenmerge for merging
              on the butterﬂy to merge the two sorted sequences A = 1, 4, 6, 9
              and B = 2, 5, 7, 8 on the 3-dimensional butterﬂy.
                 Similar to Example 3.4.

        3.36. Use the matrix multiplication algorithm on the hypercube discussed
              in Section 3.16 to compute the product C = A × B of the two 2 × 2
May 7, 2022   11:14       Parallel Algorithms           9in x 6in       b4591-ch03         page 155




                                            The Hypercube                            155


                 matrices A and B shown below. Assume a hypercube with n = 23 = 8
                 processors.
                                                                  
                                        1       3                2 1
                                     A=                  and B =       .
                                        2       4                4 3

                 Similar to Example 3.6.

        3.37. Suggest an algorithm for computing the transitive closure of an adja-
              cency matrix A on the hypercube. What is the running time of the
              algorithm?

                 Use an algorithm analogous to the one for the PRAM presented
                 in Section 2.17. Recall that this algorithm computes the transitive
                 closure by squaring the adjacency matrix log n times. Thus, the
                 running time is O(log2 n) using O(n3 ) processors.

        3.38. Suggest an algorithm for computing the shortest paths in a directed
              graph G represented by its adjacency matrix A on the hypercube.
              What is the running time of the algorithm?

                 Use an algorithm analogous to the one for the PRAM presented in
                 Section 2.18. Recall that this algorithm computes the shortest paths
                 by ﬁrst computing a matrix similar to the transitive closure matrix
                 using repeated squaring of the weight matrix log n times. Hence,
                 the running time is O(log2 n) using O(n3 ) processors.

        3.39. Let S = x0 , x1 , . . . , xn−1 be a sequence of numbers stored in a
              hypercube with n processors where xi is stored in Pi , 0 ≤ i < n,
              and let y be stored in P0 . Give an algorithm to count the number of
              elements in S that are larger than y.

                 First, broadcast y to all processors. Next, each processor Pi sets
                 zi = 1 if xi > y and zi = 0 if xi ≤ y. Finally, ﬁnd the sum of
                 zi , 0 ≤ i ≤ n − 1, in all processors, and store the sum, which is the
                 number of 1s, in P0 .

        3.40. Following the example shown in Fig. 3.19, show how to compute the
              preﬁx sums of the sequence 1, 2, 3, 4 on the 2-dimensional butterﬂy.


                 Similar to Fig. 3.19.
May 7, 2022   11:14       Parallel Algorithms             9in x 6in              b4591-ch03   page 156




        156                                       Parallel Algorithms


                                                     (110,1)        (111,1)
                                        (110,2)                                    (111,3)
                                                                   (111,2)
                                                     (110,3)
                              (100,2)     (100,1)                     (101,2)
                                                     (101,1)
                           (100,3)        (010,3)                   (101,3)        (011,3)

                                        (010,2)                    (011,1)
                                                      (010,1)                    (011,2)

                                           (000,2)     (001,3)
                           (000,3)                                     (001,2)

                                      (000,1)            (001,1)

                      Fig. 3.28.     3-dimensional cube-connected cycles (CCC).

        3.41. The d-dimensional cube-connected cycles (CCC) is constructed from
              the d-dimensional hypercube by replacing each node with a cycle
              of length d (see Fig. 3.28). The nodes in the cycle corresponding
              to node x in the hypercube are labeled as (x, 1), (x, 2), . . . , (x, d).
              Node (x, i) is connected to node (y, j) if and only if x = y and
              | i − j | = 1 (mod d) or i = j and x and y are connected in the corre-
              sponding hypercube. The CCC has d2d nodes. Derive an algorithm
              to ﬁnd the sum of n = d2d numbers stored in the CCC — one num-
              ber per processor. The resulting sum should be stored in processor
              P(0d ,1) .
                 Let the cycles of the CCC be C1 , C2 , . . . , C2d , and let ti be the sum
                 of all numbers in cycle Ci . First, ﬁnd the sum ti in each cycle and
                 broadcast it to all processors in the same cycle. This takes Θ(d)
                 time. Next, ﬁnd the sum of all totals ti and store it in P(0d ,1) . The
                 rest, i.e., ﬁnding the total of these sums is similar to ﬁnding the sum
                 of 2d numbers in the hypercube (see, for example, Exercise 3.2).

        3.42. What are the degree and diameter of the d-dimensional cube-
              connected cycles described in Exercise 3.41?
                 Its degree is 3, and its diameter is Θ(d) = Θ(log n).

        3.43. What is the bisection width of the d-dimensional cube-connected
              cycles described in Exercise 3.41?
May 7, 2022   11:14        Parallel Algorithms      9in x 6in                b4591-ch03                               page 157




                                             The Hypercube                                                     157


                 If we consider a d-dimensional cube-connected cycles with n = d2d
                 processors and cut it by a line into two halves, the line will cut
                 2d−1 links. Hence, the bisection width of the d-dimensional CCC is
                 Θ(n/ log n).

        3.44. Give an algorithm for computing the preﬁx sums on the
              d-dimensional cube-connected cycles described in Exercise 3.41. Your
              algorithm should run in O(d) = O(log n) time.
                 Similar to ﬁnding the sum in CCC and ﬁnding the preﬁx sums in the
                 hypercube; see Exercises 3.41 and 3.3.

        3.45. Give an embedding function from the d-dimensional hypercube to
              the d-dimensional cube-connected cycles(CCC) network described in
              Exercise 3.41. What is the dilation of the embedding?
                 Map node x in the hypercube to node (x, 1) in the CCC, and map
                 the edge (x, y) in the hypercube to edge ((x, 1), (y, 1)) (see Fig. 3.29).
                 The dilation of the embedding is 1 + 2d/2 = Θ(d).

        3.46. What is the congestion of the embedding in Exercise 3.45?
                 Consider Fig. 3.30, which shows a cycle of length d in the
                 d-dimensional cube-connected cycles. It is easy to see that the edge
                 ((00 . . . 0, 1), (00 . . . 0, 2) is used by d/2 paths. Hence, the conges-
                 tion is d/2 = Θ(d).

                                                                              (110,1)        (111,1)
                                                                 (110,2)                                    (111,3)
                                                                                            (111,2)
                                                                              (110,3)
                                                      (100,2)      (100,1)                     (101,2)
                                                                              (101,1)
                                                    (100,3)        (010,3)                   (101,3)        (011,3)

                                                                 (010,2)                    (011,1)
                                                                               (010,1)                    (011,2)

                                                                    (000,2)     (001,3)
                                                    (000,3)                                     (001,2)

                                                                (000,1)           (001,1)

        Fig. 3.29. Embedding of d-dimensional hypercube into the d-dimensional cube-
        connected cycles.
May 7, 2022   11:14           Parallel Algorithms           9in x 6in           b4591-ch03   page 158




        158                                         Parallel Algorithms




                                                        (00...0,1)      (00...0,2)




                      Fig. 3.30.   A cycle in the d-dimensional cube-connected cycles.

        3.47. Explain how to simulate a hypercube with 2d processors on the cube-
              connected cycles with d2d processors described in Exercise 3.41.
                 Any step of the hypercube can be simulated in d steps on the CCC
                 by using one cycle of the CCC to simulate the action of one node of
                 the hypercube.
May 7, 2022   11:14     Parallel Algorithms       9in x 6in    b4591-ch04                   page 159




                                              Chapter 4


                The Linear Array and the Mesh



        4.1     Introduction

        Linear arrays are the simplest example of a ﬁxed-connection network. An
        example of a linear array is shown in Fig. 4.1(a). It consists of n proces-
        sors P1 , P2 , . . . , Pn , where each interior processor is connected with bidi-
        rectional links to its left neighbor and its right neighbor. The outermost
        processors P1 and Pn have just one connection each. If we connect them by
        a link, we obtain a ring, which is a simple extension of the linear array (see
        Fig. 4.1(b)).
            A two-dimensional mesh is an extension of the linear array to two dimen-
        sions. A mesh of size n consists of n simple processors arranged in a square
        lattice. To simplify our exposition, it is assumed that n = 4k for some posi-
                                                  √
        tive integer k. For all i, j, 1 ≤ i, j ≤ n, and processor Pi,j representing the
        processor in row i and column j is connected via bidirectional communica-
        tion links to its four neighbors, processors Pi±1,j and Pi,j±1 — assuming
        they exist. (See Fig. 4.1(c)).
            A torus is simply a mesh with wraparound connections; each row and
        each column has a wraparound connection. Fig. 4.2 depicts a torus on 16
        processors.




                                                 159
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch04          page 160




        160                                    Parallel Algorithms


              (a)                                      (c)                        n




              (b)




                                                             n

                      Fig. 4.1.     (a) A linear array. (b) A ring. (c) A mesh.




                                  Fig. 4.2.    A torus on 16 processors.
                                                                     √           √
            The communication diameter of a mesh of size n is 2( n − 1) = Θ( n),
        and this can be seen by examining the distance between processors in oppo-
        site corners of the mesh. This means if a processor in one corner of the mesh
        needs data from a processor in another corner of the mesh sometime during
        the execution of an algorithm, then a lower bound on the running time of
                             √
        the algorithm is Ω( n).
            There is no linear ordering on the set of processors in the mesh. However,
        there are several two-dimensional orderings, called indexing schemes, like
        row-major and snakelike shown in Fig. 4.3.
May 7, 2022   11:14           Parallel Algorithms      9in x 6in     b4591-ch04             page 161




                                     The Linear Array and the Mesh                    161




                                       (a)                          (b)

                  Fig. 4.3.     Mesh indexing schemes. (a) Row-major (b) Snakelike.




                          Fig. 4.4.       Embedding a linear array into a mesh.


        4.2     Embedding between a Mesh and a Linear Array

        We consider the problem of embedding between a mesh and a linear array
        with the same number of processors. Embedding a linear array into the
        mesh is obvious; it is illustrated in Fig. 4.4. This mapping has dilation 1,
        since every edge of the linear array is mapped to one edge of the mesh. The
        congestion is also 1, since every edge of the mesh is used by exactly one
        edge of the linear array, as is evident from the ﬁgure.
            Now, consider inverting the above mapping to obtain the embedding of
        the mesh into the linear array illustrated in Fig. 4.5. Edge e1 in the mesh
        is mapped to the path from s to t in the linear array, which is of length 7.
May 7, 2022   11:14         Parallel Algorithms           9in x 6in          b4591-ch04        page 162




        162                                       Parallel Algorithms




                                                                                          y
                                                               s         u         w
                      e1    e2          e3           e4
                                                               t
                                                                        v          x      z
                                 Mesh                                   Linear array

                           Fig. 4.5.     Embedding a mesh into a linear array.


        It is not hard to see that this is maximum, and in general, the dilation of
                                                                 √
        this embedding of the mesh into the linear array is 2 n − 1. Now, consider
        the number of edges of the mesh mapped to edge (y, z) in the linear array.
        It is evident from the ﬁgure that there are 4 edges of the mesh mapped
        to this edge in the linear array. Speciﬁcally, e1 , e2 , e3 and e4 in the mesh
        are all mapped to paths that contain edge (y, z) in the linear array. For
        example, the edge e2 in the mesh is mapped to the path u, w, y, z, x, v in
        the linear array. Hence, the congestion of the mapping in Fig. 4.5 is 4. It is
                                                  √
        not diﬃcult to see that, in general, it is n.


        4.3     Broadcasting in the Linear Array and the Mesh

        Let L be a linear array of n processors. To broadcast a datum x from P1 to
        all other processors, x is sent to P2 , P3 , . . . , Pn in this order. The number of
        steps is n − 1 = Θ(n). If the origin of broadcasting is not P1 , say Pi (i < n),
        x is sent in both directions in parallel. The number of steps in this case is
        equal to the maximum of the distances from Pi to P1 and Pn .
            Let M be a mesh of size n. Broadcasting a datum x from P1,1 to all other
        processors can be achieved in two phases. First, x is sent to all processors
        in row 1. Next, x is sent in parallel from all processors in row 1 along
        all columns of the mesh. The total number of steps in the two phases is
          √              √
        2( n − 1) = Θ( n).
            If the origin of broadcasting is Pi,j (which is diﬀerent from P1,1 ), then
        broadcasting of x to all other processors can be achieved in two phases: in
May 7, 2022    11:14       Parallel Algorithms      9in x 6in         b4591-ch04               page 163




                                  The Linear Array and the Mesh                         163

        phase 1, x is sent to all processors in row i. In phase 2, x is sent in parallel
        from all processors in row i along all columns in M . The running time is
           √
        Θ( n).


        4.4      Computing Parallel Preﬁx on the Mesh

        The parallel preﬁx problem was deﬁned in Section 2.5. In this section, we
        show how to compute it on the linear array and the mesh. For simplicity,
        we will assume addition as the binary operation. Let L be a linear array
        with n processors, where each processor Pi contains item xi , 1 ≤ i ≤ n.
        Assume that each processor Pi has register si . The algorithm is shown as
        Algorithm laparprefix. In this algorithm, si−1 is passed to Pi , 2 ≤ i ≤ n,
        where xi is added to it to produce si , as in the sequential algorithm. The
        algorithm runs in time Θ(n).

          Algorithm 4.1 laparprefix
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers.
          Output: S = s1 , s2 , . . . , sn , the preﬁx sums of X.
              1. s1 ← x1
              2. for i ← 2 to n do
              3.     Processor Pi computes si ← si−1 + xi .
              4. end for



             Now, we consider computing parallel preﬁx on the mesh. Let M be a
        √      √
           n × n mesh, and assume the row-major indexing scheme. The algorithm
        is given as Algorithm meshparprefix. First, the individual preﬁx sums of
                                                                                        √
        all rows are computed using Algorithm laparprefix. For 1 ≤ i ≤ n,
        let the preﬁx sums of row i be yi,1 , yi,2 , . . . , yi,√n . Note that these are not
                                                                                         √
        the ﬁnal preﬁx sums, except for row 1. Next, the preﬁx sums of column n
        are computed, again using Algorithm laparprefix. These are denoted
        by s1,√n , s2,√n , . . . , s√n,√n , and they are the ﬁnal preﬁx sums for column
        √                                                     √             √
           n. Finally, for all processors Pi,j , 2 ≤ i ≤ n, 1 ≤ j ≤ n − 1, we set
        si,j ← yi,j + si−1,√n . This implies broadcasting si−1,√n to row i.
                                     √                             √
             Steps 1–3 take Θ( n) time. Step 4 takes Θ( n) time too. Steps 5–9
                                                                                       √
        take Θ(1) time plus the time needed for broadcasting, which is Θ( n).
                                                                      √
        Hence, the total running time of the algorithm is Θ( n).
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch04                    page 164




        164                                     Parallel Algorithms


          Algorithm 4.2 meshparprefix
                                           √
          Input: X = xi,j | 1 ≤ i, j ≤ n, a sequences of n numbers.
                                              √
          Output: S = si,j | 1 ≤ i, j ≤ n, the preﬁx sums of X.
                            √
           1. for i ← 1 to n do in parallel
           2.     Use Algorithm laparprefix to compute the preﬁx sums of row i.
                  Let these be yi,1 , yi,2 , . . . , yi,√n .
           3. end for
           4. Use Algorithm laparprefix to compute the preﬁx sums
                              √
                  of column√ n. Let these be s1,√n , s2,√n , . . . , s√n,√n .
           5. for i ← 2 to n √   do in parallel
           6.     for j ← 1 to n − 1 do in parallel
           7.         si,j ← yi,j + si−1,√n
           8.     end for
           9. end for



        4.5     Odd–Even Transposition Sort

        This sorting algorithm is for linear arrays (and rows and columns of
        meshes). The algorithm is very simple. It alternates between odd steps
        and even steps. At odd steps, we compare the contents of processors P1
        and P2 , P3 and P4 , and so on exchanging values if necessary. At even steps,
        we repeat the same procedure on processors P2 and P3 , P4 and P5 , and so
        on. The algorithm takes n steps to sort its input x1 , x2 , . . . , xn , one item
        xi per processor Pi , 1 ≤ i ≤ n. Hence its running time is Θ(n).

        Theorem 4.1         Odd–even transposition sort correctly sorts any sequence
        of numbers.

        Proof. By Lemma 2.1 in Section 2.10, we may assume that the input
        sequence X consists of 0’s and 1’s. We prove by induction on |X| that the
        algorithm sorts the binary sequence X. If n = 1 or 2, then the hypothesis
        is true. So assume it is true for all sequences of size k, 1 ≤ k ≤ n − 1.
        Let X = x1 , x2 , . . . , xn  stored in processors P1 , P2 , . . . , Pn . Let xj be the
        rightmost 1, where 1 ≤ j ≤ n. xj will start moving rightward in the ﬁrst
        or second step of the algorithm. Once it starts moving, it will subsequently
        move rightward in each step until it reaches the right end — that is, until
        xn = 1. Now, it remains to sort X  = x1 , x2 , . . . , xn−1  in processors
        P1 , P2 , . . . , Pn−1 . By induction, X  will be sorted by the algorithm. It follows
        that X will be sorted correctly by the algorithm.                                      
May 7, 2022    11:14        Parallel Algorithms       9in x 6in     b4591-ch04         page 165




                                   The Linear Array and the Mesh                 165




                          Fig. 4.6.    Example of odd–even transposition sort.

        Example 4.1 An example of the algorithm is shown in Fig. 4.6.             


        4.6      Shearsort
                                                                           √     √
        This sorting algorithm is for meshes, and it sorts n items in a n × n
                                                     √
        mesh in snakelike order. It consists of 2 log n + 1 = log n + 1 phases. The
        algorithm alternates between odd and even phases. At odd phases, it sorts
        the rows of the mesh, and at even phases, it sorts its columns. The odd
        rows are sorted so that smaller numbers move leftward, and the even rows
        are sorted so that smaller numbers move rightward. The columns are sorted
        so that smaller numbers move upward. Odd–even transposition sort may
        be used to sort the rows and columns. In this case, the running time of the
                        √
        algorithm is Θ( n log n). An outline of the algorithm is given as Algorithm
        shearsort.

          Algorithm 4.3 shearsort
          Input: A sequence S = a1 , a2 , . . . , an .
          Output: The elements in S in sorted order.
              1. for i ← 1 to log n + 1
              2.     if i is odd then sort all rows in snake-like order
              3.     else sort all columns
              4. end for
May 7, 2022   11:14         Parallel Algorithms            9in x 6in       b4591-ch04        page 166




        166                                       Parallel Algorithms


                      (a) 4 6   2 15         (b) 2     4   6 15        (c) 1   4   6    3
                         8 16 12 3                 16 12 8      3         2    7   8    5
                         7 13 1 10                 1 7     10 13          1 4 11 9 1 3
                         5 9 1 4 11                1 4 11 9 5             16 12 10 15
                            Input                     Sort rows           Sort columns

                      (d) 1 3   4    6       (e) 1     3   4    2      (f) 1   2   3    4
                         8 7    5    2             8   7   5    6         8    7   6    5
                         9 11 13 14                9 11 1 2 1 0           9 1 0 11 1 2
                         16 15 12 10               16 15 13 14            16 15 14 13
                           Sort rows               Sort columns             Sort rows

                         Fig. 4.7.       An illustration of Algorithm shearsort.

        Example 4.2             An illustration of Algorithm shearsort is given in
        Fig. 4.7.                                                               


        Theorem 4.2 Algorithm shearsort correctly sorts any sequence of n
                    √   √               √
        numbers on a n × n mesh in 2 log n + 1 phases.

        Proof.       By Lemma 2.1 in Section 2.10, we may assume that the input
                                                                               √      √
        consists of 0’s and 1’s. So, let the input be initially stored in the n × n
        mesh, one number per processor. A row of the mesh will be called dirty if
        it consists of both 0’s and 1’s, and clean if it consists of only 0’s or only 1’s.
                                              √
        Initially, there may be as many as n dirty rows. During the execution of
        the algorithm, there will be rows all 0’s followed by dirty rows followed by
        rows with all 1’s. After the algorithm terminates, there will be at most one
        dirty row. Let an iteration of the algorithm consist of two phases, a row sort
        phase and a column sort phase. We will show that after each iteration, at
                                                                                     √
        least half of the dirty rows become clean. This will imply that after log( n)
        iterations there will be at most one dirty row, which can be sorted using
                                                           √
        an additional sorting phase for a total of 2 log( n) + 1 = log n + 1 phases.
        Thus, it remains to show that the number of dirty rows will decrease by a
        factor of at least 2 in each iteration.
            Consider two adjacent rows in an iteration after the phase of row sorting.
        There are three possibilities according to whether there are more 0’s than
        1’s (Fig. 4.8(a)), more 1’s than 0’s (Fig. 4.8(b)), or an equal number of
        0’s and 1’s (Fig. 4.8(c)). Now, after sorting the columns of the mesh, each
May 7, 2022    11:14         Parallel Algorithms         9in x 6in             b4591-ch04              page 167




                                     The Linear Array and the Mesh                               167




                           (a)                     (b)               (c)

                                 Fig. 4.8.   Dirty rows after rows are sorted.

        one of these three cases will contribute at least one clean row. If there are
        more 0’s than 1’s (part (a) of the ﬁgure), then after sorting the columns,
        there will be at least one more clean row consisting of all 0’s. If there are
        more 1’s than 0’s (part (b) of the ﬁgure), then after sorting the columns,
        there will be at least one more clean row consisting of all 1’s. If there are
        equal number of 0’s and 1’s (part (c) of the ﬁgure), then after sorting the
        columns, there will be two more clean rows one consisting of all 0’s and
        one consisting of all 1’s. Thus the number of dirty rows will decrease by a
        factor of at least 2 in each iteration.                                    


        Corollary 4.1 If the number of dirty rows is k, then Algorithm shear-
        sort performs 2 log k + 1 phases.

                              √
        4.7        A Simple Θ( n) Time Algorithm for Sorting on the Mesh
                                                  √
        In this section, we derive a simple Θ( n) time algorithm for sorting n
                         √      √
        numbers on the n × n mesh. It is a divide-and-conquer      √ algorithm,
                                                                         √      where
        the mesh is ﬁrst partitioned into four submeshes of size 2n × 2n each. The
        algorithm ﬁrst sorts each quadrant recursively in snake-like order. It then
        sorts the rows of the entire mesh in snake-like order, and ﬁnally performs ﬁve
        phases of Algorithm shearsort. It is shown as Algorithm meshsortrec.

          Algorithm 4.4 meshsortrec
          Input: A sequence S = a1 , a2 , . . . , an .
          Output: The elements in S in sorted order.
                                                                           √       √
              1.   Partition the mesh into four quadrants of size 2n ×              2
                                                                                     n
                                                                                         each.
              2.   Recursively sort each quadrant in snake-like order.
              3.   Sort the rows of the entire mesh in snake-like order.
              4.   Sort the columns top-down.
              5.   Perform ﬁve phases of Algorithm shearsort.
May 7, 2022   11:14                 Parallel Algorithms                            9in x 6in             b4591-ch04                             page 168




        168                                                       Parallel Algorithms


                      (a) 0     1   1 1 1       1   0   1   (b) 0      0   0   0    0   0   0   0   (c) 0   0   0   0   0   0   0   0
                          1     0   1 0 0       1   0   1       0      0   0   0    0   0   0   0       0   0   0   0   0   0   0   0
                          0     1   0 0 1       1   0   0       1      1   1   1    1   1   1   1       1   1   1   1   1   1   1   1
                          0     0   1 1 1       0   0   0       1      1   1   1    1   1   1   1       1   1   1   1   1   1   1   1
                          1     1   0 0 1       1   1   1       0      0   0   0    0   0   0   0       0   0   0   0   0   0   0   0
                          0     0   1 0 1       1   0   1       0      0   0   0    1   1   1   0       1   1   1   0   0   0   0   0
                          1     0   1 0 1       0   0   1       0      1   1   1    1   1   1   1       0   1   1   1   1   1   1   1
                          1     0   0 1 0       1   0   1       1      1   1   1    1   1   1   1       1   1   1   1   1   1   1   1
                                    Input                             Sort recursively                          Sort rows
                      (d)                                   (e)
                            0   0   0   0   0   0   0   0         0    0   0   0   0    0   0   0
                            0   0   0   0   0   0   0   0         0    0   0   0   0    0   0   0
                            0   0   0   0   0   0   0   0         0    0   0   0   0    0   0   0
                            0   1   1   0   0   0   0   0         1    1   0   0   0    0   0   0
                            1   1   1   1   1   1   1   1         1    1   1   1   1    1   1   1
                            1   1   1   1   1   1   1   1         1    1   1   1   1    1   1   1
                            1   1   1   1   1   1   1   1         1    1   1   1   1    1   1   1
                            1   1   1   1   1   1   1   1         1    1   1   1   1    1   1   1
                                Sort columns                      Apply shear sort

         Fig. 4.9.          An illustration of Algorithm meshsortrec on input of 0’s and 1’s.

          (a)                                               (b)                                                     (c)
                      0             0                                              0
                                                                       balanced rows                                                    0
                      1             1                                              1                                            4 dirty rows
                                                    Bordering                                         4 Bordering
                                    0               rows                           0                  rows
                      0                                                balanced rows                                                    1
                      1             1                                              1

                  After recursive                                      After sorting                                            After sorting
                  calls                                                by rows                                                  by columns

                                                Fig. 4.10.             Proof of Theorem 4.3.

        Example 4.3 An illustration of Algorithm meshsortrec on input of
        0’s and 1’s is shown in Fig. 4.9.                             

        Theorem 4.3 Algorithm meshsortrec correctly sorts any sequence
                         √   √
        of n numbers on a n × n mesh.

        Proof.     By the zero-one principle (Lemma 2.1 in Section 2.10), we may
        consider any input sequence of 0’s and 1’s. See Fig. 4.10. After the recursive
May 7, 2022   11:14      Parallel Algorithms      9in x 6in       b4591-ch04                    page 169




                                The Linear Array and the Mesh                            169

        calls, the data in each quadrant is such that all but at most one of the rows
        are either all 0’s or all 1’s (see Fig 4.10(a)). A row in the mesh is balanced if
        the left half consists of all 0’s, the right half consists of all 1’s, or vice-versa.
        Thus, in the entire mesh, all, but at most four of the rows are either all 0’s,
        all 1’s or balanced. Call these four lines the borderline rows.
            After sorting all rows, the borderline rows are sorted and both blocks
        of balanced rows alternate between 1’s to the left and 1’s to the right (see
        Fig 4.10(b)).
            After sorting all columns, the (at most) four borderline rows will be
        contiguous (see Fig 4.10(c)), and since there are at most four dirty rows,
        then by Corollary 4.1, only 2 log 4 + 1 = 5 phases of Algorithm shearsort
        are required to sort the numbers.                                                  



        4.8     Odd–Even Merging and Sorting on the Mesh
                                                                                 √       √
        In this section, we implement odd–even merging and sorting on a n × n
        mesh; odd–even merging and sorting on the PRAM were discussed in
        Section 2.11. Let A = a0 , a1 , . . . , an/2−1  and B = b0 , b1 , . . . , bn/2−1 
        be two sorted sequences of n distinct numbers, where n is a power of 4.
                                                                  √
        Initially, A and B are input in the ﬁrst and second n/2 columns of the
        mesh. The odd–even merging method is outlined in Algorithm meshod-
        devenmerge. k, the number of columns, is input to the algorithm. In
                                √
        the beginning, k = n, which is a power of 2. The algorithm divides
        the input into Aeven , Aodd , Beven , and Bodd , and each part occupies k/4
        columns. Next, Aodd and Bodd are interchanged, and the algorithm recur-
        sively merges Aeven with Bodd to produce C, and recursively merges Beven
        with Aodd to produce D. C and D are then shuﬄed into E, which is scanned
        from left to right (in one parallel step) for pairs that are out of order, which
        are ordered, if necessary.
            Notice that the algorithm is general for any mesh with k columns and
        √
          n rows, where k is a power of 2. We express the running time of the
                                                                            √
        algorithm in terms of the number of columns k, 2 ≤ k ≤ n. Step 1
                          √
        takes T (2) = Θ( n) time, which is the time needed to merge in a linear
                      √
        array with 2 n processors. Steps 2 and 3 take Θ(k) time, as data has to
        be routed from left to right and from right to left. Step 4 of interchanging
May 7, 2022    11:14         Parallel Algorithms          9in x 6in       b4591-ch04                     page 170




        170                                        Parallel Algorithms


          Algorithm 4.5 meshoddevenmerge
          Input: Two sorted sequences A = a0 , a1 , . . . , an/2−1  and B =
                 b0 , b1 , . . . , bn/2−1  of n/2 elements each sorted √
                                                                         in ascending order,
                 where n = 4k ≥ 4, number of columns k, 2 ≤ k ≤ n.
          Output: The elements in S = A ∪ B in sorted order.
              1. if k = 2 then merge the two columns using an algorithm for the linear
                                                                                   √
                 array to produce a sorted snake with two columns and n rows. Exit.
              2. Let Aeven = a0 , a2 , . . . , an/2−2  and Aodd = a1 , a3 , . . . , an/2−1  be the
                 even and odd subsequences         √ of A, respectively. Aeven and Aodd are snakes
                 with k/4 columns and n rows each.
              3. Let Beven = b0 , b2 , . . . , bn/2−2  and Bodd = b1 , b3 , . . . , bn/2−1  be the
                 even and odd subsequences of B, respectively. Beven and Bodd are snakes
                                                   √
                 with k/4 columns and n rows each.
              4. Interchange Aodd with Bodd . Thus Aeven and Bodd occupy the ﬁrst k/2
                 columns, and Beven and Aodd occupy the next k/2 columns.
              5. Recursively merge Aeven and Bodd to obtain C = c0 , c1 , . . . , cn/2−1 , a
                                                      √
                 mesh of k/2 columns and n rows.
              6. Recursively merge Aodd and Beven to obtain D = d0 , d1 , . . . , dn/2−1 , a
                                                      √
                 mesh of k/2 columns and n rows.
              7. Let E be the shuﬄe of C and D, that is,
                 E = c0 , d0 , c1 , d1 , . . . , cn/2−1 , dn/2−1 .
              8. Traverse the pairs (ci , di ) in E, 0 ≤ i ≤ n/2 − 1, and interchange the
                 elements in each pair if they are out of order to obtain the sorted sequence
                                                                                √
                 S = s0 , s1 , . . . , sn−1  in a mesh with k columns and n rows.
              9. return S



        columns takes Θ(k) time. Steps 5 and 6 take T (k/2) time. Step 7 of shuﬄing
        columns takes Θ(k) time. Step 8 takes Θ(1) time. Hence, the running time
        of the algorithm is governed by the recurrence T (k) = T (k/2) + Θ(k),
                                                              √                 √
        whose solution is T (k) = Θ(k) + T (2) = Θ(k) + Θ( n). When k = n,
           √          √
        T ( n) = Θ( n). The proof of correctness is given by Theorem 2.2 in
        Section 2.11.

        Example 4.4 Consider the mesh shown in Fig. 4.11. It consists of four
        rows and four columns. The ﬁrst input A is in the ﬁrst half of the mesh,
        in the ﬁrst two columns in a snakelike order. The second input B is in
        the last two columns in a snakelike order. A = 3, 5, 6, 9, 11, 13, 14, 16 and
        B = 1, 2, 4, 7, 8, 10, 12, 15. First we partition A and B into their even and
May 7, 2022   11:14         Parallel Algorithms     9in x 6in       b4591-ch04                    page 171




                                    The Linear Array and the Mesh                          171




                              (a)                 (b)                  (c)




                              (d)                 (e)                  (f)

                      Fig. 4.11.    An example of odd–even merging on the mesh.




        odd parts. The even parts are shown in shaded squares of Fig. 4.11(a).
        Thus, Aeven = {3, 6, 11, 14} is in the ﬁrst column (see part (b) of the ﬁgure)
        and Aodd = {5, 9, 13, 16} is in the second column. Beven = {1, 4, 8, 12} is
        shown in the third column, and Bodd = {2, 7, 10, 15} is in the last column.
        These are shown in part (b) of the ﬁgure. In part (c) of the ﬁgure, Aodd is
        interchanged with Bodd . So, the ﬁrst two columns are merged recursively
        to produce C = 2, 3, 6, 7, 10, 11, 14, 15 in snakelike order, and the last
        two columns are merged recursively to produce D = 1, 4, 5, 8, 9, 12, 13, 16
        in snakelike order. In Fig. 4.11(e), C and D are shuﬄed to produce
        E = 2, 1, 3, 4, 6, 5, 7, 8, 10, 9, 11, 12, 14, 13, 15, 16, which spans the four
        columns in a snakelike order. The pair (2, 1) is out of order, so 2 and 1 are
        exchanged. The same applies to the pair (6, 5), etc. The sorted sequence
        is S = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 shown in part (f) of the
        ﬁgure.                                                                               


             The algorithm for sorting is given as Algorithm meshoddevensort. It
        is similar to Algorithm oddevenmergesort for the PRAM in Section 2.11.
May 7, 2022    11:14          Parallel Algorithms          9in x 6in      b4591-ch04         page 172




        172                                         Parallel Algorithms


          Algorithm 4.6 meshoddevensort
          Input: A sequence S = a0 , a1 , . . . , an−1  where n is a power of 4.
          Output: The elements in S in sorted order.
              1.   S1 ← a0 , a1 , . . . , an/2−1 .
              2.   S2 ← an/2 , an/2+1 , . . . , an−1 .
              3.   S1 ← meshoddevensort(S1 )
              4.   S2 ← meshoddevensort(S2 )
              5.   S ← meshoddevenmerge(S1 , S2 )
              6.   return S



            The running time of the algorithm is governed by the recurrence T (n) =
                   √                                 √
        T (n/2)+Θ( n), whose solution is T (n) = Θ( n). The cost of the algorithm
             √
        is Θ( n) × n = Θ(n1.5 ).


        4.9        Routing on the Linear Array and the Mesh

        We consider the problem of permutation routing on the linear array and the
        mesh with n processors, in which every processor tries to send to a diﬀerent
        destination.

        4.9.1          Routing in the linear array
        Consider the problem of permutation routing in a linear array with n pro-
        cessors. Note that n − 1 is a lower bound on the worst case number of steps
        needed to route a packet at processor Pi to processor Pj , since i and j may
        be equal to 1 and n, respectively. Consider the following greedy method of
        routing a packet v from processor Pi to processor Pj . If Pj is to the left of
        Pi , then move v to the left one step, and if Pj is to the right of Pi , then move
        v to the right one step. This greedy approach is guaranteed to deliver v to
        Pj using the least number of steps, which is the distance between Pi and
        Pj , that is |i − j|. Note that no two packets moving in the same direction
        will contend for the same link. However, two packets may use the same
        (bidirectional) link if they are moving in opposite directions.
May 7, 2022   11:14         Parallel Algorithms   9in x 6in        b4591-ch04                  page 173




                                   The Linear Array and the Mesh                        173

        4.9.2         Deterministic routing in the mesh
                                                                        √     √
        The greedy algorithm for permutation routing in the n × n mesh is a
        generalization of that in the linear array. Let v be a packet to be routed
        from processor Pi,j to processor Pk,l . The algorithm consists of two phases.
        In the ﬁrst phase, v is routed along column j towards row k, which is
        the destination row. In the second phase, v is routed along row k towards
        its destination processor Pk,l . In each phase, a row or column is treated
                                  √
        like a linear array with n processors. In the ﬁrst phase, there is no con-
        tention on the links, which implies that all packets will arrive to their
                                      √
        destination row in at most n − 1 steps. In the second phase, however,
        many packets may pile up at an intermediate processor. For example, con-
        sider the case in which all processors in column 1 need to send to row
        √
           n/2. At each single step, processor P√n/2,1 receives two packets; one
        from the top and another from the bottom. This results in half of the
        incoming packets piling up at this intermediate processor. However, using
        the right protocol to arbitrate link contention results in an eﬃcient imple-
        mentation of phase 2. By giving priority to packets that need to go far-
                                                                                     √
        thest, routing in the second phase can be accomplished in at most n − 1
        steps. It follows that using the farthest-ﬁrst heuristic, all packets can be
                                                         √
        routed to their destinations in at most 2 n − 2 steps. To see this, con-
        sider the instance in which there is only one queue Q in row i. Let the
        packets stored in Q be ui,1 , ui,2 , . . ., where the ui,j ’s are sorted in decreas-
        ing order of the distance from their destinations. Initially, ui,1 is allowed
        to move to its destination without delay. Its destination can be as far as
        √
           n, which means the distance between Q and the target of ui,1 is at most
        √
           n − 1. In the next step, ui,2 is allowed to move to its destination with-
        out delay; it follows ui,1 and never collides with it. Note in this case that
                                                        √
        the destination of ui,2 can be as far as n − 1 because of the assump-
        tion of permutation routing. Hence, it will take ui,2 to reach its destination
             √         √
        1 + n − 2 = n − 1 steps. In general, it will take ui,k to reach its destina-
                                 √             √
        tion in at most k − 1 + n − k = n − 1 steps. The generalization to more
        than one queue is straightforward.
May 7, 2022   11:14        Parallel Algorithms           9in x 6in     b4591-ch04          page 174




        174                                      Parallel Algorithms

        4.9.3         Randomized routing on the mesh
        Although, as we have shown, the greedy algorithm is optimal in the sense
        that it uses the least amount of time, it suﬀers from large queues being built
        up at intermediate processors. This makes the greedy algorithm impractical.
        In this section, we show that using randomization, the maximum queue size
        can be reduced drastically without increasing the routing time substantially.
                                                                    √      √
        We show that, using randomization, the routing time is 3 n + o( n) using
        queues of size O(log n) with high probability. Let v be a packet with source
        Pi,j and destination Pk,l . The algorithm routes v in three phases:

        Phase 1: Route v to a random intermediate processor in column j, say
                 processor Pr,j .
        Phase 2: Send v along the same row r to its destination column l.
        Phase 3: Send v to its ﬁnal destination, i.e., to processor Pk,l .

            In phase 1, assume that edge contention is resolved using the farthest-
        ﬁrst protocol. Thus, each packet moves without contention to its randomly
        chosen row, and thus suﬀers no delays. Hence, as discussed in Section 4.9.1
                                                                             √
        for routing in the linear array, phase 1 is completed within n − 1 steps
        or less.
            We will assume that edge contention in phase 2 is resolved by giving
        priority at a processor to the packet which most recently entered that pro-
        cessor. Thus, once a packet starts moving in a row, it never stops until
        it reaches its destination column. Consider a packet that starts moving
        at processor Pr,j in phase 2. This packet may be delayed by all packets
                                                                                √
        originating at processors Pr,1 , Pr,2 , . . . , Pr,j . There are at most n pack-
        ets at the end of phase 1 at processor Pr,j in column j. Each packet at
        column j ends up at processor Pr,j with probability √1 . For 1 ≤ s ≤ j,
                                                                         n
        deﬁne the random variable Xr,s to be the number of packets at processor
        Pr,s at the start of phase 2. Then, Xr,s has the binomial distribution with
                   √
        E[Xr,s ] = n × √1n = 1 (see Section A.4.3). Let

                                                        
                                                        j
                                                 Yj =         Xr,s .
                                                        s=1
May 7, 2022   11:14     Parallel Algorithms          9in x 6in       b4591-ch04                       page 175




                               The Linear Array and the Mesh                                    175

        That is, Yj counts the total number of packets at processors Pr,1 , Pr,2 ,
        . . . , Pr,j at the start of phase 2. By linearity of expectations (see
        Section A.4.3),
                                      j     
                                               
                                                j             
                                                              j
                      μ = E[Yj ] = E     Xr,s =   E [Xr,s ] =   1 = j.
                                         s=1            s=1             s=1

        (See Section A.4.3). Now, we can apply Chernoﬀ bound in Theorem A.3 to
        the probability of there being a substantial number of packets delaying a
        particular packet v at processor Pr,j . The Chernoﬀ bound is
                                                                
                                         
                                         j
                                                                         2
         Pr [Yj > (1 + δ)μ] = Pr               Xr,s > (1 + δ)μ < e−μδ        /4
                                                                                  ;   (δ < 2e − 1).
                                         s=1
                                                            
        We compute the probability that v is delayed by j+ 4(c + 1)j √ ln n packets,
                                                                        4(c+1)j ln n
        c > 0. So, we require that (1+δ)μ = j + 4(c + 1)j ln n or δ =        j        .
        That is,
                                               √           2
                                              −
                                                                  = e−(c+1) ln n
                                                   4(c+1) ln n /4
             Pr Yj > j + 4(c + 1)j ln n < e
                                                                              = n−c−1 .
                                                            
            Thus, the probability that v is delayed by j + 4(c + 1)j ln n packets
        is bounded above by n−c−1 , c > 0. This is a     bound for the probability
        that a given packet is delayed more than j + 4(c + 1)j ln n steps. But
        we want to geta bound for the probability that no packet gets delayed
        more than j + 4(c + 1)j ln n steps. For that, it is enough to use Boole’s
        inequality for probabilities as a bound (Eq. (3.1)): There are n packets in
        total,
             and the probability that one of these packets is delayed by more than
        j + 4(c + 1)j ln n steps is bounded above by n × n−c−1= n−c , c > 0.
        Notice that if a packet
                            at processor √  Pr,j is delayed by j + 4(c + 1)j ln n,
        then it takes j + 4(c + 1)j ln n + n − j steps for this packet to reach
                                            √        √
        its correct column. This is at most n + o( n) steps. So we can make the
        following assertion: With probability at least 1 − n1c every packet reaches
                                   √       √
        its phase 2 destination in n + o( n) or fewer steps.
May 7, 2022   11:14     Parallel Algorithms          9in x 6in         b4591-ch04        page 176




        176                                   Parallel Algorithms

            In phase 3, each packet is in its correct column, and there is at most one
        packet destined for each processor. We will assume that edge contention in
        phase 3 is resolved using the farthest-ﬁrst protocol. Hence, this phase is
                           √
        completed within n − 1 steps or less. Thus, the overall running time of
                                        √        √
        the randomized algorithm is 3 n + o( n) with probability at least 1 − n1c ,
        c > 0.
            Now, we bound the queue size in all processors. At the end of phase 1 and
        during phase 2, the number of packets that can accumulate at any processor
                    √
        is at most n. Recall that the random variable Xr,s denotes the number
        of packets at processor Pr,s at the start of phase 2, and that Xr,s has the
                                                     √
        binomial distribution with μ = E[Xr,s ] = n × √1n = 1 (see Section A.4.3).
        Now, we can apply the Chernoﬀ bound in Theorem A.3 to the probability
        of there being a substantial number of packets at processor Pr,s at the end
        of phase 1 and during phase 2. The Chernoﬀ bound is

                        Pr [Xr,s > (1 + δ)μ] < 2−δμ ;              (δ > 2e − 1).

           We compute the probability that there are more than 1 + (1 + c) log n
        packets, c > 0, at processor Pr,s , where μ = 1. So, we require that (1+δ)μ =
        1 + (1 + c) log n, or δ = (1 + c) log n. That is,

                         Pr [Xr,s > 1 + (1 + c) log n] < 2−(1+c) log n
                                                                 = n−(1+c) .

        Using Boole’s inequality (Eq. (3.1)), the probability that there is at least
        one processor with queue size more than 1 + (1 + c) log n is at most n ×
        n−(1+c) = n−c . It follows that in phases 1 and 2, the queue size is at
        most 1 + (c + 1) log n = O(log n) with probability at least 1 − n1c . Since
        queues can never increase during phase 3, the queue size during this phase is
        O(log n).
                                                                          √      √
           In summary, the above randomized algorithm runs in time 3 n+ o( n)
        steps and uses queues of size O(log n) with probability at least 1 − O(1/nc ),
                                                         √          √
        c > 0. The running time can√be reduced to 2 n + o( n) by dividing
        each column to strips of size lognn and routing each packet in phase 1 to
        a random location in its own strip. The analysis is similar to the above.
        Thus, we conclude that there is a randomized algorithm that runs in time
May 7, 2022   11:14        Parallel Algorithms             9in x 6in         b4591-ch04         page 177




                                  The Linear Array and the Mesh                           177

          √       √
        2 n + o( n) steps and uses queues of size O(log n) with probability at
        least 1 − O(1/nc ), c > 0.


        4.10      Matrix Multiplication on the Mesh

        Consider the problem of matrix multiplication on the mesh: Given two
                                        √    √
        square matrices A and B of order n × n, ﬁnd their product C = AB.


        4.10.1        The first algorithm
        In this section, we show how to perform matrix multiplication C = AB of
                      √      √          √         √
        dimensions n × n on a 2 n × 2 n mesh. It is assumed that matrix A
        is stored in the lower-left quadrant, matrix B is stored in the upper-right
        quadrant, and the resultant matrix C is to be computed in the lower-right
                                                   √
        quadrant (see Fig. 4.12 for the case n = 4).
            Initially, the values of the ci.j ’s are set to 0. At time 1, row 1 of matrix A
        moves one step to the right and column 1 of matrix B moves one step down,
        and the product of a1,√n b√n,1 is computed and added to c1,1 . At time 2,
        row 1 of matrix A and column 1 of matrix B continue moving in the same
        directions, and row 2 of matrix A and column 2 of matrix B start moving
        left to right, and top down, respectively. In general, at time k, the kth
        row of matrix A and the kth column of matrix B start moving right and

                                                            b11   b12 b13   b14
                                                            b21   b22 b23   b24

                                                            b31   b32 b33   b34

                                                            b41   b42 b43   b44

                                     a11   a12 a13   a14    c11   c12 c13   c14
                                     a21   a22 a23   a24    c21   c22 c23   c24

                                     a31   a32 a33   a34    c31   c32 c33   c34

                                     a41   a42 a43   a44    c41   c42 c43   c44


                        Fig. 4.12.    Matrix multiplication, the ﬁrst algorithm.
May 7, 2022   11:14          Parallel Algorithms          9in x 6in        b4591-ch04                    page 178




        178                                        Parallel Algorithms

          Table 4.1.       Computing c1,1 and c1,2 by the ﬁrst matrix multiplication
          algorithm.

          Time                         c11                                       c12

          1           a14 b41                                    0
          2           a14 b41 + a13 b31                          a14 b42
          3           a14 b41 + a13 b31 + a12 b21                a14 b42 + a13 b32
          4           a14 b41 + a13 b31 + a12 b21 + a11 b11      a14 b42 + a13 b32 + a12 b22
          5           a14 b41 + a13 b31 + a12 b21 + a11 b11      a14 b42 + a13 b32 + a12 b22 + a11 b12



        down, respectively. Each processor Pi,j upon receiving data from its left
        and top neighbors, computes the product of these values and adds them to
        the partial sum ci,j . At time k + 1, each processor sends the values received
        during time k to its neighboring processors in the direction they are moving.
                            √        √                                √
        Therefore, at time n, the nth row of matrix A and the nth column of
                                                                               √
        matrix B start moving right and down, respectively, and additional n − 1
        steps are needed to reach the processor holding c√n,√n . Clearly, the running
                                     √
        time of the algorithm is Θ( n).

        Example 4.5 Table 4.1 shows the results of the computations of c1,1 and
        c1,2 using the ﬁrst matrix multiplication algorithm. The values of c1,1 and
        c1,2 are determined incrementally starting at 0. Note that some of the other
        computations for the rest of the ci,j ’s are done concurrently.           


        4.10.2        The second algorithm
        In this section, we show how to compute the matrix product C = AB of
                    √      √                                    √     √
        dimensions n × n on a mesh of size n, that is, a n × n mesh. Assume
        that the mesh is a wrap-around mesh (torus), so additions of indices are to
                                 √
        be carried out modulo n. Initially, the input matrices are stored in the
        mesh, where processor Pi,j holds the elements ai,j and bi,j , and the output
        elements are to be stored in ci,j . The algorithm consists of two phases; the
        ﬁrst phase is the shifting phase, and the second phase is the multiplication
        phase.
            In the shifting phase, the ai,j ’s are shifted to the left, and the bi,j ’s are
        shifted upwards as follows. The a1,j ’s in the ﬁrst row are shifted to the
        left by one position, those a2,j ’s in the second row by two positions, and in
May 7, 2022   11:14         Parallel Algorithms     9in x 6in          b4591-ch04                   page 179




                                   The Linear Array and the Mesh                              179


                      a11   a12         a13   a14               a12   a13         a14   a11
                      b11   b12         b13   b14               b21   b32         b43   b14
                      a21   a22     a23       a24               a23   a24         a21   a22
                      b21   b22     b23       b24               b31   b42         b13   b24
                      a31   a32         a33   a34               a34   a31         a32   a33
                      b31   b32         b33   b34               b41   b12         b23   b34

                      a41   a42     a43       a44               a41   a42         a43   a44
                      b41   b42     b43       b44               b11   b22         b33   b44

                                  (a)                                       (b)

        Fig. 4.13. Matrix multiplication, the second algorithm. (a) Initial input.
        (b) After the shifting phase.


        general, the elements ai,j in the ith row are shifted to the left by i positions.
        The bi,1 ’s in the ﬁrst column are shifted upwards by one position, those
        bi,2 ’s in the second column by two positions, and in general, the elements
        bi,j in the jth column are shifted upwards by j positions. So, the data
        is rearranged so that processor Pi,j holds ai,i+j and bi+j,j . Figure 4.13(a)
        shows the initial input, and Fig. 4.13(b) shows the input after the shifting
        phase.
             In the multiplication  phase, P1,1 evaluates c1,1 by computing the dot
                           √n
        product c1,1 =       k=1 a1,k bk,1 as in the traditional matrix multiplication
        method. It does this using the following steps (see Fig. 4.13(b)):

         (1) Set c1,1 ← a1,2 b2,1 .
        (2) Shift the ﬁrst row to the left and the ﬁrst column upwards, and set
             c1,1 ← c1,1 + a1,3 b3,1 .
        ..
         .

        (3) Shift the ﬁrst row to the left and the ﬁrst column upwards, and set
                                                         √
            c1,1 ← c1,1 + a1,√n b√n,1 (in Fig. 4.13(b), n = 4).
        (4) Shift the ﬁrst row to the left and the ﬁrst column upwards, and compute
            the ﬁnal result c1,1 ← c1,1 + a1,1 b1,1 .

        The computation of the rest of ci,j ’s is done in a similar fashion. The algo-
        rithm is shown as Algorithm meshmatrixmult. For clarity, the ai,j ’s and
        bi,j ’s will be renamed so that the contents of Pi,j after shifting will be
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch04                 page 180




        180                                    Parallel Algorithms


          Algorithm 4.7 meshmatrixmult
                    √    √
          Input: Two n × n matrices A and B.
          Output: The product C = A × B.
                             √
           1. for i ← 1 to n do in parallel
           2.     Shift row i to the left i positions
           3. end for         √
           4. for j ← 1 to n do in parallel
           5.     Shift column j upwards j positions
           6. end for        √
           7. for i ← 1 to n √    do in parallel
           8.     for j ← 1 to n do in parallel
           9.         ci,j ← ai,j bi,j
          10.     end for
          11. end for         √
          12. for k ← 1 to n√     − 1 do in parallel
          13.     for i ← 1 to n √     do in parallel
          14.         for j ← 1 to n do in parallel
          15.              ai,j ← ai,j+1
          16.              bi,j ← bi+1,j
          17.              ci,j ← ci,j + ai,j bi,j
          18.         end for
          19.     end for
          20. end for



        called ai,j and bi,j . Recall that additions of indices are to be carried out
                 √
        modulo n.
                                                                           √
            Clearly, both the ﬁrst phase and the second phase take Θ( n) time,
                                                                    √
        and hence the running time of the entire algorithm is Θ( n)


        4.11      Computing the Transitive Closure on the Mesh
                    √     √
        Let A be a n × n adjacency matrix of a directed graph G. The transitive
                                               √         √
        closure of G is represented as a n × n Boolean matrix A∗ in which
        A∗ (i, j) = 1 if and only if there is a path in G from i to j, where we
                                                           √
        assume that the set of vertices is {1, 2, . . . , n}. Computing the transitive
        closure is critical to a variety of eﬃcient solutions to fundamental graph
        problems.
            Deﬁne Ak (i, j) to be 1 if and only if there is a path from i to j that
        passes by vertices in the set {1, 2, . . . , k}, and 0 otherwise. A0 (i, j) = A(i, j)
May 7, 2022    11:14        Parallel Algorithms     9in x 6in          b4591-ch04                     page 181




                                   The Linear Array and the Mesh                                181


        is 1 if and only if there is an edge in G from i to j. Deﬁne aki,j = Ak (i, j).
        Ak (i, j) is computed from the recurrence

              Ak (i, j) = Ak−1 (i, j) ∨ (Ak−1 (i, k) ∧ Ak−1 (k, j));     A0 (i, j) = A(i, j).
                                                                                           (4.1)

        By Eq. (4.1), we see that

                 Ak (k, k) = Ak−1 (k, k) ∨ (Ak−1 (k, k) ∧ Ak−1 (k, k)) = Ak−1 (k, k), (4.2)
                 Ak (k, j) = Ak−1 (k, j) ∨ (Ak−1 (k, k) ∧ Ak−1 (k, j)) = Ak−1 (k, j), (4.3)

        and

               Ak (i, k) = Ak−1 (i, k) ∨ (Ak−1 (i, k) ∧ Ak−1 (k, k)) = Ak−1 (i, k).        (4.4)

            The algorithm to be presented makes use of Eqs. (4.1)–(4.4) to compute
        the transitive closure of A eﬃciently in parallel. Assume the n processors
                                                                             √
        are numbered P1,1 , P1,2 , . . . , P√n,√n . The algorithm consists of n phases,
        where in phase k, the rows of Ak are computed from the rows of Ak−1 for
                  √
        1 ≤ k ≤ n. The rows of the matrix A0 = A are entered from the top of
        the mesh starting from row 1 one at a time (see Fig. 4.14(a)), and travel
        in a systolic fashion to the bottom of the mesh, where they exit starting
        from row 1. We will distinguish between two states of matrix rows. The ﬁrst
        state is the “unmarked” state, where all rows are in the unmarked state by
        default. So, all rows start as unmarked rows once they enter the mesh from
        the top. The second state is the “marked” state. Matrix row i enters the
        marked state once it bypasses all marked rows, and stops moving downward
        when reaching row i of the mesh in step 2i − 1. It stays as a marked row
                                                                   √
        until all other rows in the matrix pass over it at step n + 2i − 1, where it
        becomes an unmarked row and starts moving downward again towards the
        bottom of the mesh.
            Consider Fig. 4.14 in which the process is shown using a mesh with four
        rows. The shaded rectangles are marked rows, while the small white rectan-
        gles are unmarked rows. First, row 1 of the input matrix A0 is entered into
        row 1 of the mesh (see Fig. 4.14(b)). It immediately becomes a marked row
                                                                              √
        in step 1. The ﬁrst phase commences next where rows 2, 3, . . . , n, which
        are unmarked rows, pass over the ﬁrst marked row (see Figs. 4.14(c)–(e)).
        Consider the ﬁrst time unmarked row 2 is moved to the ﬁrst row of
May 7, 2022   11:14             Parallel Algorithms             9in x 6in         b4591-ch04                    page 182




        182                                           Parallel Algorithms


                      row 4
                      row 3              row 4
                      row 2              row 3
                  (a) row 1          (b) row 2          (c) row 4       (d)            (e)
                                                            row 3             row 4
                                           row 1            row 1             row 1            row 1
                                                            row 2             row 3            row 4
                                                                                               row 2
                                                                              row 2            row 3




                  (f)                (g)                (h)             (i)            (j)

                        row 1
                        row 2              row 2
                        row 4              row 1              row 2
                        row 3              row 3              row 3           row 3            row 3
                                           row 4              row 1           row 2
                                                              row 4           row 4            row 4
                                                                              row 1            row 2
                                                                                               row 1
                  (k)                (l)                (m)


                                                                                      Marked rows


                        row 4                                                         Unmarked rows
                        row 3              row 4
                        row 2              row 3              row 4
                        row 1              row 2              row 3
                                           row 1              row 2
                                                              row 1
                                                                                                   √
              Fig. 4.14.    Computing the transitive closure on the mesh, where                        n = 4.


        the mesh next to the marked row 1 (see Fig. 4.14(c)). First, processor
        P1,1 broadcasts a02,1 to all other processors in the ﬁrst row of the mesh.
                                            √
        Next, for each j, 1 ≤ j ≤ n, a02,j is updated to a12,j using the formula
        a12,j = a02,j ∨ (a02,1 ∧ a01,j ). Next, row 2 of the matrix is moved to row 2 of the
        mesh and becomes a marked row (Fig. 4.14(d)). Later, when unmarked row i
        meets marked row 1, processor P1,1 broadcasts a0i,1 to all other processors
                                                                        √
        in the ﬁrst row of the mesh. Next, for each j, 1 ≤ j ≤ n, a0i,j is updated to
        a1i,j using the formula a1i,j = a0i,j ∨ (a0i,1 ∧ a01,j ). As the unmarked rows of A0
        pass over the ﬁrst marked row, they are thus updated to become the rows
        of A1 . Once processing row i is complete by marked rows 1, 2, . . . , i − 1, it
        is moved to row i of the mesh and becomes a marked row (see, for example,
        row 3 in Fig. 4.14(f)).
May 7, 2022   11:14         Parallel Algorithms          9in x 6in            b4591-ch04                  page 183




                                     The Linear Array and the Mesh                                  183


            It should be emphasized, however, that, by Eq. (4.3), the kth row is not
        processed during the kth phase. This is why, for example, row 1 was not
        processed in the ﬁrst phase. In general, the kth phase is accomplished as
                                                          √
        rows 1, 2, . . . , k − 1 and k + 1, k + 2, . . . , n pass over the marked row k in
        some order. By the time an unmarked row reaches the kth marked row in
        the kth row of the mesh, it has already been updated to be a row of Ak−1 .
        (See Figs. 4.14(d)-(h)). As the unmarked ith row passes over the marked
        kth row, processor Pk,k broadcasts ak−1       i,k to all processors in the kth row
        of the mesh. Processor Pk,j in this row can update ak−1      i,j using the formula
        ai,j = ai,j ∨ (ai,k ∧ ak,j ).
         k       k−1         k−1    k−1

            Recall that marked row i will be stored in the ith row of the mesh in
                                    √
        step 2i − 1, and at step n+ 2i − 1, it becomes an unmarked row and begins
                                                                                 √
        moving downward. It will exit the mesh from the bottom at step 2 n+i−1.
        The remaining parts of Figs. 4.14 depict the rest of the algorithm. It follows
                                  √                                                     √
        that after a total of 3 n − 1 steps excluding data broadcasting, A∗ = A n
        will have been output from the bottom of the mesh. This implies that the
                                       √
        overall running time is Θ( n) excluding data broadcasting.
            Broadcasting of data items can be accomplished eﬃciently by interleav-
        ing it with updating the matrix elements. Figure 4.15 shows how broad-
        casting at multiple rows can be interleaved with row computations. In
        Fig. 4.15(a), broadcasting of ak4,k in a 4 × 4 mesh is shown for 0 ≤ k ≤ 3.
        Note that broadcasting many elements can take place concurrently in the
        same row. For example, broadcasting of a03,1 may be in progress in row 1
        while a04,1 is moving to the right.
            In Fig. 4.16, the overall data ﬂow for the construction of transitive clo-
        sure is shown without the details of synchronization; delays are required in

                               0
                              a4,1
                      (a)
                                                                        (b)     ai,k-1
                                       1                                             k
                                      a4,2

                                                   2                 ai,k-1
                                                                          k
                                                                                           ai,k-1
                                                                                                k
                                                  a4,3

                                                          3
                                                         a4,4



        Fig. 4.15. Interleaving broadcasting with updating elements of the transitive
        closure in a 4 × 4 mesh.
May 7, 2022    11:14          Parallel Algorithms            9in x 6in              b4591-ch04            page 184




        184                                         Parallel Algorithms


                       (a)

                                                                              (b)     ai,k-1
                                                                                           j



                                                                         ai,k-1
                                                                              k       ak,k-1j    ai,k-1
                                                                                                      k



                                                                                       ai,k j



              Fig. 4.16.     Data ﬂow in computing the transitive closure in a 4 × 4 mesh.


                                             1           2        5
                                                                          7

                                             4           3        6

                 Fig. 4.17.     An undirected graph with three connected components.


        some data transmissions. As shown in Fig. 4.16(b), each processor computes
                                         i,j ∨ (ai,k ∧ ak,j ). Thus, the construction
        aki,j using the formula aki,j = ak−1     k−1    k−1

        of the transitive closure is not performed row by row; each element of the
        matrix moves downward independently.
             The foregoing description implies that the overall running time of the
                                                √     √
        construction of transitive closure on a n × n mesh, including broadcast-
                   √
        ing, is Θ( n).


        4.12       Connected Components

        Let G = (V, E) be an undirected graph with adjacency matrix A and tran-
        sitive closure matrix A∗ = {ai,j }. A∗ partitions V into connected compo-
        nents, where two vertices ai and aj are in the same connected component
        if and only if there is a path in G between them, that is, if and only if
        a∗i,j = 1. Figure 4.17 shows a graph with three connected components.
        Thus, to compute the connected components of G, we compute the tran-
        sitive closure A∗ . For example, the connected components in Fig. 4.17 are
        {1, 2, 3, 4}, {5, 6}, {7}.
May 7, 2022    11:14        Parallel Algorithms    9in x 6in       b4591-ch04                   page 185




                                   The Linear Array and the Mesh                         185

        4.13       Shortest Paths

        Let G = (V, E) be a weighted directed graph on n vertices with no loops, in
        which each edge (i, j) has a weight w[i, j]. If there is no edge from vertex i
        to vertex j, then w[i, j] = ∞. For simplicity, we will assume that V =
                      √
        {1, 2, . . . , n}. We assume that G does not have negative weight cycles,
        that is, cycles whose total weight is negative. The problem is to ﬁnd the
        distance from each vertex to all other vertices, where the distance from
        vertex i to vertex j is the length of a shortest path from i to j. Let i
        and j be two diﬀerent vertices in V . Deﬁne Ak (i, j) to be the shortest
        distance from i to j that passes by vertices in the set {1, 2, . . . , k}, and
        A0 (i, j) = w(i, j). Ak (i, j) is computed from the recurrence

          Ak (i, j) = min{Ak−1 (i, j), Ak−1 (i, k) + Ak−1 (k, j)};      A0 (i, j) = A(i, j).
                                                                                        (4.5)
        By Eq. (4.5), we see that

          Ak (k, j) = min{Ak−1 (k, j), Ak−1 (k, k) + Ak−1 (k, j)} = Ak−1 (k, j), (4.6)

        and

              Ak (i, k) = min{Ak−1 (i, k), Ak−1 (i, k) + Ak−1 (k, k)} = Ak−1 (i, k).   (4.7)

           Notice the resemblance between Eqs. 4.1–4.4 and Eqs. 4.5–4.7. Hence,
                                                        √     √
        the algorithm for transitive closure on the n × n mesh discussed in
        Section 4.11 can be used with simple modiﬁcations. Speciﬁcally, ∨ and ∧ in
        Eqs. 4.1–4.4 and the rest of the algorithm for transitive closure are replaced
        by min and +. It follows that computing all shortest paths can be eﬀected
             √               √      √
        in Θ( n) time on a n × n mesh, which is optimal.


        4.14       Computing the Convex Hull of a Set of Points
                   on the Mesh
                                                                                     √ √
        Let S = {p1 , p2 , . . . , pn } be a set of n points in the plane stored in a n× n
        mesh one point per processor, where n is a power of 4. For deﬁnitions related
        to the convex hull, refer to Section 2.20; In this section, we present two
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch04                 page 186




        186                                      Parallel Algorithms

                                                                       √     √
        algorithms for computing the convex hull of S, CH(S), on the n × n
                                      √                                 √
        mesh; the ﬁrst runs in time O( n log n) and the other in time Θ( n).



        4.14.1        The first algorithm
        The ﬁrst algorithm is almost a straightforward implementation of the
        PRAM algorithm presented in Section 2.20, and given in Algorithm par-
        convexhull. The algorithm consists of repeated applications of the steps
        given in Observations 2.2 and 2.3.
            As a preprocessing step, the points in S are ﬁrst sorted in ascend-
                                                    √
        ing order of their x-coordinates in Θ( n) time. So, assume that x(p1 ) ≤
        x(p2 ) ≤ . . . ≤ x(pn ), where x(pi ) denotes the x-coordinate of point pi . We
        will assume for simplicity that no three points of S are collinear, and no
        two points have the same x-coordinate. Next, the set of points S is divided
        into four parts S1 = {p1 , p2 , . . . , pn/4 }, S2 = {pn/4+1 , pn/4+2 , . . . , pn/2 },
        S3 = {pn/2+1 , pn/2+2 , . . . , p3n/4 } and S4 = {p3n/4+1 , p3n/4+2 , . . . , pn },
        and arranged in the mesh as shown in Fig. 4.18(b). Now, we recursively
        determine the four convex hulls of the four parts CH(S1 ), CH(S2 ), CH(S3 )
        and CH(S4 ). Figure 4.18(c) shows the four convex hulls of the points in
        part (a) of the ﬁgure.
            From CH(S1 ) and CH(S2 ), we identify CH(S1 ∪ S2 ), and denote the
        set of vertices representing S1 ∪ S2 as P . From CH(S3 ) and CH(S4 ), we
        identify CH(S3 ∪ S4 ), and denote the set of vertices representing S3 ∪ S4 as
        Q. From CH(P ) and CH(Q), we identify CH(P ∪ Q), which is the desired
        convex hull CH(S). In what follows, we turn our attention to computing
        the upper hull of P , U H(P ). Computing the lower hull of P , LH(P ), and
        hence CH(P ) can be determined in a similar fashion and in parallel with
        U H(P ). Finally, ﬁnding CH(Q), and hence CH(S) can be achieved by a
        similar means.
            The steps for ﬁnding U H(P ) and hence LH(P ) are similar to those
        described in Section 2.20. In each iteration of the binary search, vertex xi
        of U H(S1 ) is broadcast to the processors holding the vertices of U H(S2 )
        and one of those processors succeeds in ﬁnding√its tangent√        line xi vi with
                                          √                   n    n
        U H(S2 ). Clearly, this takes Θ( n) time on the 2 × 2 mesh. Since there
        are O(log n) iterations in the binary search for ﬁnding the upper common
                                                                                √
        tangent, the overall running time for ﬁnding this tangent is O( n log n).
        Recall that the computation of LH(P ) is done in parallel with that of
May 7, 2022   11:14             Parallel Algorithms                 9in x 6in            b4591-ch04              page 187




                                          The Linear Array and the Mesh                                    187


                      (a)


                                                                                                       v
                            u




                                     S1                 S2                      S3              S4


                                                  (b)
                                                               S1         S2


                                                               S4         S3




                      (c)

                                                                                                       v
                            u



                                 UH(S1)               UH(S2)                    UH(S3)        UH(S4)

        Fig. 4.18. (a) The set of points S. (b) Arrangement of the subsets on the mesh.
        (c) Convex hulls of S1 , S2 , S3 and S4 .


        U H(P ). Clearly, the remaining work of ﬁnding U H(P ) and then CH(P )
                 √
        takes Θ( n) time. Hence the overall running time for ﬁnding CH(P ) from
                                 √
        CH(S1 ) ad CH(S2 ) is O( n log n). It should be noted that ﬁnding CH(P )
        and CH(Q) are done concurrently, and it remains to ﬁnd CH(S), which
        asymptotically takes the same running time. It follows that the running
                                                                       √
        time of the algorithm obeys the recurrence T (n) = T (n/4) + O( n log n) =
           √
        O( n log n).


        4.14.2        The second algorithm
        The algorithm to be presented is similar to the ﬁrst algorithm. However,
        the main diﬀerence is in the binary search and how it is conducted.In this
        algorithm, the number of elements considered in iteration i is O( n/2i ),
May 7, 2022   11:14     Parallel Algorithms          9in x 6in      b4591-ch04            page 188




        188                                   Parallel Algorithms

                            √
        which results in Θ( n) running time for the binary search. This is to be
                                                                          √
        contrasted to the ﬁrst algorithm in which each iteration takes O( n) for a
                    √
        total of O( n log n).
           In what follows, we describe in detail ﬁnding the tangents using binary
        search for the two sets S1 and S2 . The rest of the algorithm is similar to
        that of the ﬁrst algorithm.
           The correctness of the algorithm hinges on the following lemma (see
        Fig. 4.19). Here v and u are the vertices with the minimum and maximum
        x-coordinates in CH(S1 ), respectively, and v  and u are the vertices with
        minimum and maximum x-coordinate in CH(S2 ), respectively.


        Lemma 4.1 Let w be a vertex of CH(S1 ). If there is another vertex w
        of CH(S2 ) such that ww is the common upper tangent of CH(S1 ) and
        CH(S2 ), then all vertices in CH(S2 ) must lie below the line passing by xw
        and some points in CH(S2 ) must lie above the line passing by wy, where x
        and y are the two vertices in CH(S1 ) immediately succeeding and preceding
        w in counterclockwise order.

        Proof.    The tangent line must lie entirely within the wedge deﬁned by
        xw and wy. If xw is not above all points in CH(S2 ), then any line that
        passes by w and lies entirely inside the wedge either intersects CH(S2 ) at
        more than one point or lies below the line v  u . On the other hand, if wy is
        above CH(S2 ), then this wedge does not contain a point from CH(S2 ). In
        both cases, there does not exist a common upper tangent ww of CH(S1 )
        and CH(S2 ).                                                                 

            Lemma 4.1 suggests the following method for identifying the vertex w.
        We perform binary search on the set of vertices of CH(S1 ). Initially, w is
        assigned the hull vertex in CH(S1 ) that is half the way between u and
        v in counterclockwise order. Next, in each iteration, we do one of the
        following according to the result of the test implied by Lemma 4.1 (see
        Fig. 4.19).

        (a) If all vertices in CH(S2 ) lie below the line passing by xw and some
            points in CH(S2 ) lie above the line passing by wy, then w, x and y
            have been identiﬁed.
        (b) If xw is not above CH(S2 ), then assign the vertex x to u and recompute
            w as the middle between u and v in counterclockwise order.
May 7, 2022   11:14           Parallel Algorithms              9in x 6in     b4591-ch04              page 189




                                     The Linear Array and the Mesh                             189




                                        w                                       w’

                          x
                                                     y
                                                                v’
                                                                           CH(S2)
                                   CH(S1)                  u                              u’
                      v




                                        Fig. 4.19.       Proof of Lemma 4.1

        (c) If (a) above is not satisﬁed and xw is above CH(S2 ), then assign ver-
            tex y to v and recompute w as the middle between u and v in counter-
            clockwise order.
        Example 4.6 Consider Fig. 4.20 in which the steps of binary search are
        shown. In Fig. 4.20(a), the two convex hulls are shown. w is set half the
        way between v and u, in counterclockwise order. The extension of the line
        xw crosses CH(S2 ) at more than one point. Hence, the vertex x is assigned
        to u. w is recomputed as half the way between u and v and x and y are
        relocated as shown in Fig. 4.20(b). y is assigned to the vertex before w in
        counterclockwise order, which happens to be u. Next, since the extensions
        of both xw and wy are above CH(S2 ), v is set equal to y in Fig. 4.20(c).
        Then, w, x and y are recomputed as shown in Fig. 4.20(c). In this part of
        the ﬁgure, u = v = w, and the test in (a) above is satisﬁed, so the search is
        halted, and w is declared as one end of the tangent line.                   
                                                                            √
            If we perform binary search naturally, each iteration takes Θ( n) for a
                    √
        total of Θ( n log n). Hence, an approach is needed to reduce the running
        time. We will succeed if we can reduce the running time of the ith iteration
        of binary search to Θ( n/2i ). Luckily, this can be done by eliminating
        half of the vertices in CH(S1 ) and CH(S2 ) from future consideration by
        binary search. Thus, after the end of each iteration of the binary search, the
        remaining vertices in CH(S1 ) and CH(S2 ) are compressed using parallel
        preﬁx. Hence, in the ith iteration, the binary search isperformed on Θ(n/2i )
        vertices, which means that the ith iteration takes Θ( n/2i ) time, including
        the time required for broadcasting and data compression. This implies that
May 7, 2022   11:14                   Parallel Algorithms             9in x 6in   b4591-ch04   page 190




        190                                                 Parallel Algorithms


                 (a)                         x          w

                                                                  y


                                                 CH(S1)                  u          CH(S2)
                      v




                 (b)                         u=y
                                  w


                          x

                                                 CH(S1)                            CH(S2)
                      v




                                       u=v=w
                 (c)                                y
                              x




                                            CH(S1)                                CH(S2)




                                            Fig. 4.20.      Example of binary search.

                                                        O(log n) 
        the total running time for the binary search is i=0          Θ( n/2i ), which
             √
        is Θ( n).
            Note that in each iteration, w, x and y are broadcast to the processors
        holding hull vertices in CH(S2 ) above the line u v  . Then, the equations of
        the two lines xw and wy are computed. The results of the tests given in
        (a)–(c) above are sent to the vertices of CH(S1 ) above the line vu.
May 7, 2022   11:14       Parallel Algorithms   9in x 6in        b4591-ch04             page 191




                                 The Linear Array and the Mesh                   191

             Similar computations of all the above are performed to identify
        w , w x , y  w for CH(S2 ). It is important that identifying w and w be
        done simultaneously, and so is data compression for the remaining data of
        CH(S1 ) and CH(S2 ). This is to ensure that half the number of hull vertices
        after compression in CH(S1 ) between v and u and in CH(S2 ) between v 
        and u are eliminated from further inspection in subsequent iterations of
        the two binary searches.
             Let P = S1 ∪ S2 and Q = S3 ∪ S4 . Now, we construct CH(P ) =
        CH(S1 ) ∪ CH(S2 ) by connecting w and w and z and z  by two edges,
        where zz  is the lower tangent. Also, the vertices inside the quadrilateral
        deﬁned by w, w , z and z  are removed. At the same time, we construct
        CH(Q) = CH(S3 ) ∪ CH(S4 ), and ﬁnally CH(S)= CH(P ) ∪ CH(Q). Note
        that the computations of CH(P ) and CH(Q) are done concurrently. The
        above discussion implies that the overall running time of the algorithm
                                                       √        √
        obeys the recurrence T (n) = T (n/4) + Θ( n) = Θ( n).


        4.15      Labeling Connected Components

        In this section, we consider the problem of labeling ﬁgures, i.e., connected
        black components, of a digitized black picture on a white background. The
        components are represented as n contiguous 0–1 pixel values stored on a
        √     √
           n × n mesh, where n is a power of 4. Two black pixels are neighbors
        if and only if they are adjacent horizontally, vertically or diagonally. Two
        black pixels are connected if they are in the same connected component.
        Every processor that contains a black pixel uses its snake-like index as
        the initial label of its pixel. When a labeling algorithm terminates, every
        processor that contain a black pixel will store the minimum label in the
        component that it belongs to. Figure 4.21 depicts an example in which
        part (a) is the initial input, and part (b) is the ﬁnal assignment of labels
        to connected components.

        4.15.1        The propagation algorithm
        The ﬁrst algorithm is a simple propagation algorithm. In this algorithm,
        every processor that contains a black pixel (black processor) deﬁnes its ini-
        tial label as its snake-like index. During each subsequent iteration of the
        algorithm, every black processor sends its current component label to its
        (at most) eight black neighbors. Every black processors then compares its
May 7, 2022   11:14        Parallel Algorithms               9in x 6in             b4591-ch04               page 192




        192                                          Parallel Algorithms


                  1    1   0    0      1       0     1   1       1       1               1         1   1

                  0    0   1     1     1       1     0   1                    1    1     1    1        1

                  0    0   0    0      0       0     0   0
                  1    1   0     1     1       0     0   0      28       28        28 28
                  1    0   1    0      1       1     0   1      28            28         28   28       40
                  0    0   0    1      1       0     0   0                         28    28
                  0    1   0    0          0   0     0   0               57
                  0    0   1    1      1       1    1    1                    57   57    57   57 57    57

                                     (a)                                               (b)

                               Fig. 4.21.          Labeling connected components.


        label with the (at most) eight labels just received, and keeps the mini-
        mum of these labels as its component label. This process is repeated for
        each black processor until all neighboring black processors have the same
        label.
            Let d be the maximum internal distance between any processor P con-
        taining a black pixel and the processor P  containing the pixel of minimum
        label in its component, where the distance is measured in terms of the
        number of black pixels between P and P  . Then, the maximum number of
        iterations of the algorithm is d. For instance, in Fig. 4.22, d = 4. It is easy
        to see that d can be as large as Θ(n) as shown in the instance in Fig. 4.23.
        Hence, the running time of the propagation algorithm is O(n).

        Example 4.7 An example of the propagation algorithm is shown in
        Fig. 4.22. The number of steps is 4.                         

        4.15.2        The recursive algorithm
        The large cost of the propagation algorithm calls for another alternative
        that labels the components in o(n) time. One possibility is an algorithm that
                                                               √
        uses divide-and-conquer to label the ﬁgures in time O( n) regardless of the
        number or shape of the ﬁgures. In this algorithm, the pixels are partitioned
        into four equal quadrants, where the components in each quadrant are
        labeled independently. After the recursive calls, the only components that
        may have an incorrect label are those that have a pixel on the border
May 7, 2022    11:14            Parallel Algorithms                         9in x 6in                       b4591-ch04                       page 193




                                        The Linear Array and the Mesh                                                                  193


                   1    2   3                            7        8              1         1        2                        7    7
                                                                  9                                                               8
                  17                     21 22                    24             17                                 21 21         9
                  32                                                             17

                       34          36    37 38                                            32                 36     36 37

                                                                  41                                                              41

                  49   50 51       52           54       55       56             49       49 50              51         54   54   41



                                     (a)                                                                          (b)

                   1    1   1                            7        7              1         1        1                        7    7
                                                                  7                                                               7
                  17                     21 21                    8              17                                 21 21         7
                  17                                                             17

                       17          36    36 36                                            17                 36     36 36

                                                                  41                                                              41

                  49   49 49       50           54       41       41             49       49 49              49         41   41   41



                                     (c)                                                                      (d)

              Fig. 4.22.    Labeling connected components using the propagation method.


                                           1         2        3        4     5        6        7        8
                                                                                                        9
                                           17        18 19             20    21 22             23       24
                                           32

                                           33 34 35                    36    37 38             39       40
                                                                                                        41

                                           49        50 51             52 53          54       55       56



                       Fig. 4.23.       Worst case instance of the propagation method.
May 7, 2022   11:14         Parallel Algorithms             9in x 6in             b4591-ch04     page 194




        194                                       Parallel Algorithms


                           1   1    0   0   1 0    1   1       1 1            5   5         5
                           0   0    1   1   1 1    0   1                1   1 5 5           5
                           0   0    0   0   0 0    0   0
                           1   1    0   1   1 0    0   1      31 31    29 28                25
                           1   0    1   0   1 1    0   1      33    35    37                40
                           0   0    0   1   1 0    1   0               35 37           40
                           0 1      0 0 0 0 0 0                   50
                           0 0      1 1 1 1 1 1                         50 50 57 57 57 57

                                        (a)                                  (b)


                           1 1            1   5        5      1 1             1   1         1
                                    1   1 1 5          5                1   1 1 1           1


                           31 31    28 28              25     31 31    28 28                25
                           33    35    28              40     33    35    28                40
                                    28 28         40                   28 28          40
                               50                                50
                                    50 50 50 57 57 57                   50 50 50 50 50 50

                                        (c)                                 (d)

                           1 1            1   1        1      1 1             1   1         1
                                    1   1 1 1          1                1   1 1 1           1


                           28 28    28 28              25     28 28    28 28                25
                           28    28    28              25     28    28    28                25
                                    28 28         40                   28 28           25
                               50                                 50
                                    50 50 50 50 50 50                   50 50 50 50 50 50
                                        (e)                                 (f)

              Fig. 4.24.   The recursive algorithm for labeling connected components.


        between the quadrants. An example is shown in Fig. 4.24. Part (b) of the
        ﬁgure is the result of the recursive calls on the instance shown in part (a).
            Next, we merge components that cross the quadrant boundaries. This
        will be accomplished in two phases. In the ﬁrst phase, we update the labels
        of pixels in components that cross the vertical boundary. In the second
        phase, we update the labels of pixels in components that cross the horizontal
        boundary.
May 7, 2022   11:14      Parallel Algorithms    9in x 6in       b4591-ch04                   page 195




                                The Linear Array and the Mesh                         195

            First, we describe how to merge components around the vertical bound-
        ary. The ﬁrst step is to apply the propagation algorithm on pixels inside the
                                                                                      √
        vertical strip consisting of the two middle columns. This will take O( n)
                                                 √
        time since the number of pixels is 2 n.
            For clarity, we will now use the two-dimensional numbering of proces-
        sors. Assume that there are two registers associated       with
                                                                     √ every processor in
                                                   √ √
        the vertical strip: αi,j and βi,j , 1 ≤ i ≤ n, 2n ≤ j ≤ 2n + 1. αi,j will hold
        the label of the pixel in processor Pi,j just after the recursive calls, and βi,j
        will hold the label of the pixel in processor Pi,j just after the propagation
        process in the vertical boundary (for white pixels, αi,j = βi,j = 0 shown
        as blank in the ﬁgure). Figure 4.24(c) depicts the two columns associated
        with Fig. 4.24(b) after applying the propagation algorithm on their pixels.
        For instance, as shown in these two ﬁgures,√α1,5 = 5 and β1,5 = 1.
            We copy the α and β values in column 2n to all columns all the way to
                                                                     √
        the left of the mesh, and the α and β values in column 2n +1 to all columns
        all the way to the right. In other words, for each row i, we copy αi,√n/2 and
        βi,√n/2 all the way to the left, and copy αi,√n/2+1 and βi,√n/2+1 all the
        way to the right. (See Exercise 4.39 for the α and β values corresponding to
        Fig. 4.24(c)). Then, we pipeline all (αi,j , βi,j ) pairs vertically through every
        pixel. Each time a new pair arrives, we test its α value with the label of the
        current processor. If they are equal, we set the value of the pixel label equal
                                                                             √
        to the β value of the pair. Thus, every processor will inspect n pairs, and
                                   √
        will process them in O( n) time. Since this is done in parallel among all
                                                           √
        columns, the total time for all columns is O( n). Figure 4.24(d) shows the
        labels after the (vertical) updates.
            The second phase is symmetrical to the ﬁrst phase, in which we process
        the horizontal strip consisting of the two middle horizontal rows. Assume
        in this phase that there are two registers associated with         every processor
                                                               √ √                √
        in the horizontal strip: αi,j and βi,j , 1 ≤ j ≤ n, 2n ≤ i ≤ 2n + 1 ,
        where the β values are as deﬁned in phase 1, and the α values are the
        pixel values after
                        √ the vertical update discussed above. We copy the α and β
        values in row 2n + 1 to all rows all the way to the bottom. Note that we
                                     √
        do not need to copy row 2n to the top half of the mesh, since all labels
        in the upper half of the mesh are smaller than the labels in the lower half.
        Figure 4.24(e) depicts the two horizontal rows associated with Fig. 4.24(b)
        after applying the propagation algorithm on their pixels. Figure 4.24(f)
        shows the ﬁnal labels. As in the ﬁrst phase, the second phase will take
May 7, 2022   11:14    Parallel Algorithms                         9in x 6in                       b4591-ch04   page 196




        196                                            Parallel Algorithms

           √
        O( n) time. It follows that the overall time taken by the algorithm is
                                                    √       √
        given by the recurrence T (n) = T (n/4) + Θ( n) = Θ( n).


        4.16      Columnsort

        The r × s two-dimensional mesh is a generalization of the square mesh.
        It has r rows and s columns. Columnsort is a sorting algorithm designed
        especially for the r × s mesh in which r ≥ 2(s − 1)2 . The algorithm is shown
        as Algorithm columnsort. It is a generalization of Algorithm oddeven-
        merge for odd–even merging. Assume an r × s mesh, where r ≥ 2(s − 1)2 ,
        n = rs and s | r, where n is the number of elements to be sorted. The algo-
        rithm sorts into column-major order, so after completion of the algorithm,
        the (i, j)th entry, 0 ≤ i ≤ r − 1, 0 ≤ j ≤ s − 1, will contain the kth item,
        where k = i + jr.
            There are eight steps in the algorithm. In Steps 1, 3, 5 and 7, the ele-
        ments within each column are sorted. In Step 2, the elements are permuted
        by performing a row-column transformation that corresponds to a trans-
        pose of the matrix that deﬁnes the mesh, as shown in Fig. 4.25. Step 4 is
        the reverse of Step 2, as shown in the same ﬁgure.
            Step 6 of the algorithm consists of a shift of the elements by r/2
        positions, as shown in Fig. 4.26, and Step 8 is the reverse of Step 6.

                                   a       g m             transpose               a       b c
                                   b       h n                                     d       e f
                                   c       i       o                               g       h i
                                   d       j       p                               j   k           l
                                                           untranspose
                                   e       k q                                     m n o
                                   f       l       r                               p       q       r

                      Fig. 4.25.           Transpose and untranspose operations.


                              a        g m                                     -       d       j p
                                                                                   8




                                                           shift
                              b        h n                                     -       e       k q
                                                                                   8




                              c        i       o                               -       f       l       r
                                                                                   8




                              d        j       p                               a g m
                                                                                                       8




                                                           unshift
                              e        k q                                     b h n
                                                                                                       8




                              f        l       r                               c i             o
                                                                                                       8




                           Fig. 4.26.                  Shift and unshift operations.
May 7, 2022    11:14         Parallel Algorithms    9in x 6in       b4591-ch04                  page 197




                                    The Linear Array and the Mesh                         197


          Algorithm 4.8 columnsort
          Input: X = x0 , x1 , . . . , xn−1 , a sequences of n numbers, where n = rs.
          Output: X sorted in ascending order.
              1.   Sort each column.
              2.   Perform a row-column transposition.
              3.   Sort each column.
              4.   Perform the inverse transformation of Step 2.
              5.   Sort each column.
              6.   Shift the entries by r/2 positions.
              7.   Sort each column.
              8.   Perform the inverse of Step 6.



            As will be shown in Lemmas 4.2 and 4.3 below, after Step 4, every ele-
        ment will be within (s − 1)2 of its correct sorted position. In the special
        case where r = n/2 and s = 2, the algorithm reduces to Algorithm odde-
        venmerge. In Algorithm oddevenmerge, the input sequence is divided
        into two subsequences of n/2 elements each. These two subsequences are
        sorted as done in Step 1 of the algorithm. Then, the odd-index numbers
        in each subsequence are combined to form a new subsequence, as are the
        even-index numbers. This corresponds to the transpose operation in Step 2
        of Algorithm columnsort. Next, each subsequence is sorted, as is done in
        Step 3 of Algorithm columnsort. In Algorithm oddevenmerge, this is
        done by calling the algorithm recursively. After sorting, the subsequences
        are shuﬄed together, as is done in Step 4 of Algorithm columnsort. At
        this point, every number is within (s − 1)2 = 1 of its correct sorted posi-
        tion, so each number is compared and possibly exchanged with its neighbor,
        which completes the sorting. In Step 5 of Algorithm columnsort, all but
        the top and bottom numbers in each column are compared to their neigh-
        bors by sorting the columns. Steps 6–8 ensure that comparisons are made
        between numbers at the bottom of one column and the top of the next
        column.

        Example 4.8 An illustration of the algorithm is shown in Fig. 4.27. The
        input is shown in Fig. 4.27(a). Notice that, for simplicity, we have chosen
        r = 6 and s = 3 even though it does not satisfy the constraint r ≥ 2(s− 1)2 .
        The results of applying Steps 1–8 are shown in Figs. 4.27(b)–(i).         

           An equivalent sorting method is given by Algorithm columnsort2.
        Here, the shift operation has been replaced by sorting the columns in
May 7, 2022    11:14             Parallel Algorithms                     9in x 6in                  b4591-ch04                 page 198




        198                                                 Parallel Algorithms


                   (a) 6 1 5 1 2     (b) 3      1       0    (c) 3        5       6    (d) 0        2   6 (e) 0 3 1 0
                       14 4 7             5     4       2            10 14 17                  1    4   7        2 5 14
                       10 1 13            6     8       7            1    4       8            3    5   8        6 8 16
                       3 16 9             1 0 11 9                   11 1 5 1 6                9 12 13           1 9 11
                       17 8 2             14 15 12                   0    2       7            10 14 16          4 12 15
                       5 11 0             17 16 13                   9 12 13                   11 1 5 1 7        7 13 17
                         Input                Step 1                  Step 2                       Step 3             Step 4

                       (f) 0 3 1 0 (g) -                4 9 1 5 (h) -                 3 9 1 5 (i) 0 6 1 2
                                                    8




                                                                                  8
                           1 5 11               -       6 12 16               -       4 10 16               1 7 13
                                                    8




                                                                                  8
                           2 8 14               -       7 13 17               -       5 11 1 7              2 8 14
                                                    8




                           4 9 15                0 3 10                           8
                                                                               0 6 12                       3 9 15


                                                                                               8
                                                                 8




                           6 12 16               1 5 11                        1 7 13                       4 10 16

                                                                                               8
                                                                 8




                           7 13 17               2 8 14                        2 8 14                       5 11 1 7
                                                                                               8
                                                                 8




                            Step 5                      Step 6                        Step 7                 Step 8

                            Fig. 4.27.         Illustration of Algorithm columnsort.

        alternating order in Step 5, and applying two steps of Odd-Even trans-
        position sort to each row in Step 6.

          Algorithm 4.9 columnsort2
          Input: X = x0 , x1 , . . . , xn−1 , a sequences of n numbers, where n = rs.
          Output: X sorted in ascending order.
              1.   Sort each column.
              2.   Perform a row-column transposition.
              3.   Sort each column.
              4.   Perform the inverse transformation of Step 2.
              5.   Sort each column in alternating order.
              6.   Apply two steps of Odd-Even transposition sort to each row.
              7.   Sort each column.




        Example 4.9 An illustration of Algorithm columnsort2 is shown in
        Fig. 4.28. The input is shown in Fig. 4.28(a). The results of applying Steps
        1–7 are shown in Figs. 4.28(b)–(h).                                       

           We now prove the correctness of Algorithm columnsort. Recall that
        rank(x, S) is the number of elements less than x in S.
May 7, 2022   11:14      Parallel Algorithms                  9in x 6in           b4591-ch04               page 199




                                  The Linear Array and the Mesh                                     199


                      (a) 7 9 1 2 (b) 2            3      1   (c) 2       4   7   (d) 1    4   7
                          4 16 1             4     5      6         10 15 18          2    5   8
                          18 5 14            7     9      8         3     5   9       3    6   9
                          2 17 8             1 0 11 1 2             11 1 6 1 7        10 13 14
                          1 5 11 6           15 16 13               1     6   8       11 1 5 1 7
                          10 3 13            18 17 14               12 13 14          12 16 18
                              Input              Step 1             Step 2                Step 3

                      (e) 1     3 11 (f) 1 1 4 11             (g) 1 11 1 4        (h) 1    7 13
                          4     6 15         2 13 12                2 12 13           2    8 14
                          7     9 17         4 10 15                4 10 15           3    9 15
                          2 1012             5     9 16             5     9 16        4 10 16
                          5 13 16            7     6 17             6     7 17        5 11 1 7
                          8 14 18            8     3 18             3     8 18        6 12 18
                           Step 4             Step 5                 Step 6           Step 7

                      Fig. 4.28.       Illustration of Algorithm columnsort2.


        Lemma 4.2 Let S be a sequence of rs elements to be sorted by Algorithm
        columnsort in an r × s mesh, and let x be any element in S that is in
        position (i, j) of the mesh after Step 3 of the algorithm. Then, rank(x, S)
        is at least is + js − (s − 1)2 .

        Proof.    From the position of x after Step 3, we know that x is greater
        than or equal to at least i + 1 elements in the jth column of the mesh after
        Step 2. Let αk denote the number of these i + 1 elements that originally
        come from column k of the mesh, i.e., before Step 2 transposed the elements.
        By deﬁnition,

                                                              
                                                              s−1
                                                  i+1=              αk .                           (4.8)
                                                              k=0

        Since only the jth and every sth element thereafter of the sorted kth column
        after Step 1 appear in the jth column after Step 2, this means that x is
        greater than or equal to at least (αk −1)s+j +1 elements in the kth column
        of the mesh after Step 1. Hence, the true rank of x is at least

                                       
                                       s−1
                                             [(αk − 1)s + j + 1] − 1.                              (4.9)
                                       k=0
May 7, 2022   11:14    Parallel Algorithms           9in x 6in      b4591-ch04            page 200




        200                                  Parallel Algorithms

                                s−1
        Substituting i + 1 for k=0 αk in (4.9) and simplifying, we ﬁnd that the
        true rank of x is at least

                                         is + js − (s − 1)2 .

                                                                                     

        Example 4.10 We illustrate the proof of Lemma 4.2. Let x = 12 in
        Fig. 4.27. As is evident from Fig. 4.27(d), i = 3 and j = 1 (Recall that
        indices start from 0). After Step 3 (Fig. 4.27(d)), there are i + 1 = 4
        elements on or above the (i, j)th entry. These elements are {2, 4, 5, 12}.
        Thus, α0 = α1 = 1 and α2 = 2. The true rank of x is at least is + js − (s −
        1)2 = 3 × 3 + 1 × 3 − 4 = 8.                                              


        Lemma 4.3 Let S be a sequence of rs elements to be sorted by Algorithm
        columnsort in an r × s mesh, and let x be any element in S that is in
        position (i, j) of the mesh after Step 3 of the algorithm. Then, rank(x, S)
        is at most is + js.

        Proof.     We use an argument similar to that in Lemma 4.2. From the
        position of x after Step 3, we know that x is less than or equal to at least
        r − i elements in the jth column of the mesh after Step 2. Let βk denote
        the number of these r − i elements that originally come from column k of
        the mesh, i.e., before Step 2 transposed the elements. By deﬁnition,

                                                     
                                                     s−1
                                             r−i=          βk .                  (4.10)
                                                     k=0

        Since only the jth and every sth element thereafter of the sorted kth column
        after Step 1 appear in the jth column after Step 2, this means that x is
        less than or equal to at least (βk − 1)s + s − j elements in the kth column
        of the mesh after Step 1. Hence, the number of elements greater than or
        equal to x is at least

                                      
                                      s−1
                                             [(βk − 1)s + s − j].                (4.11)
                                      k=0
May 7, 2022   11:14       Parallel Algorithms      9in x 6in     b4591-ch04                   page 201




                                 The Linear Array and the Mesh                         201

                              s−1
        Substituting r − i for k=0 βk in (4.11) and simplifying, we ﬁnd that the
        number of elements greater than or equal to x is at least

                                     (r − i)s − js = rs − is − js.

        Hence, the true rank of x is at most

                                    rs − (rs − is − js) = is + js.

                                                                                         

        Example 4.11 We illustrate the proof of Lemma 4.3. Let x = 5 in
        Fig. 4.27. As is evident from Fig. 4.27(d), i = 2 and j = 1 (Recall that
        indices start from 0). After Step 3 (Fig. 4.27(d)), there are r − i = 6 − 2 = 4
        elements on or below the (i, j)th entry. These elements are {5, 12, 14, 15}.
        Thus, β0 = 2, β1 = β2 = 1. The true rank of x is at most is + js =
        2 × 3 + 1 × 3 = 9.                                                           

        Theorem 4.4 Let S be a sequence of rs elements to be sorted by Algo-
        rithm columnsort in an r × s mesh, and let x be any element in S that
        is in position (i, j) of the mesh after Step 3 of the algorithm. Then, the
        position of x after Step 4 is within (s − 1)2 from its correct position.

        Proof.    Consider an element x that is in position (i, j) of the mesh after
        Step 3. Clearly, x is sent to a position in Step 4 that corresponds to a
        rank of

                                                is + j                               (4.12)

        in the sorted list. (Recall our convention that the smallest number has
        rank zero). By Lemma 4.2, rank(x, S) is at least is + js − (s − 1)2 . Hence,
        subtracting this quantity from (4.12), the position of x after Step 4 is at
        most

                 (is + j) − (is + js − (s − 1)2 ) = (s − 1)2 − j(s − 1) ≤ (s − 1)2

        beyond its correct position. By Lemma 4.3, rank(x, S) is at most is + js.
        Hence, subtracting (4.12) from this quantity, the position of x after Step 4
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch04          page 202




        202                                     Parallel Algorithms

        is at most

                            (is + js) − (is + j) = j(s − 1) ≤ (s − 1)2

        short of its correct sorted position. Thus, we have established that every
        element is within (s−1)2 of its correct position after Step 4 of the algorithm.
                                                                                     

        Theorem 4.5        Algorithm columnsort correctly sorts an arbitrary
        sequence of rs elements in an r × s mesh with r ≥ 2(s − 1)2 .

        Proof.     By Theorem 4.4, we only need to show that Steps 5–8 will be
        suﬃcient to ﬁnish the sorting. For simplicity, we assume that every number
        is within r/2 of its correct sorted position. Since r ≥ 2(s − 1)2 , we are
        always guaranteed that this condition is met after completion of Step 4.
        After Step 5, every number that belongs in the top half of column j is
        in the top half of column j or the bottom half of column j − 1, and every
        number that belongs in the bottom half of column j is in the bottom half of
        column j or the top half of column j + 1. Otherwise, some number would be
        more than r/2 away from its correct position. Hence, Steps 6–8 complete
        the sorting.                                                             



        4.17      3-dimensional Mesh

        A 3-dimensional mesh of sides m = n1/3 can be viewed as a connection of
        m successive levels of 2-D meshes of size m × m. It has n = m3 processors
        and 3m3 − 3m2 links. Two processors are connected by a two-way link if
        and only if they diﬀer in precisely one coordinate and if the absolute value
        of the diﬀerence in that coordinate is 1 (see Fig. 4.29). In this ﬁgure, m = 4
        and n = 64.
            In 3-D mesh, the degree of each node is between 3 and 6, so meshes are
        not regular. Of course, the degree of a corner vertex is less than the degree
        of an internal vertex. The diameter is 3(m − 1) = Θ(n1/3 ).

        4.17.1        Sorting on 3-dimensional meshes
        Consider the problem of sorting n = m3 numbers on a 3-dimensional mesh
        with n processors in lexicographic zyx-order. In a zyx-ordering, elements
May 7, 2022    11:14       Parallel Algorithms              9in x 6in           b4591-ch04         page 203




                                  The Linear Array and the Mesh                              203


                                                      (3,0,0) (3,0,1) (3,0,2)      (3,0,3)


                                            (3,1,0)     (3,1,1) (3,1,2) (3,1,3)


                                      (3,2,0) (3,2,1)      (3,2,2) (3,2,3)


                                (3,3,0)   (3,3,1)     (3,3,2)   (3,3,3)




                                                      (0,0,0) (0,0,1) (0,0,2) (0,0,3)


                                            (0,1,0) (0,1,1)      (0,1,2) (0,1,3)


                                     (0,2,0) (0,2,1) (0,2,2)         (0,2,3)



                             (0,3,0) (0,3,1) (0,3,2) (0,3,3)

                                   Fig. 4.29.       A 3-dimensional mesh.



        of processors in the plane with coordinate z = 0 come ﬁrst, followed by
        those with z = 1, and so on. The xy-planes are sorted in yx-order, that is,
        in columnwise order. The following algorithm needs just ﬁve steps, where
        each step sorts numbers within 2-D meshes. These steps are outlined in
        Algorithm threedmeshsort.

          Algorithm 4.10 threedmeshsort
          Input: n = m3 elements stored in a 3-D mesh.
          Output: The elements sorted in ascending zyx-order.
              1. Sort all xz-planes in zx-order.
              2. Sort all yz-planes in zy-order.
              3. Sort all xy-planes in yx-order. Reverse the order on every other plane.
              4. Perform one Odd-Even and one even-odd transposition within all columns
                 in parallel.
              5. Sort all xy-planes in yx-order.
May 7, 2022   11:14    Parallel Algorithms          9in x 6in      b4591-ch04           page 204




        204                                  Parallel Algorithms

            Recall that a dirty row is a row consisting of 0’s and 1’s. A dirty plane
        is one containing at least one dirty row or column. A z-column is a column
        of processors parallel to the z-axis. A 0-row is a row of 0’s and no 1’s.

        Theorem 4.6      Algorithm threedmeshsort correctly sorts a given
        sequence of numbers in zyx-order.

        Proof.     By the zero-one principle (Lemma 2.1 in Section 2.10), we may
        consider any input sequence of 0’s and 1’s. After Step 1 is completed, in
        every xz-plane, there is at most one dirty row and therefore the diﬀerence
        in the number of zeroes between any two z-columns in the same xz-plane
        is at most one. Hence, any two yz-planes can diﬀer in at most m 0’s. It
        follows that after Step 2 is completed, the diﬀerence in the number of
        0-rows between any two yz-planes is at most one, which means that all
        dirty rows can span at most two adjacent xy-planes. If there is only one
        dirty xy-plane, we can go directly to Step 5 and we are done. If there are
        two dirty xy-planes, Steps 3 and 4 eliminate at least one of them and Step 5
        completes the sorting.                                                    

        Example 4.12 Figure 4.30 illustrates the algorithm on a sequence of 0’s
        and 1’s shown in part (a). First, the xz-planes are sorted in Fig. 4.30 (b).
        Next, the yz-planes are sorted in Fig. 4.30 (c). In this part of the ﬁgure,
        both the middle and top xy-planes are dirty, and so Steps 3 and 4 are
        needed, as shown in parts (d) and (e) of the ﬁgure. Finally, Fig. 4.30(f)
        shows the result after Step 5 is executed, in which the input is sorted. Note
        that there is only one dirty plane, the middle xy-plane.                   


        Example 4.13 Figure 4.31 illustrates the algorithm on a sequence of
        integers shown in part (a). First, the xz-planes are sorted in Fig. 4.31 (b).
        Next, the yz-planes are sorted in Fig. 4.31 (c). The xy-planes are then
May 7, 2022   11:14               Parallel Algorithms           9in x 6in                       b4591-ch04                                     page 205




                                         The Linear Array and the Mesh                                                                   205

                  (a)                                              (b)                                   1
                              0              0              0                       0                                            1
                          1              0              0                   1                     1                      1
                      1             1             0                     1                   1                    1
                              1              0              0                           0                0                           0
                          1              0              1                   0                        0                       1
                      1             0              1                 0                      1                        1
                              1              0              0                           0                    0                   0
                          1              0              0                       0                    0                       0
                      0             1             0                     0                   0                    0



                  (c)                                              (d)              1                    1                       1
                              1              1              1
                                                                            1                    1                       1
                          1              1              1
                                                                    0                       1                    1
                      0              1             1                                0                    0                       0
                              0              1              1
                                                                            0                     0                      0
                          0              0              1
                                                                    1                       1                    1
                      0              0             0                                0                    0                       0
                              0              0              0               0                     0                      0
                          0              0              0
                                                                    0                       0                    0
                      0              0             0


                  (e)                                              (f)              1                    1                       1
                              1              1              1               1                    1                       1
                          1              1              1           1                       1                    1
                      1              1             1                                0                    1                       1
                              0              0              0               0                    0                       0
                          0              0              0           0                       0                    0
                      0              1             1                                0                    0                       0
                              0              0              0               0                    0                       0
                          0              0              0           0                       0                    0
                      0              0             0

                              Fig. 4.30.         Sorting in the 3-D mesh of 0’s and 1’s

        sorted in reverse order according to Step 3 of the algorithm as shown in
        part (d) of the ﬁgure. Next, two iterations of odd–even sort are executed,
        and the result is shown in Fig. 4.31(e). Finally, Fig. 4.31(f) shows the result
        after Step 5 is executed, in which the input is sorted.                      
May 7, 2022   11:14              Parallel Algorithms                  9in x 6in                 b4591-ch04                              page 206




        206                                                 Parallel Algorithms


               (a)          3                13                 5        (b)               15                  24                  27
                      25                8                  16                       20                   22                  25
                26               17                  14                     21                   23                 26
                            24               7                  15                          6                  7                   13
                      18                12                 22                       12                   16                  18
                21                2                  23                     14                   17                  19
                            27               6                  1                           1                  3                   5
                      20                11                 10                          8                 10                  11
                 4               19                  9                         2                    4                   9


               (c)                                                       (d)               25                 26                  27
                            21               24                 27
                                                                                   22                   23                  24
                       20               23                 26
                                                                           15                   20                  21
                 15               22                 25                                    12                 10                  8
                            14               17                 19                 16                   14                  13
                      12                16                 18             19                    18                  17
                 8                10                 13                                    7                  9                   11
                             6                7                 11                 4                    5                    6
                       2                 4                 9
                                                                           1                    2                   3
                 1                3                   5


               (e)                                                        (f)              25                 26                  27
                            25                26                 27                22                   23                  24
                       22               23                 24             19                    20                  21
                 19                20                 21                                   16                 17                  18
                            12                10                 11                13                   14                  15
                      16                14                 13             10                    11                  12
                 15               18                  17                                   7                  8                   9
                             7                   9               8                 4                    5                   6
                       4                 5                  6              1                    2                   3
                  1                2                  3

                       Fig. 4.31.            Sorting in the 3-D mesh of arbitrary numbers



        4.18      Bibliographic Notes

        There are a number of books that cover parallel algorithms on the mesh.
        These include Akl [4], Akl [5], Akl [6], Cosnard and Trystram [29], Grama,
        Gupta, Karypis and Kumar [39], Horowitz, Sahni and Rajasekaran [43],
        Leighton [57], Miller and Boxer [66], and Miller and Stout [67]. A survey of
        parallel sorting and selection algorithms can be found in Rajasekaran [75].
        Parallel algorithms for many problems including problems in computa-
        tional geometry on the mesh can be found in Miller and Stout [67], and
        Leighton [57]. The randomized algorithm for packet routing is due to
May 7, 2022   11:14     Parallel Algorithms     9in x 6in       b4591-ch04                   page 207




                               The Linear Array and the Mesh                          207


        Valiant and Brebner[101]. For deterministic algorithms on routing, see
        Leighton, Makedon and Tollis [58], and Nassimi [71]. Shearsort was pre-
        sented independently by Sado and Igarashi[79] and Scherson, Sen and
        Shamir [81]. The odd–even mergesort on the mesh can be found in
        Thompson and Kung [92]. The algorithm for transitive closure is due
        to Christopher [24]. The algorithm presented is a modiﬁed version of
        the algorithm presented in Leighton [57]. Columnsort algorithm is from
        Leighton [56]. For more references on parallel algorithms on the mesh inter-
        connection network, see for instance Miller and Stout [67].


        4.19      Exercises

         4.1. What are the expansion and load of the embedding of the linear
              array into the mesh shown in Fig. 4.4 (page 161)? How about the
              embedding of the mesh into the linear array shown in Fig. 4.5?

         4.2. Explain how to broadcast an item x in an arbitrary processor to all
              other processors in the ring with n processors.

         4.3. Describe an algorithm to ﬁnd the sum of all elements {x1 , x2 , . . . , xn }
                         √      √
              stored in a n × n mesh and store the sum in all processors. How
              many steps are required by the algorithm?

         4.4. One method to smooth a picture is as follows. Let p be the pixel in
              the middle of a square of a 3 × 3 square of pixels. Replace the value
              of p by the average of all the 3 × 3 pixels. Suggest a computation
              model to solve this problem, and show how to solve it.
                                                       √   √              √
         4.5. What is the bisection width of the        n × n mesh? Assume n is
              even.

         4.6. Give a lower bound on the problems of sorting and routing on the
              mesh.
                                                       √   √               √
         4.7. What is the bisection width of the        n × n torus? Assume n is
              even (see Fig. 4.2).

         4.8. Give a recursive algorithm to ﬁnd the maximum of n numbers stored
                  √      √
              in a n × n mesh. Analyze its running time.
May 7, 2022   11:14      Parallel Algorithms               9in x 6in          b4591-ch04                page 208




        208                                    Parallel Algorithms

         4.9. Give a recursive algorithm to ﬁnd the preﬁx sums of n numbers
                                              √   √
              x1 , x2 , . . . , xn stored in a n × n mesh. Analyze its running time.

        4.10. Illustrate your solution to Exercise 4.9 on the input 1, 2, 3, . . . , 16.

        4.11. The transpose of a matrix A, denoted by                        AT , is the matrix whose
              columns are the rows of A. That is, if
                                     ⎡                                              ⎤
                                       a1,1 a1,2 . . .                       a1,n
                                     ⎢                                            ⎥
                                     ⎢ a2,1 a2,2 . . .                       a2,n ⎥
                                     ⎢                                            ⎥
                                A=⎢  ⎢ ..     ..     ..                       .. ⎥
                                                                                  ⎥
                                     ⎢ .       .      .                        . ⎥
                                     ⎣                                            ⎦
                                       an,1 an,2 . . .                       an,n

                 then
                                                ⎡                                   ⎤
                                                    a1,1     a2,1      ...   an,1
                                         ⎢                                        ⎥
                                         ⎢ a1,2              a2,2      ...   an,2 ⎥
                                         ⎢                                        ⎥
                                      A =⎢
                                       T
                                         ⎢ ..                   ..      ..    .. ⎥
                                                                                  ⎥
                                         ⎢ .                     .       .     . ⎥
                                         ⎣                                        ⎦
                                           a1,n              a2,n      . . . an,n

                 Given the matrix A stored one element per processor in an n × n
                 mesh, show how to compute AT . What is the number of steps in
                 your algorithm?

        4.12. Apply the algorithm for odd–even transposition sort on the input
              3, 7, 5, 2. Assume a linear array with four processors.

        4.13. Consider Algorithm merge-split, which is a generalization of odd–
              even transposition sort for the case p < n. Let S be a sequence
              of numbers to be sorted, and assume that each of the p proces-
              sors in the linear array holds a subsequence of S of length n/p.
              In Algorithm merge-split, the comparison-exchange operations of
              odd–even transposition sort are replaced with merge-split operations
              on subsequences. Let Si denote the subsequence held by processor Pi .
              In Step 1, each Pi sorts Si using a sequential algorithm. In Step 2
              each odd-numbered processor Pi merges the two subsequences Si
              and Si+1 , into a sorted sequence Si . It retains the ﬁrst half of Si
May 7, 2022   11:14          Parallel Algorithms     9in x 6in      b4591-ch04                  page 209




                                    The Linear Array and the Mesh                         209

                 and assigns to its neighbor Pi+1 the second half. Step 3 is identical
                 to 2 except that it is performed by all even-numbered processors.
                 Steps 2 and 3 are repeated alternately. After p/2 iterations, no
                 further exchange of elements can take place between two processors,
                 where an iteration consists of Steps 2 and 3. Analyze the running
                 time of this algorithm.

        4.14. Do Exercise 4.13 for the case p = log n. Is the algorithm optimal?

        4.15. Consider the problem of permutation routing on the mesh with n
              processors, in which every processor tries to send to a diﬀerent des-
              tination. Outline a sorting-based algorithm to route every packet to
              its destination. Compare your algorithm with the greedy algorithm.

        4.16. Modify your algorithm in Exercise 4.15 so that it works for the more
              general one-to-one routing problem, in which not every processor is
              the source of a packet. Note here that no processors Pi and Pj send
              to the same destination.

        4.17. Illustrate the operation of the odd–even merging algorithm on the
              input:

                      A = 1, 9, 8, 17, 3, 11, 14, 12 and B = 2, 5, 15, 7, 13, 9, 16, 10.

                 Assume a mesh of 16 processors.

        4.18. Show how to compute the preﬁx sums on the mesh for the snakelike
              indexing scheme.

        4.19. In a window broadcast, we start with data in the top left w × w
                            √     √                   √                    √
              submesh of a n × n mesh, where w | n, that is, w divides n.
              Following the window broadcast operation, the initial w × w win-
              dow tiles the entire mesh. Outline an algorithm to implement this
              operation. What is the running time of your algorithm?

        4.20. Give an algorithm to evaluate the polynomial an−1 xn−1 +an−2 xn−2 +
                                                          √      √
              · · · + a1 x + a0 at the point x0 on the n × n mesh. Assume that
              each ai is stored in processor Pi , 0 ≤ i ≤ n − 1 (the processors
              are indexed as P0 , P1 , . . . , Pn−1 ). What is the running time of your
              algorithm?
May 7, 2022   11:14     Parallel Algorithms          9in x 6in      b4591-ch04           page 210




        210                                   Parallel Algorithms

        4.21. Consider the following method for sorting on the mesh. The method
              alternately sorts all rows from left to right and all columns from
              top to bottom. Will this method always work in sorting any input?
              Assume an unlimited amount of time.

        4.22. Consider sorting the rows and then the columns of a 2 × n mesh M .
              Does this leave the rows in sorted order?

        4.23. This is a generalization of Exercise 4.22. Consider sorting the rows
              and then the columns of a general n × n mesh. Does this leave the
              rows in sorted order?

        4.24. Let A = a1 , a2 , . . . , an  be a sequence of elements stored in the
                              √         √
              processors of a n × n mesh, one element per processor, and let x
              be a given element. Design an algorithm for the search problem in
              the mesh: If ai = x for some i, 1 ≤ i ≤ n, then return i, else return 0.
              Analyze its running time.

        4.25. How many steps are required by the matrix multiplication algorithm
              on the mesh of Section 4.10.1?

        4.26. Show the results of the computations of c1,3 and c1,4 in the matrix
              multiplication algorithm on the mesh of Section 4.10.1.
                                          √      √
        4.27. Give an algorithm for the n × n mesh to determine whether a
              given graph G is cyclic or acyclic. What is the running time of your
              algorithm?

        4.28. Let G be a connected undirected and unweighted graph on n vertices.
              A breadth-ﬁrst spanning tree for G is a spanning tree that can be
              obtained by performing breadth-ﬁrst traversal on G starting at some
              vertex, say r. Equivalently, a breadth-ﬁrst spanning tree of G is a
              tree in which every path from the root to any vertex is of shortest
              length, where the distance is measured in terms of number of edges.
                                                                        √      √
              Present an eﬃcient algorithm to ﬁnd such a tree for the n × n
              mesh. What is the running time of your algorithm?

        4.29. Suggest another algorithm for computing the transitive closure of a
              matrix A diﬀerent from the one given in Section 4.11. What is the
              running time of the algorithm?
May 7, 2022   11:14      Parallel Algorithms        9in x 6in         b4591-ch04                 page 211




                                The Linear Array and the Mesh                              211

        4.30. Suggest another algorithm for computing the shortest paths in a
              directed graph G diﬀerent from the one given in Section 4.13. What
              is the running time of the algorithm?

        4.31. Illustrate the operation of Algorithm columnsort discussed in
              Section 4.16 on the input

                          17, 1, 18, 12, 8, 10, 11, 2, 4, 14, 5, 6, 9, 13, 15, 16, 7, 3,

                 where n = 18. Assume an r × s mesh, where r = 6 and s = 3.

        4.32. Illustrate the operation of Algorithm columnsort2 discussed in Sec-
              tion 4.16 on the input

                          8, 10, 11, 2, 4, 14, 5, 6, 17, 1, 18, 12, 9, 13, 15, 16, 7, 3,

                 where n = 18. Assume an r × s mesh, where r = 6 and s = 3.

        4.33. Explain why Algorithm columnsort does not work on square
              meshes.

        4.34. This exercise is similar to Exercise 2.12. Consider Algorithm
              columnsort discussed in Section 4.16. If we let s = 2, then the
              algorithm reduces to Algorithm oddevenmerge in Section 2.11 for
              odd–even merging with the even part of A merged with the even
              part of B and the odd part A merged with the odd part of B. Let A
              and B be the ﬁrst and second columns after Step 1, respectively.
              Let C and D be the ﬁrst and second columns after Step 3, respec-
              tively. Let E be the whole list after Step 5. Assume the elements
              in A ∪ B are distinct. Given a sequence X and an element x, recall
              that rank(x, X) is the number of elements in X less than x. Express
              rank(x, C) and rank(x, D) in terms of rank(x, A) and rank(x, B).

        4.35. This exercise is similar to Exercise 2.13. Use the result of Exercise
              4.34 to show that for c ∈ C, either c is in its correct position in E or
              to the right of it.

        4.36. This exercise is similar to Exercise 2.14. Use the result of Exercise
              4.34 to show that for d ∈ D, either d is in its correct position in E
              or to the left of it.
May 7, 2022   11:14      Parallel Algorithms                   9in x 6in                  b4591-ch04   page 212




        212                                         Parallel Algorithms


                                                    1              1                 1
                                             0                1                 1
                                     0                   0              1
                                                     0             1                 1
                                             0                1                 0
                                     0                   1                 0
                                                     0             1                 1
                                                0             1                 1
                                        1                0              1

                                         Fig. 4.32.           Exercise 4.37.


                                                    17             13                25
                                            5                 22                16
                                    6                    3              14
                                                    24             21                15
                                            18                12                8
                                    7                    27                23
                                                    2              26                4
                                            9                 10                11
                                    1                    19                20

                                         Fig. 4.33.           Exercise 4.38.

        4.37. Illustrate the operation of the algorithm for sorting on a
              3-dimensional mesh/sorting on 3-dimensional meshes on the input
              shown in Fig. 4.32.

        4.38. Illustrate the operation of the algorithm for sorting on a
              3-dimensional mesh/sorting on 3-dimensional meshes on the input
              shown in Fig. 4.33.

        4.39. Compute the values of α and β corresponding to Fig. 4.24(c).


        4.20      Solutions

         4.1. What are the expansion and load of the embedding of the linear array
              into the mesh shown in Fig. 4.4? How about the embedding of the
              mesh into the linear array shown in Fig. 4.5?
                 In both embeddings, the expansion is 1. The load is also 1 in both
                 embeddings, as precisely one node is mapped to each image node.

         4.2. Explain how to broadcast an item x in an arbitrary processor to all
              other processors in the ring with n processors.
May 7, 2022   11:14       Parallel Algorithms   9in x 6in        b4591-ch04                  page 213




                                 The Linear Array and the Mesh                        213


                 One copy of x moves n/2 steps to the left, and another copy moves
                 n/2 steps to the right.

         4.3. Describe an algorithm to ﬁnd the sum of all elements {x1 , x2 , . . . , xn }
                         √      √
              stored in a n × n mesh and store the sum in all processors. How
              many steps are required by the algorithm?

                 Find the sum of all numbers and store it in processor P1 . Next,
                 broadcast the sum to all other processors. The number of steps is
                   √            √            √
                 (2 n − 2) + (2 n − 2) = 4 n − 4. See Exercise 8.5 for a more
                 eﬃcient implementation.

         4.4. One method to smooth a picture is as follows. Let p be the pixel in
              the middle of a square of a 3 × 3 square of pixels. Replace the value
              of p by the average of all the 3 × 3 pixels. Suggest a computation
              model to solve this problem, and show how to solve it.

                 The mesh is the natural model to solve this problem. Do smoothing
                 for all squares in parallel.
                                                       √   √              √
         4.5. What is the bisection width of the        n × n mesh? Assume n is
              even.

                 If we consider a mesh of size n, and cut it by a line through the
                                          √
                 center, the line will cut n links. Hence, the bisection width of the
                          √
                 mesh is n.

         4.6. Give a lower bound on the problems of sorting and routing on the
              mesh.

                 Since all n data items may have to cross from one side of the mesh
                                           √       √
                 to the other, at least n/ n = Ω( n) time is required just to get
                 data across the middle of the mesh (see Exercise 4.5). That is, the
                                    √
                 lower bound is Θ( n)
                                                       √   √               √
         4.7. What is the bisection width of the        n × n torus? Assume n is
              even (see Fig. 4.2).

                 If we consider a torus of size n, and cut it by a line through the
                                            √
                 center, the line will cut 2 n links. Hence, the bisection width of the
                           √
                 torus is 2 n.
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch04                  page 214




        214                                      Parallel Algorithms

         4.8. Give a recursive algorithm to ﬁnd the maximum of n numbers stored
                  √      √
              in a n × n mesh. Analyze its running time.
                 Assume the processors are numbered as P1 , P2 , . . . , Pn , and that n
                 is a power of 4. Partition
                                        √   √the mesh into four submeshes of the same
                 size, that is, of size 2n × 2n each. Recursively ﬁnd the maximum in
                 each quadrant, and store the result in the processor near the center of
                 the mesh. Finally, ﬁnd the maximum of the four computed maxima,
                 and route it to processor P1 . The running time is governed by the
                                                 √                                  √
                 recurrence T (n) = T (n/4)+Θ( n), whose solution is T (n) = Θ( n).

         4.9. Give a recursive algorithm to ﬁnd the preﬁx sums of n numbers
                                              √   √
              x1 , x2 , . . . , xn stored in a n × n mesh. Analyze its running time.
                 Assume that n is a power of 4. For convenience, assume also the
                 proximity indexing scheme shown in Fig. 6.7. First, partition      √      the
                                                                                            √
                 mesh into four submeshes of the same size, that is, of size 2n × 2n
                 each. Recursively ﬁnd the preﬁx sum in each quadrant, and store the
                 ﬁnal preﬁx sum yj in the processor closest to the center of the mesh.
                                    √
                 This takes Θ( n) time since it requires sending the ﬁnal sums to the
                 appropriate processors near the center. At this point, y1 = x1 + x2 +
                 · · · + xn/4 , y2 = xn/4+1 + xn/4+2 + · · · + xn/2 , y3 = xn/2+1 + xn/2+2 +
                 · · · + x3n/4 , and y4 = x3n/4+1 + x3n/4+2 + · · · + xn . Next, ﬁnd the
                 preﬁx sums of y1 , y2 , y3 , y4 and store them in registers z1 , z2 , z3 , z4 .
                 Now, rotate the values stored in registers zj ; that is, for j = 1, 2, 3,
                 set zj+1 ← zj , and set z1 ← 0. Note that ﬁnding the preﬁx sums of
                 y1 , y2 , y3 , y4 and rotating the zj ’s take constant time. Finally, for j =
                 1, 2, 3, 4, broadcast zj to all processors in quadrant j, and add zj to all
                 preﬁx sums computed earlier in quadrant j. This broadcasting step
                             √
                 takes Θ( n) time. It follows that the running time of the algorithm is
                                                                         √
                 governed by the recurrence T (n) = T (n/4) + Θ( n), whose solution
                                   √
                 is T (n) = Θ( n). A summary of the algorithm is shown as Algorithm
                 meshprefixsumrec.

        4.10. Illustrate your solution to Exercise 4.9 on the input 1, 2, 3, . . . , 16.
                 The algorithm in the solution of Exercise 4.9 is illustrated in
                 Figs. 4.34 and 4.35. The input is shown in Fig. 4.34(a). The pre-
                 ﬁx sums of the four partitions are shown in Fig. 4.34(b). Part (c) of
                 the ﬁgure shows the four ﬁnal preﬁx sums — that is, the totals of
May 7, 2022    11:14         Parallel Algorithms              9in x 6in                 b4591-ch04         page 215




                                    The Linear Array and the Mesh                                    215


          Algorithm 4.11 meshprefixsumrec
          Input: n numbers x1 , x2 , . . . , xn stored in a mesh of size n, one element per
                 processor.
          Output: The preﬁx sums s1 , s2 , . . . , sn .
              1. if n = 1 then set s1 ← x1 and exit                  √      √
              2. Partition the mesh into four submeshes of size 2n × 2n each. Recursively
                 ﬁnd the preﬁx sum in each quadrant, and store the ﬁnal preﬁx sum in the
                 processor closest to the center of the mesh in register yj , j = 1, 2, 3, 4.
              3. Find the preﬁx sums of y1 , y2 , y3 , y4 and store them in z1 , z2 , z3 , z4 .
              4. Rotate the values stored in zj : for j = 1, 2, 3: set zj+1 ← zj , and set z1 ← 0.
              5. For j = 1, 2, 3, 4, broadcast zj to all processors in quadrant j.
              6. Every processor Pi in the mesh sets si ← si + zj .




                  all elements in each quadrant. These are the contents of registers yj ,
                  j = 1, 2, 3, 4. The preﬁx sums of these four values is computed in
                  part (d). Next, these preﬁx sums are rotated in part (e), and y1 is
                  set to 0. Now, these entries are broadcast in all four quadrants as
                  shown in part (f). Finally, Fig. 4.35 shows the ﬁnal preﬁxes after
                  summing the entries in part (f) with those in part (b) of Fig. 4.34.

        4.11. The transpose of a matrix A, denoted by AT , is the matrix whose
              columns are the rows of A. That is, if
                                                   ⎡                                          ⎤
                                                       a1,1    a1,2         ...        a1,n
                                             ⎢                                              ⎥
                                             ⎢ a2,1            a2,2         ...        a2,n ⎥
                                             ⎢                                              ⎥
                                           A=⎢
                                             ⎢ ..                 ..         ..         .. ⎥
                                                                                            ⎥
                                             ⎢ .                   .          .          . ⎥
                                             ⎣                                              ⎦
                                              an,1             an,2         . . . an,n

                  then
                                                   ⎡                                          ⎤
                                                       a1,1     a2,1        ...        an,1
                                             ⎢                                              ⎥
                                             ⎢ a1,2             a2,2        ...        an,2 ⎥
                                             ⎢                                              ⎥
                                          A =⎢
                                           T
                                             ⎢ ..                      ..         ..    .. ⎥
                                                                                            ⎥
                                             ⎢ .                        .          .     . ⎥
                                             ⎣                                              ⎦
                                              a1,n              a2,n        . . . an,n
May 7, 2022   11:14    Parallel Algorithms                  9in x 6in               b4591-ch04   page 216




        216                                     Parallel Algorithms


                      (a) quadrant 1 quadrant 2                 (b)

                            1      4    5          6                  1        10    5     11


                             2     3     8        7                   3        6     26    18


                            15    14    9         10                42         27    9     19


                            16    13    12        11                58         13   42     30

                            quadrant 4 quadrant 3
                      (c)                                       (d)




                                  10    26                                     10    36


                                  58    42                                 136      78




                      (e)                                       (f)

                                                                      0        0    10    10


                                   0    10                            0        0    10    10


                                  78     36                         78         78    36   36

                                                                    78         78    36   36


        Fig. 4.34. Example of Algorithm meshprefixsumrec for ﬁnding the preﬁx sums
        on the mesh recursively (Exercise 4.10).


                                            1         10       15         21

                                            3          6       36         28


                                         120          105      45         55


                                         136          91      78          66


                       Fig. 4.35.      Solution to Exercise 4.10 continued.
May 7, 2022   11:14       Parallel Algorithms    9in x 6in       b4591-ch04                  page 217




                                 The Linear Array and the Mesh                        217

                 Given the matrix A stored one element per processor in an n × n
                 mesh, show how to compute AT . What is the number of steps in
                 your algorithm?
                 This is a special case of the routing problem. Assume the processors
                 are numbered as P1,1 , P1,2 , . . . , Pn,n . The elements in the diagonal
                 will not change; only elements below the diagonal and elements above
                 the diagonal will change. The elements of A will move in parallel. An
                 element below the diagonal stored in processor Pi,j moves rightward
                 until it reaches the diagonal where it switches direction and moves
                 upward until it reaches processor Pj,i . An element above the diagonal
                 stored in processor Pk,l moves downward until it reaches the diag-
                 onal where it switches direction and moves leftward until it reaches
                 processor Pl,k . The number of steps is 2n − 2 since element a1,n in
                 processor P1,n requires this number of moves.

        4.12. Apply the algorithm for odd–even transposition sort on the input
              3, 7, 5, 2. Assume a linear array with four processors.
                 Similar to Example 4.1.

        4.13. Consider Algorithm merge-split, which is a generalization of odd–
              even transposition sort for the case p < n. Let S be a sequence
              of numbers to be sorted, and assume that each of the p proces-
              sors in the linear array holds a subsequence of S of length n/p.
              In Algorithm merge-split, the comparison-exchange operations of
              odd–even transposition sort are replaced with merge-split operations
              on subsequences. Let Si denote the subsequence held by processor Pi .
              In Step 1, each Pi sorts Si using a sequential algorithm. In Step 2
              each odd-numbered processor Pi merges the two subsequences Si
              and Si+1 , into a sorted sequence Si . It retains the ﬁrst half of Si
              and assigns to its neighbor Pi+1 the second half. Step 3 is identical
              to 2 except that it is performed by all even-numbered processors.
              Steps 2 and 3 are repeated alternately. After p/2 iterations, no
              further exchange of elements can take place between two processors,
              where an iteration consists of Steps 2 and 3. Analyze the running
              time of this algorithm.
                 There are p phases, where an iteration consists of two phases. The
                 ﬁrst phase, the sorting step, takes O( np log np ) time. The merge-split
May 7, 2022   11:14          Parallel Algorithms          9in x 6in      b4591-ch04            page 218




        218                                        Parallel Algorithms


                 phases after that take O( np ) time each for a total of O(n) time. Hence,
                 the running time is O(max{ np log np , n}).

        4.14. Do Exercise 4.13 for the case p = log n. Is the algorithm optimal?

                 If p = log n, then the running time is Θ(n), which is optimal.

        4.15. Consider the problem of permutation routing on the mesh with n
              processors, in which every processor tries to send to a diﬀerent des-
              tination. Outline a sorting-based algorithm to route every packet to
              its destination. Compare your algorithm with the greedy algorithm.

                 Sort the packets into column-major order according to the column
                 destination of each packet. It can be shown that this algorithm uses
                 queues of size 1, since there is never any contention for edges. If we
                          √                                                       √
                 use a Θ( n) sorting algorithm, the running time will be Θ( n).
                 However, the running time is more than the greedy algorithm by a
                 constant factor.

        4.16. Modify your algorithm in Exercise 4.15 so that it works for the more
              general one-to-one routing problem, in which not every processor is
              the source of a packet. Note here that no processors Pi and Pj send
              to the same destination.

                 First, sort the packets into column-major order according to the col-
                 umn destination of each packet. Then, route each packet to its correct
                 column, and then on to its correct destination. It can be shown that
                 this algorithm uses queues of size 1, since there is never any con-
                                                   √
                 tention for edges. If we use a Θ( n) sorting algorithm, the running
                                  √
                 time will be Θ( n). However, the running time is more than the
                 greedy algorithm by a constant factor.

        4.17. Illustrate the operation of the odd–even merging algorithm on the
              input:

                      A = 1, 9, 8, 17, 3, 11, 14, 12 and B = 2, 5, 15, 7, 13, 9, 16, 10.

                 Assume a mesh of 16 processors.

                 Similar to Example 4.4.
May 7, 2022   11:14       Parallel Algorithms     9in x 6in      b4591-ch04                   page 219




                                 The Linear Array and the Mesh                         219

        4.18. Show how to compute the preﬁx sums on the mesh for the snakelike
              indexing scheme.
                 Similar to that for the row-major indexing scheme discussed in Sec-
                 tion 4.4.

        4.19. In a window broadcast, we start with data in the top left w × w
                            √      √                  √                    √
              submesh of a n × n mesh, where w | n, that is w divides n.
              Following the window broadcast operation, the initial w × w win-
              dow tiles the entire mesh. Outline an algorithm to implement this
              operation. What is the running time of your algorithm?
                 The data in the initial window simply moves to the bottom and to
                                                  √
                 the right. The algorithm takes 2( n − w) steps.

        4.20. Give an algorithm to evaluate the polynomial an−1 xn−1 +an−2 xn−2 +
                                                          √      √
              · · · + a1 x + a0 at the point x0 on the n × n mesh. Assume that
              each ai is stored in processor Pi , 0 ≤ i ≤ n − 1 (the processors
              are indexed as P0 , P1 , . . . , Pn−1 ). What is the running time of your
              algorithm?
                 Compute the sequence 1, x0 , x20 , . . . , xn−1
                                                             0   using parallel preﬁx. Each
                 xj0 is stored in Pj , 0 ≤ j ≤ n− 1. Next, compute the products aj × xj0 ,
                 0 ≤ j ≤ n − 1. Finally, compute the sum a0 + a1 x0 + a2 x20 + · · · +
                                                               √
                 an−1 xn−1
                         0   . The total running time is Θ( n).

        4.21. Consider the following method for sorting on the mesh. The method
              alternately sorts all rows from left to right and all columns from
              top to bottom. Will this method always work in sorting any input?
              Assume an unlimited amount of time.
                 The method will not work in sorting any input. We will succeed in
                 showing this, if we can exhibit an example in which the method does
                 not terminate, or terminates before sorting the input. We will choose
                                                 
                                       x1,1 x1,2 
                 the latter. Let M =            
                                                  
                                       x2,1 x2,2 
                 where x1,1 = 3, x1,2 = 2, x2,1 = 1 and x2,2 = 4. After sorting by rows
                 and then by columns, M becomes:
                                                                
                          2 3                              1 3
                  Mh =                             Mv =       .
                            1 4                               2 4
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch04                    page 220




        220                                      Parallel Algorithms

                 Clearly, Mv is sorted by rows and by columns, but the input is not
                 sorted. So, the method terminated without sorting the input.

        4.22. Consider sorting the rows and then the columns of a 2 × n mesh M .
              Does this leave the rows in sorted order?
                 Call a column Cj of the mesh “good” if sorting that column leaves the
                 rows sorted. We prove by induction on the number of columns that
                 all columns are good, and hence sorting the mesh by columns leaves
                 the rows sorted. If all columns are unsorted, then there is nothing
                 to prove, as exchanging the two rows leaves them sorted. So, assume
                 without loss of generality that column C1 = x1,1 , x2,1  is sorted, that
                 is, x1,1 < x2,1 . Hence, column C1 is good by assumption. Assume for
                 the induction hypothesis that column Ck−1 is good, 1 < k < n. We
                 show that column Ck is also good. We have the following situation:
                                                                      
                                         x1,1 . . . x1,k−1 x1,k . . . 
                                                                      
                                   M=                                 .
                                         x2,1 . . . x2,k−1 x2,k . . . 

                 By induction, x1,k−1 < x2,k−1 < x2,k . If x1,k > x2,k , then we have
                 the following situation after sorting column Ck :
                                                                     
                                        x1,1 . . . x1,k−1 x2,k . . . 
                                                                     
                                 M=                                  .
                                        x2,1 . . . x2,k−1 x1,k . . . 

                 In this case, we have x1,k−1 < x2,k−1 < x2,k < x1,k , whence x1,k−1 <
                 x2,k and x2,k−1 < x1,k . Thus, column Ck is good, and, by induction,
                 all columns are good. It follows that if all columns are sorted, then
                 the rows will remain sorted.

        4.23. This is a generalization of Exercise 4.22. Consider sorting the rows
              and then the columns of a general n × n mesh. Does this leave the
              rows in sorted order?
                 Call a column Cj of the mesh “good” if sorting that column leaves the
                 rows sorted. We prove by induction on the number of columns that
                 all columns are good, and hence sorting the mesh by columns leaves
                 the rows sorted. Let the ﬁrst column be C1 = x1,1 , x2,1 , . . . , xn,1 
                 and let C1 after sorting be C1 = x1,1 , x2,1 , . . . , xn,1 . Thus, we have
                 xj,1 ≤ xj,1 for 1 ≤ j ≤ n. Since row i is sorted, and since xi,1 ≤
                 xi,1 , we have xi,1 ≤ xi,2 . Therefore, we may assume without loss of
May 7, 2022   11:14       Parallel Algorithms    9in x 6in       b4591-ch04                  page 221




                                 The Linear Array and the Mesh                        221

                 generality that column C1 is sorted. Hence, column C1 is good by
                 assumption. Assume for the induction hypothesis that column Ck−1
                 is good, 1 < k < n. We show that column Ck is also good. We have
                 the following situation:
                                                                    
                                                    ..     ..       
                                                     .      .       
                                                                    
                                         . . . xi,k−1  x      . . . 
                                                        i,k         
                                                  .      .          
                                   M=             ..     ..         ,
                                                                    
                                        ... x          xj,k . . . 
                                                j,k−1
                                                   ..     ..        
                                                    .      .        

                 We show that sorting column k leaves the rows sorted. We will use
                 selection sort algorithm to sort column k. Recall that this algorithm
                 sorts by interchanging the elements to be sorted if they are out of
                 order. Let xi,k and xj,k , where i < j, be the next two numbers
                 in column k to be interchanged because xi,k > xj,k . We have the
                 following situation for columns k − 1 and k after the interchange of
                 xi,k and xj,k :
                                                                     
                                                   ..     ..         
                                                    .      .         
                                                                     
                                          . . . xi,k−1  xj,k . . . 
                                         
                                                    ..     ..        
                                   M=                .      .        ,
                                                                     
                                         ... x           xi,k . . . 
                                                 j,k−1
                                                     ..     ..       
                                                      .      .       

                 where xi,k−1 < xi,k and xj,k−1 < xj,k . By Exercise 4.22, exchanging
                 xi,k and xj,k will leave the two rows i and j sorted. Now, the pro-
                 cedure is repeated for each pair xi ,k and xj  ,k that are out of order
                 until column k is sorted. Thus, column Ck is good, and, by induction,
                 all columns are good. It follows that if all columns are sorted, then
                 the rows will remain sorted.

        4.24. Let A = a1 , a2 , . . . , an  be a sequence of elements stored in the
                              √         √
              processors of a n × n mesh, one element per processor, and let x
              be a given element. Design an algorithm for the search problem in
              the mesh: If ai = x for some i, 1 ≤ i ≤ n, then return i, else return 0.
              Analyze its running time.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch04               page 222




        222                                     Parallel Algorithms

                 Assume the processors are numbered P1 , P2 , . . . , Pn . First, initialize
                 the search index k ← 0, which is stored in P1 . Next broadcast x to
                                      √
                 all processors in Θ( n) time. Each processor Pj now compares aj
                                                                                √
                 with x. If aj = x, then processor Pj sends j to P1 in Θ( n) time,
                 which sets k ← j. Note that we have assumed here that the aj ’s are
                                                       √
                 distinct. The total running time is Θ( n).

        4.25. How many steps are required by the matrix multiplication algorithm
              on the mesh of Section 4.10.1?
                   √                                                  √
              The nth row (and column) will start moving in the nth step,
                           √
              and it needs n − 1 steps to arrive at the processor holding c√n,√n .
                                                   √
              Hence, the total number of steps is 2 n − 1.

        4.26. Show the results of the computations of c1,3 and c1,4 in the matrix
              multiplication algorithm on the mesh of Section 4.10.1.
                 Similar to Table 4.1 in Example 4.5.
                                          √      √
        4.27. Give an algorithm for the n × n mesh to determine whether a
              given graph G is cyclic or acyclic. What is the running time of your
              algorithm?
                 Let A be the adjacency matrix of G. Find A∗ , the transitive closure
                 of A. G is cyclic if and only if there is a 1 in the diagonal of A∗ . The
                                      √
                 running time is Θ( n).

        4.28. Let G be a connected undirected and unweighted graph on n vertices.
              A breadth-ﬁrst spanning tree for G is a spanning tree that can be
              obtained by performing breadth-ﬁrst traversal on G starting at some
              vertex, say r. Equivalently, a breadth-ﬁrst spanning tree of G is a
              tree in which every path from the root to any vertex is of shortest
              length, where the distance is measured in terms of number of edges.
                                                                        √      √
              Present an eﬃcient algorithm to ﬁnd such a tree for the n × n
              mesh. What is the running time of your algorithm?
                 Deﬁne the weight matrix w by: w[i, j] = 1 if there is an edge between i
                 and j, and w[i, j] = ∞ if there is no such edge. Use the shortest paths
                 algorithm to ﬁnd the distance d[r, j] from r to every other vertex j.
                 Then, d[r, j] is the level of vertex j. For all vertices in V (G) − {r},
                 select an edge that connects a vertex at level l to a vertex at level
May 7, 2022   11:14       Parallel Algorithms       9in x 6in         b4591-ch04                 page 223




                                 The Linear Array and the Mesh                             223

                 l − 1. The resulting tree is a breadth-ﬁrst spanning tree for G. The
                                    √
                 running time is Θ( n).

        4.29. Suggest another algorithm for computing the transitive closure of a
              matrix A diﬀerent from the one given in Section 4.11. What is the
              running time of the algorithm?
                 Use an algorithm analogous to the one for the PRAM presented
                 in Section 2.17. Recall that this algorithm computes the transitive
                 closure by squaring the adjacency matrix log n times. Thus, the
                                   √
                 running time is Θ( n log n).

        4.30. Suggest another algorithm for computing the shortest paths in a
              directed graph G diﬀerent from the one given in Section 4.13. What
              is the running time of the algorithm?
                 Use an algorithm analogous to the one for the PRAM presented in
                                                     √
                 Section 2.18. The running time is Θ( n log n).

        4.31. Illustrate the operation of Algorithm columnsort discussed in Sec-
              tion 4.16 on the input

                          17, 1, 18, 12, 8, 10, 11, 2, 4, 14, 5, 6, 9, 13, 15, 16, 7, 3,

                 where n = 18. Assume an r × s mesh, where r = 6 and s = 3.
                 Similar to Example 4.8.

        4.32. Illustrate the operation of Algorithm columnsort2 discussed in Sec-
              tion 4.16 on the input

                          8, 10, 11, 2, 4, 14, 5, 6, 17, 1, 18, 12, 9, 13, 15, 16, 7, 3,

                 where n = 18. Assume an r × s mesh, where r = 6 and s = 3.
                 Similar to Example 4.9.

        4.33. Explain why Algorithm columnsort does not work on square
              meshes.
                 Note that after Step 4, every number will be within (s − 1)2 of its
                                                                   √
                 correct sorted position. Thus, if we let r = s = n, every number
                                 √
                 will be within ( n − 1)2 = Θ(n) of its correct sorted position, which
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch04            page 224




        224                                     Parallel Algorithms

                 means that nothing is gained by applying the algorithm on a square
                 mesh.

        4.34. This exercise is similar to Exercise 2.12. Consider Algorithm
              columnsort discussed in Section 4.16. If we let s = 2, then the
              algorithm reduces to Algorithm oddevenmerge in Section 2.11 for
              odd–even merging with the even part of A merged with the even
              part of B and the odd part A merged with the odd part of B. Let A
              and B be the ﬁrst and second columns after Step 1, respectively.
              Let C and D be the ﬁrst and second columns after Step 3, respec-
              tively. Let E be the whole list after Step 5. Assume the elements
              in A ∪ B are distinct. Given a sequence X and an element x, recall
              that rank(x, X) is the number of elements in X less than x. Express
              rank(x, C) and rank(x, D) in terms of rank(x, A) and rank(x, B).
                 Let x ∈ A ∪ B. Then,
                                                                  
                                           rank(x, A)     rank(x, B)
                              rank(x, C) =             +               ,
                                               2              2
                 and
                                                                 
                                          rank(x, A)     rank(x, B)
                             rank(x, D) =             +              .
                                              2              2

        4.35. This exercise is similar to Exercise 2.13. Use the result of Exer-
              cise 4.34 to show that for c ∈ C, either c is in its correct position in
              E or to the right of it.
                 For x ∈ X, let pos(x, X) be the position of x in the sequence X, where
                 pos(x, X) ≥ 0. Thus, if X is sorted, then pos(x, X) = rank(x, X).
                 For c ∈ C, let r1 = rank(c, A) and r2 = rank(c, B), and rc = r1 + r2 .
                 Either c ∈ A or c ∈ B. If c ∈ A, then r1 is even since pos(c, A) is
                 even, and it follows that the position of c in E is
                              pos(c, E) = 2rank(c,
                                                   C)
                                                     
                                        = 2 2 + 2 r22
                                             r1

                                        ≤ r1 + (r2 + 1), since r1 is even
                                        = rc + 1.
                                                
                 Since rc = r1 + r2 ≤ 2 r21 + 2 r22 = pos(c, E), we have

                                            rc ≤ pos(c, E) ≤ rc + 1.               (4.13)
May 7, 2022   11:14       Parallel Algorithms           9in x 6in               b4591-ch04            page 225




                                 The Linear Array and the Mesh                                 225


                 Thus, either pos(c, E) = rc or pos(c, E) = rc + 1. That is, either c is
                 in its correct position in E or to the right of it.
                 On the other hand, if c ∈ B, then r2 is even since pos(c, B) is even,
                 and we get the same inequalities.

        4.36. This exercise is similar to Exercise 2.14. Use the result of Exer-
              cise 4.34 to show that for d ∈ D, either d is in its correct position in
              E or to the left of it.
                 For x ∈ X, let pos(x, X) be the position of x in the sequence X, where
                 pos(x, X) ≥ 0. Thus, if X is sorted, then pos(x, X) = rank(x, X).
                 For d ∈ D, let r3 = rank(d, A), r4 = rank(d, B) and rd = r3 + r4 . If
                 d ∈ A then r3 is odd since pos(d, A) is odd. It follows that if d ∈ A,
                 then the position of d in E is
                            pos(d, E) =2rank(d,
                                                 D)  +
                                                        1
                                           r3       r4
                                      =2 2 +2 2 +1
                                      ≤(r3 − 1) + (r4 ) + 1, since r3 is odd
                                      =rd .
                                                            
                 Since rd − 1 = (r3 − 1) + (r4 ) ≤ 2 r23 + 2 r24 + 1 = pos(d, E), we
                 have

                                             rd − 1 ≤ pos(d, E) ≤ rd .                       (4.14)

                 Thus, either pos(d, E) = rd or pos(d, E) = rd − 1. That is, either d
                 is in its correct position in E or to the left of it.
                 If d ∈ B, then r4 is odd, and we get the same inequalities.

        4.37. Illustrate the operation of the algorithm for sorting on a
              3-dimensional mesh/sorting on 3-dimensional meshes on the input
              shown in Fig. 4.36.

                                                1           1               1
                                            0           1               1
                                      0             0           1
                                                0           1               1
                                            0           1               0
                                      0             1               0
                                                0           1               1
                                            0           1               1
                                      1             0           1

                                          Fig. 4.36.    Exercise 4.37.
May 7, 2022   11:14       Parallel Algorithms               9in x 6in                       b4591-ch04                page 226




        226                                      Parallel Algorithms


                                                 17             13                  25
                                            5              22                  16
                                       6              3                   14
                                                 24             21                  15
                                            18             12                  8
                                       7              27                  23
                                                 2              26                      4
                                            9              10                  11
                                       1              19                  20

                                           Fig. 4.37.      Exercise 4.38.


                                   5                                                               5   5    5    5
                                   1                                                               1   1    1    1
                             1     5                                 1     1        1        1     5   5    5    5
                             1     1                                 1     1        1        1     1   1    1    1


                            29 28                                    29       29 28
                                                                           29 29                       28   28   28
                            28 28                                    28       28 28
                                                                           28 28                       28   28   28
                               37                                                37                    37   37   37
                               28                                                28                    28   28   28
                            35 37                                    35 35 35 35 37                    37   37   37
                            28 28                                    28 28 28 28 28                    28   28   28


                            50 57                                    50    50 50 50 57
                            50 50                                    50    50 50 50 50
                                 (a)                                                             (b)

         Fig. 4.38.   The values of α and β corresponding to Fig. 4.24(c)(Exercise 4.39).

                 Similar to Example 4.12.

        4.38. Illustrate the operation of the algorithm for sorting on 3-dimensional
              mesh on the input shown in Fig. 4.37.
                 Similar to Example 4.13.

        4.39. Compute the values of α and β corresponding to Fig. 4.24(c).
                 The values of α and β corresponding to Fig. 4.24(c) are shown in
                 Fig. 4.38. Part (a) of the ﬁgure shows the values of α and β computed
                 in the middle two columns, and part (b) shows the α and β values
                 after copying them to their corresponding rows. In this ﬁgure, α is
                 shown on the top and β on the bottom. The values with α = β = 0
                 are not shown.
May 7, 2022   11:14      Parallel Algorithms       9in x 6in    b4591-ch05                    page 227




                                               Chapter 5


                        Fast Fourier Transform



        5.1     Introduction

        The Fourier transform has a wide range of applications in science and engi-
        neering. We will describe a version of Fourier transform called discrete
        Fourier transform(DFT), and present a fast method for computing the
        DFT, called the fast Fourier transform(FFT).
            Let r and θ be the polar coordinates of the point (x, y) corresponding
                                                         √
        to the complex number z = x + iy, where i = −1. Since x = r cos θ and
        y = rsin θ, z can be written in polar form as z = r(cos θ + isin θ). Using
        Euler’s formula eiθ = cos θ + isin θ, z can also be written as z = reiθ .
           For n ≥ 2, the n distinct roots of the equation xn − 1 = 0 are called the
        n roots of unity. Deﬁne the complex number
                                  ω = ei2π/n = cos 2π         2π
                                                    n + i sin n .

        ω is called a primitive nth root of unity, which means ω n = 1 and ωj = 1
        for 0 < j < n. If ω n = 1, then (ω j )n = (ω n )j = 1. Hence, the remaining
        complex roots of unity are the powers of ω. That is, 1 = ω0 , ω, ω 2 , . . . , ωn−1
        constitute the n distinct roots of unity, where
                               ω k = ei2πk/n = cos 2πk       2πk
                                                    n + i sin n .

           Pictorially, these roots are distributed in the complex plane evenly
        around the circumference of the unit circle. Figure 5.1 illustrates the n
        roots of unity for n = 2, 4, 8, which are powers of 2. As shown in the ﬁgure,


                                                  227
May 7, 2022   11:14           Parallel Algorithms               9in x 6in             b4591-ch05           page 228




        228                                         Parallel Algorithms


                                                                                     ω1= i
                      (a) n = 2.                                  (b) n = 4.


                      ω = -1                ω0= 1
                                                                     2
                                                                   ω = -1                          ω0= 1



                                                                                  ω3= -i
                                                           2
                                (c) n = 8.              ω = -i
                          3
                         ω = -(1/2) + (1/2)i                             ω1= (1/2) + (1/2)i


                                      4
                                   ω = -1                                    ω0= 1



                        ω5= -(1/2) -(1/2)i                               ω7= (1/2) -(1/2)i

                                                        ω6= i

                               Fig. 5.1.        The n roots of unity for n = 2, 4, 8.


        the pairs ωj and ω j+n/2 are symmetrically located with respect to the ori-
        gin. Algebraically, we have ω j+n/2 = −ωj (Property 5.2), and in particular,
        ω n/2 = −1.
            Let a be the column vector [a0 , a1 , . . . , an−1 ]T , where n is a power of 2.
        Let Fn be the Vandermonde matrix
                               ⎛                                                             ⎞
                                1           1          1           ...            1
                               ⎜                                                    ⎟
                               ⎜1           ω          ω2          ...         ω n−1⎟
                               ⎜                                                    ⎟
                               ⎜1          ω2          ω4                  ω 2n−2 ⎟
                               ⎜                                   ...              ⎟
                               ⎜                                                    ⎟
                               ⎜ ..          ..         ..          ..        ..    ⎟
                               ⎜.             .          .           .         .    ⎟
                               ⎜                                                    ⎟
                               ⎜1         ω n−2     ω2(n−2)        ... ω (n−1)(n−2) ⎟
                               ⎝                                                    ⎠
                                                                                  2
                                1         ω n−1     ω2(n−1)        ...    ω (n−1)

        Then, the product b = Fn a is called the Discrete Fourier Transform (DFT)
        of a.

           Thus, computing the DFT b of a vector a is equivalent to evaluating the
        polynomial P (x) = a0 +a1 x+. . .+an−1 xn−1 at the points 1, ω, ω 2 , . . . , ωn−1 .
May 7, 2022   11:14       Parallel Algorithms           9in x 6in            b4591-ch05                   page 229




                                        Fast Fourier Transform                                     229


           It is easy to see that the DFT of a vector a can be computed in Θ(n2 )
        sequential time and Θ(log n) parallel time using n2 / log n processors on
        the PRAM. We now show that it can be computed in optimal Θ(n log n)
        sequential time and Θ(log n) parallel time using n processors on the PRAM.
        The eﬃciency of the algorithm is based on the following properties of the
        n roots of unity.

        Property 5.1 For even n, if ω is an nth root of unity, then ω 2 is an
        (n/2)th root of unity.

        Property 5.2         For even n, ω k+n/2 = −ωk .

              For 0 ≤ i < n/2, bi can be expressed as

                                
                                n−1
                         bi =         (ω i )j aj
                                j=0

                            = (ω i )0 a0 + (ω i )1 a1 + · · · + (ω i )n−1 an−1
                            = (ω i )0 a0 + (ω i )2 a2 + · · · + (ω i )n−2 an−2
                                + (ω i )1 a1 + (ω i )3 a3 + · · · + (ω i )n−1 an−1
                                (n/2)−1                  (n/2)−1
                                                           
                                              i 2j
                            =             (ω ) a2j +                  (ω i )2j+1 a2j+1
                                  j=0                       j=0

                                (n/2)−1                        (n/2)−1
                                                               
                            =             (ω 2i )j a2j + ω i             (ω 2i )j a2j+1 .         (5.1)
                                  j=0                           j=0

        Since
                                                      ωki           if k is even
                                (ω i+(n/2) )k =
                                                      −ωki          if k is odd,

        we have
                                      (n/2)−1                        (n/2)−1
                                                                      
                      bi+(n/2) =                (ω 2i )j a2j − ωi              (ω 2i )j a2j+1 .   (5.2)
                                        j=0                            j=0

           By Eqs. (5.1) and (5.2), Fn a is computed recursively from F(n/2) ae and
        F(n/2) ao , where ae and ao are, respectively, the even and odd parts of a.
May 7, 2022   11:14        Parallel Algorithms           9in x 6in          b4591-ch05                    page 230




        230                                      Parallel Algorithms

              Let
                                      ⎛             ⎞                        ⎛           ⎞
                                           a0                                     a1
                                  ⎜        a2       ⎟                  ⎜          a3     ⎟
                                  ⎜                 ⎟                  ⎜                 ⎟
                                  ⎜        a4       ⎟                  ⎜          a5     ⎟
                       c = F(n/2) ⎜                 ⎟   and d = F(n/2) ⎜                 ⎟
                                  ⎜         ..      ⎟                  ⎜           ..    ⎟
                                  ⎝          .      ⎠                  ⎝            .    ⎠
                                          an−2                                   an−1

              Then, for 0 ≤ i < n/2, Eqs. (5.1) and (5.2) can be rewritten as

                                                 bi = ci + ωi di ,                                (5.3)

        and

                                            bi+n/2 = ci − ω i di .                                (5.4)

           On the PRAM, this gives rise to the recurrence T (n) = T (n/2) + Θ(1),
        which solves for T (n) = Θ(log n). The number of processors needed is Θ(n).


        Example 5.1           Let a = [1, 2, 3, 4]T . In this example, we compute F4 a,
        where
                                                ⎛                ⎞
                                            1           1  1  1
                                          ⎜1             i −1 −i ⎟
                                     F4 = ⎜
                                          ⎝1
                                                                 ⎟.
                                                        −1 1 −1 ⎠
                                            1           −i −1 i

                       1    1               1    1       1            4                  1    1   2
        Since F2 =     1   −1    ,c=        1    −1      3   =       −2   , and d =      1   −1   4   =
           6
          −2    . By Eq. (5.3), b0 = c0 + i0 d0 = 10, and b1 = c1 + i1 d1 = −2 − 2i,
        since ω = i. By Eq. (5.4), b2 = c0 − i0 d0 = −2, and b3 = c1 − i1 d1 = −2 + 2i.
        Hence,
                                                         ⎛   ⎞
                                                       10
                                                   ⎜ −2 − 2i ⎟
                                        b = F4 a = ⎜
                                                   ⎝ −2 ⎠ ,
                                                             ⎟

                                                     −2 + 2i

        as can be veriﬁed by direct multiplication.                                                   
May 7, 2022   11:14          Parallel Algorithms       9in x 6in          b4591-ch05         page 231




                                          Fast Fourier Transform                       231


                                          level 3 level 2 level 1 level 0
                                                            c0      b0


                                                            c1      b1


                                                            c2      b2

                                                            c3      b3


                                                            d0      b4


                                                            d1      b5


                                                            d2      b6

                                                            d3       b7

                      Fig. 5.2.   Implementation of FFT on the butterﬂy for n = 8.


        5.2     Implementation on the Butterfly

        By Eqs. 5.3 and 5.4, the implementation of the Fourier transform on the
        d-dimensional butterﬂy, where n = 2d , is straightforward. These two equa-
        tions are implemented naturally on the butterﬂy as shown in Fig. 5.2 for
        n = 8. The bi ’s are computed recursively in level 0, and ci ’s and di ’s are
        computed recursively in level 1, and so on. As an example in the ﬁgure, b3
        is computed as b3 = c3 + ω3 d3 and b6 is computed as b6 = c2 − ω 2 d2 .
            Each parallel step is carried out by one level of the butterﬂy. Hence,
        the number of parallel steps can be expressed by the recurrence T (n) =
        T (n/2) + 1, whence the number of steps is equal to d = log n.


        5.3     Iterative FFT on the Butterfly

        Unfolding recursion in the FFT algorithm discussed above results in a
        simple iterative procedure for computing Fn a on the d-dimensional but-
        terﬂy, where n = 2d . The algorithm proceeds in the reverse order, from
        level d to level 0, where the processors in level d contain the input.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch05                  page 232




        232                                     Parallel Algorithms


        If a = [a1 , a2 , . . . , an ]T , then aj is stored in node (j R , d), where j R is the
        number whose representation in binary is the reverse of the representation
        of j. For example, if j = 1, and the number of bits is 3, then j R = 4.
        The reason for this renumbering is that in the recursive algorithm, the
        items are divided into even and odd. The items are divided into two halves;
        those even in the upper half have 0 as their most signiﬁcant bit, and those
        odd in the lower half have 1 as their most signiﬁcant bit. Appending 0’s
        and 1’s is repeated recursively with repeated divisions into even and odd
        halves.
            The algorithm proceeds in d phases corresponding to levels d − 1,
        d − 2, . . . , 0, where the output of each phase except the last is the input
        to the next. Each phase is carried out in one parallel step, for a total of d
        parallel steps. In phase 1, the algorithm starts by evaluating the contents
        of the processors at level d − 1. Each pair of consecutive processors perform
        the multiplication F2 u, where u is the vector of corresponding pair of val-
        ues entered at level d. F2 u is not computed using the recursive algorithm
        discussed above, or using direct matrix multiplication; it is computed using
        Eqs. (5.3) and (5.4). There are n/2 computations of the products F2 u.
        Next, in phase 2, each group of four consecutive processors in level d − 2
        perform the multiplication F4 v using Eqs. (5.3) and (5.4), where v is the
        vector of corresponding four elements computed in phase 1 while processing
        level d − 1. There are n/4 computations of the products F4 v. This process
        of doubling the group size in each phase and computing the Fourier trans-
        forms using Eqs. (5.3) and (5.4) is repeated in the following phases, phases
        3, 4, . . . , d, until the ﬁnal product Fn a is computed. In general, in phase j,
        n/2j computations of F2j w in level d − j are carried out using Eqs. (5.3)
        and (5.4).

        Example 5.2 (See Fig. 5.3). As in Example 5.1, let a = [1, 2, 3, 4]T . We
        compute F4 a. The input is entered into level d = 2, where aj is stored in
        node (j R , d), as explained above. In phase 1 of the algorithm, the contents
        of the processors at level d − 1 = 1 are evaluated. Each pair of consecutive
        processors perform the multiplication F2 u using Eqs. 5.3 and 5.4, where u
        is the vector of corresponding pair of values entered at level 2. For example,
        the contents of node (0, 1) are computed as c0 + (−1)0 d0 = 1 + (−1)0 3 = 4
        (here ω = −1). Similarly, the contents of node (1, 1) are computed as
May 7, 2022   11:14      Parallel Algorithms                9in x 6in         b4591-ch05                                    page 233




                                         Fast Fourier Transform                                                       233


               level 2                       level 1 ω = −1                                       level 0 ω = i

                                   c0                  0
                                                                                    c0                 0
                 1                             c0+ (-1) d0= 4                                    c0+ i d0 = 10
                                  d0
                                                                                   d0
                                   c0

                                   d0                  0                           c1
                 3                             c0- (-1) d 0= -2                                  c1+ i d1 = -2 - 2i
                                                                                  c0

                                                                             d1
                                    c0                                                 d0
                                                                                                       0
                 2                                     0                                         c0 - i d0 = -2
                                   d0          c0+ (-1) d0= 6
                                                                                       c1
                                        c0
                                        d0             0                      d1
                 4                             c0- (-1) d 0= -2                                  c1 - i d1 = -2 + 2i



                            Fig. 5.3.        Iterative FFT on the butterﬂy.


                                                                                   1         1     1               4
        c0 − (−1)0 d0 = 1 − (−1)0 3 = −2. Hence, F2 u =                            1        −1     3       =      −2    .
                                                                   1    1     2                   6
        Likewise, in the lower half of level 1,                =   1    −1. Next, in
                                                                              4                  −2
        phase 2, the group of four consecutive processors in level 0 perform the
        multiplication F4 v using Eqs. 5.3 and 5.4, where v = [4, −2, 6, −2]T is the
        vector of corresponding four elements computed in phase 1. For example,
        the contents of node (0, 0) are computed as c0 + i0 d0 = 4 + i0 6 = 10
        (here ω = i). Similarly, the contents of node (1, 0) are computed as
        c1 + id1 = −2 + i(−2) = −2 − 2i. Likewise, the contents of nodes (2, 0)
        and (3, 0) are computed as −2 and −2 + 2i, respectively. Hence,
                               ⎛           ⎞⎛     ⎞ ⎛         ⎞
                                 1 1  1  1     4        10
                               ⎜ 1 i −1 −i ⎟ ⎜ −2 ⎟ ⎜ −2 − 2i ⎟
                 F4 a = F4 v = ⎜           ⎟⎜     ⎟ ⎜
                               ⎝ 1 −1 1 −1 ⎠ ⎝ 6 ⎠ = ⎝ −2 ⎠ .
                                                              ⎟

                                 1 −i −1 i     −2     −2 + 2i

        This conforms with the result obtained in Example 5.1.                                                         
May 7, 2022   11:14    Parallel Algorithms           9in x 6in       b4591-ch05         page 234




        234                                  Parallel Algorithms

        5.4     The Inverse Fourier Transform

        The inverse of the matrix     Fn turns out to be easy to describe:
           for 1 ≤ k < n, the kth     row of nFn−1 is the n − kth row of Fn :
                             ⎛                                          ⎞
                               1         1       1      ...        1
                             ⎜1        ω n−1 ω 2(n−1) . . .    ω (n−1) ⎟
                                                                      2
                             ⎜                                          ⎟
                             ⎜                                          ⎟
                           1 ⎜1        ω n−2 ω 2(n−2) . . . ω(n−1)(n−2) ⎟
                    Fn−1 = ⎜                                            ⎟
                           n⎜   .
                             ⎜ ..
                                          ..
                                           .
                                                 ..
                                                  .
                                                         ..
                                                          .
                                                                   ..
                                                                    .
                                                                        ⎟
                                                                        ⎟
                             ⎜                                          ⎟
                             ⎝1         ω2      ω4      ...     ω 2n−1 ⎠
                               1         ω      ω2      ...     ωn−1

        Simplifying yields another easy description of Fn−1 :
                           ⎛                                             ⎞
                             1      1         1       ...         1
                           ⎜1     ω −1       ω −2             ω−(n−1) ⎟
                           ⎜                          ...                ⎟
                           ⎜1       −2         −4               −2(n−1) ⎟
                         1 ⎜      ω          ω        . .  .  ω          ⎟
                 Fn−1 = ⎜  ⎜ ..     ..         ..       ..        ..     ⎟
                                                                         ⎟
                         n ⎜.        .          .        .         .     ⎟
                           ⎜                                             ⎟
                           ⎝1 ω −(n−2) ω −2(n−2) . . . ω−(n−2)(n−1) ⎠
                                                                       2
                             1 ω −(n−1) ω −2(n−1) . . .       ω −(n−1)
        That is,
                                                           ω −ij
                                             (Fn−1 )ij =         .
                                                             n
        So, the inverse of Fn is 1/n times the Fourier transform matrix of a diﬀerent
        primitive root of unity, namely ω −1 .
            To show that it is indeed the inverse of Fn , we need the following
        property.

        Property 5.3 Since
                                
                                n−1
                                               ωn − 1   1−1
                                      ωj =            =     = 0,
                                j=0
                                               ω−1      ω−1

        we have
                              
                              n−1
                                                 0   if i ≡ 0(mod n)
                                     ω ij =
                                                 n   if i ≡ 0(mod n).
                               j=0
May 7, 2022   11:14      Parallel Algorithms         9in x 6in           b4591-ch05             page 235




                                      Fast Fourier Transform                              235

              By Property 5.3, we have

                                                      1  ik −kj
                                                        n−1
                                 (Fn × Fn−1 )ij =           ω ω
                                                      n
                                                         k=0

                                                        1 
                                                          n−1
                                                    =             ω k(i−j)
                                                        n
                                                            k=0
                                    = 1 if i = j and 0 otherwise.

        Example 5.3
                         ⎛                    ⎞                        ⎛              ⎞
                           1 1          1  1                          1      1  1  1
                         ⎜1 i           −1 −i ⎟                    1⎜ 1      −i −1 i ⎟
              Since F4 = ⎜
                         ⎝1 −1
                                              ⎟,         F4−1     = ⎜                 ⎟
                                        1 −1⎠                      4 ⎝1      −1 1 −1⎠
                           1 −i         −1 i                          1       i −1 −i

        as can be easily veriﬁed.                                                          


           Clearly, the algorithm for the inverse Fourier transform is the same as
        the algorithm for FFT described above.


        5.5     Product of Polynomials

        Let f (x) be a polynomial of degree n − 1, that is,

                               f (x) = a0 + a1 x + · · · + an−1 xn−1 .

        A point-value representation of f (x) is a sequence of n (point, value) pairs

                        (x0 , f (x0 )), (x1 , f (x1 )), . . . , (xn−1 , f (xn−1 ))

        such that the xj ’s are distinct. The process of computing the coeﬃcients of
        f (x) from its point-value representation is called interpolation. For example,
        the pairs (0, 1), (2, 3) is a representation of the polynomial f (x) = x + 1,
        whose coeﬃcients can be obtained by interpolating this sequence of (point,
        value) pairs.
May 7, 2022   11:14      Parallel Algorithms          9in x 6in            b4591-ch05               page 236




        236                                    Parallel Algorithms


              Let f (x) and g(x) be two polynomials of degree n − 1, where
                                      
                                      n−1                                
                                                                         n−1
                           f (x) =          aj xj   and g(x) =                 bj xj ,
                                      j=0                                j=0

        where n is a power of 2. The product polynomial h(x) is given by
                                                             2n−1
                                                              
                                   h(x) = f (x)g(x) =               cj xj ,
                                                             j=0

        where c2n−1 = 0.
            Recall that if a is a vector of n coeﬃcients of the polynomial f (x), then
        Fn a denotes the vector consisting of the values of f (x) evaluated at the n
        roots of unity. Likewise, Fn b denotes the vector consisting of the values of
        g(x) evaluated at the n roots of unity. That is,
            ⎛             ⎞       ⎛      ⎞          ⎛            ⎞      ⎛      ⎞
                f (ω0 )              a0                 g(ω0 )             b0
            ⎜ f (ω1 ) ⎟           ⎜ a1 ⎟            ⎜ g(ω1 ) ⎟          ⎜ b1 ⎟
            ⎜             ⎟       ⎜      ⎟          ⎜            ⎟      ⎜      ⎟
            ⎜      ..     ⎟ = F n ⎜   .. ⎟     and  ⎜     ..     ⎟ = Fn ⎜ . ⎟.
            ⎝       .     ⎠       ⎝    . ⎠          ⎝      .     ⎠      ⎝   . ⎠
                                                                            .
                   n−1                                    n−1
              f (ω      )           an−1              g(ω      )          bn−1
            By inverting Fn , we can perform the process of interpolation, which in
        the above functions obtains the ai ’s from the vector of f (ωi )’s, and the bi ’s
        from the vector of g(ω i )’s. That is,
           ⎛      ⎞         ⎛            ⎞         ⎛      ⎞          ⎛             ⎞
              a0                f (ω 0 )               b0                  g(ω 0 )
           ⎜ a1 ⎟           ⎜ f (ω 1 ) ⎟           ⎜ b1 ⎟            ⎜ g(ω 1 ) ⎟
           ⎜      ⎟         ⎜            ⎟         ⎜      ⎟          ⎜             ⎟
           ⎜ . ⎟ = Fn−1 ⎜           .    ⎟ and ⎜ . ⎟ = Fn−1 ⎜                ..    ⎟.
           ⎝ .. ⎠           ⎝       ..   ⎠         ⎝ .. ⎠            ⎝        .    ⎠
               an−1               f (ωn−1 )                       bn−1                   g(ωn−1 )
        The componentwise product of vectors Fn a                 and Fn b is
                               ⎛                                   ⎞
                                    f (ω 0 )g(ω 0 )
                               ⎜ f (ω 1 )g(ω 1 )                   ⎟
                               ⎜                                   ⎟
                               ⎜            ..                     ⎟,
                               ⎝             .                     ⎠
                                            f (ω n−1 )g(ω n−1 )

        where f (ωi )g(ω i ) = h(ω i ), 0 ≤ i ≤ n − 1. By taking the inverse Fourier
        transform of the componentwise product of vectors Fn a and Fn b, we can
        obtain h(x) in its coeﬃcient form. There is a little diﬃculty, however. Given
May 7, 2022   11:14         Parallel Algorithms       9in x 6in     b4591-ch05                      page 237




                                         Fast Fourier Transform                              237


        a polynomial p(x) of degree m in its (point, value) pairs, it is well-known
        that m + 1 points are needed in order to reconstruct p(x) in its coeﬃcient
        form. The componentwise product of Fn a and Fn b provides the values of
        h(x) at only n points, but h(x) is of degree 2n − 2. Hence, we extend f (x)
        and g(x) to degree 2n − 1 by adding zeros for the terms with degree n
        through 2n − 1. Thus, deﬁne a = [a0 , a1 , a2 , . . . , an−1 , 0, 0, . . . , 0]T , and
        b = [b0 , b1 , b2 , . . . , bn−1 , 0, 0, . . . , 0]T . We compute the coeﬃcients of h(x)
        as
                             ⎛           ⎞        ⎛                   ⎞
                                   c0              f (ω 0 )g(ω 0 )
                             ⎜     c1⎟       ⎜     f (ω 1 )g(ω 1 )    ⎟
                             ⎜       ⎟    −1 ⎜                        ⎟
                             ⎜       ⎟ = F2n ⎜
                                    ..                     ..         ⎟.
                             ⎝       ⎠
                                     .       ⎝              .         ⎠
                                                    2n−1       2n−1
                               c2n−1           f (ω       )g(ω      )

        Note here that ω is the 2nth primitive root of unity. In summary, to con-
        struct the product h(x) = f (x)g(x), we do the following steps:

        (1)   Compute c1 = F2n a , and c2 = F2n b .
        (2)   Perform the componentwise product d = c1 c2 .
                                                                          −1
        (3)   Interpolate by computing the inverse Fourier transform c = F2n d.
                                                        T
        (4)   Output c = [c0 , c1 , c2 , . . . , c2n−1 ] .

        Steps 1 and 3 take Θ(log n) parallel time on the d-dimensional butter-
        ﬂy using Θ(n log n) operations. Step 2 takes Θ(1) parallel time. Hence, the
        algorithm for computing the product of two polynomials requires Θ(n log n)
        operations, and runs in Θ(log n) parallel time on the log n-dimensional but-
        terﬂy. This is much more eﬃcient than the Θ(n2 ) direct multiplication algo-
        rithm.


        Example 5.4 Let f (x) = 1 + 2x and g(x) = 1 + 3x. We will compute the
        product h(x) = f (x)g(x) using the fast Fourier transform. Write f (x) = ax,
        where a = [1, 2], and x = [1, x]T , and g(x) = bx, where b = [1, 3]. Let
        a = [1, 2, 0, 0]T and b = [1, 3, 0, 0]T . Then,
                                    ⎛                 ⎞⎛ ⎞ ⎛             ⎞
                                     1       1  1  1      1          3
                                   ⎜ 1        i −1 −i ⎟ ⎜ ⎟ ⎜            ⎟
                      c1 = F4 a = ⎜                  ⎟ ⎜ 2 ⎟ = ⎜ 1 + 2i ⎟ .
                                   ⎝1        −1 1 −1  ⎠ ⎝ 0 ⎠   ⎝   −1 ⎠
                                     1       −i −1 i      0       1 − 2i
May 7, 2022   11:14          Parallel Algorithms            9in x 6in        b4591-ch05      page 238




        238                                        Parallel Algorithms

        Similarly,
                                   ⎛                  ⎞⎛ ⎞ ⎛             ⎞
                                    1        1  1  1      1         4
                                   ⎜1         i −1 −i ⎟ ⎜ ⎟ ⎜            ⎟
                      c2 = F4 b = ⎜                  ⎟ ⎜ 3 ⎟ = ⎜ 1 + 3i ⎟ .
                                   ⎝1        −1 1 −1  ⎠ ⎝ 0 ⎠   ⎝   −2 ⎠
                                    1        −i −1 i      0       1 − 3i

        Now, we compute c1                 c2 , which is the componentwise multiplication
        of c1 and c2 .
                                      ⎛      ⎞             ⎛        ⎞ ⎛         ⎞
                                        3                      4          12
                                    ⎜ 1 + 2i ⎟             ⎜ 1 + 3i ⎟ ⎜ −5 + 5i ⎟
                        c1     c2 = ⎜
                                    ⎝ −1 ⎠
                                             ⎟             ⎜        ⎟ ⎜
                                                           ⎝ −2 ⎠ = ⎝
                                                                                ⎟.
                                                                                ⎠
                                                                           2
                                      1 − 2i                 1 − 3i     −5 − 5i

        Next, we interpolate.

               c = F4−1 (c1 c2 )
                     ⎛                               ⎞⎛          ⎞   ⎛ ⎞ ⎛ ⎞
                       1 1       1 1                       12          4      1
                   1⎜  1   −i  −1  i                 ⎟ ⎜ −5 + 5i ⎟ 1 ⎜ 20 ⎟ ⎜ 5 ⎟
                 = ⎜                                 ⎟⎜          ⎟ = ⎜ ⎟ = ⎜ ⎟.
                   4 ⎝1 −1 1 −1                      ⎠⎝     2    ⎠ 4 ⎝ 24 ⎠ ⎝ 6 ⎠
                       1 i −1 −i                         −5 − 5i       0      0

        Hence, h(x) = 1 + 5x + 6x2 , as can be veriﬁed by direct multiplication. 

           Computing the product of more than two polynomials can be found in
        the exercises (see Exercises 5.5, 5.6 and 5.7).


        5.6     Computing the Convolution of Two Vectors

        Given two vectors

                          a = a0 , a1 , . . . , an−1      and b = b0 , b1 , . . . , bm−1 ,

        the convolution of a and b, denoted by a ⊗ b, is deﬁned as the vector
        c = [c0 , c1 , . . . , cm+n−1 ]T , such that

                                                          
                                                          i
                                                   ci =         aj bi−j ,
                                                          j=0
May 7, 2022   11:14      Parallel Algorithms              9in x 6in          b4591-ch05         page 239




                                      Fast Fourier Transform                              239

        where aj = 0 for j > n − 1, and bj = 0 for j > m − 1. Convolution is closely
        related to polynomial multiplication. So, if

                                     
                                     n−1                                
                                                                        m−1
                           f (x) =         aj xj     and g(x) =                bj xj ,
                                     j=0                                j=0


        then the kth term in f (x)g(x) is the kth element in the vector a ⊗ b. Thus,
        to ﬁnd the convolution of a and b, use the DFT algorithm to compute the
        product f (x)g(x), and extract the coeﬃcients of the resulting multiplica-
        tion. When n = m, the running time on the PRAM or the butterﬂy is
        Θ(log n) using O(n) processors.


        Example 5.5 Let a = [1, 2]T and b = [1, 3]T . Then,

        f (x)g(x) = (a0 b0 ) + (a0 b1 + a1 b0 )x + (a0 b2 + a1 b1 + a2 b0 )x2 = 1 + 5x + 6x2.

        Note that a2 = b2 = 0. It follows that c0 = a0 b0 = 1, c1 = a0 b1 + a1 b0 = 5,
        and c2 = a0 b2 + a1 b1 + a2 b0 = 6.                                         


        5.7     The Product of a Toeplitz Matrix and a Vectors

        A Toeplitz matrix T is deﬁned as an n × n matrix in which T [i, j] = T [i − 1,
        j − 1] for 2 ≤ i, j ≤ n. Equivalently, the elements in each diagonal are equal.
        The entries of T will be indexed as shown below
                         ⎛                                                            ⎞
                           tn−1        tn−2        ...        t2      t1        t0
                         ⎜ t                                                        ⎟
                         ⎜ n           tn−1        tn−2       ...     t2        t1  ⎟
                         ⎜                                                          ⎟
                         ⎜ tn+1         tn         tn−1      tn−2     ...       t2  ⎟
                         ⎜ .                                                        ⎟
                         ⎜ .             ..          ..        ..                .. ⎟
                         ⎜ .              .           .         .      ...        . ⎟
                         ⎜                                                          ⎟
                         ⎝t2n−3       t2n−4        ...        tn      tn−1     tn−2 ⎠
                          t2n−2       t2n−3        ...       tn+1      tn      tn−1

              A Toeplitz matrix can conveniently be represented by the vector t of
        2n − 1 entries appearing in the ﬁrst row and ﬁrst column. That is, t =
        [t0 , t1 , . . . , t2n−2 ]T .
May 7, 2022   11:14       Parallel Algorithms              9in x 6in         b4591-ch05           page 240




        240                                     Parallel Algorithms

        Example 5.6         Let
                                                  ⎛                     ⎞
                                               4           3    2      1
                                             ⎜5            4    3      2⎟
                                          T =⎜
                                             ⎝6
                                                                        ⎟
                                                           5    4      3⎠
                                               7           6    5      4

        Then, T is deﬁned by the vector t = [1, 2, 3, 4, 5, 6, 7]T .                         

            Let a = [a0 , a1 , . . . , an−1 ]T be a vector of n elements, and let T be a
        Toeplitz matrix. We are interested in computing the product b = T a. Using
        direct matrix by vector multiplication, the kth entry in b is given by
                                                  
                                                  n−1
                                         bk =           aj tn+k−j−1 .                     (5.5)
                                                  j=0

              Now, consider computing the convolution c of a and t given by
                                                       
                                                       i
                                                ci =         aj ti−j ,
                                                       j=0

              Substituting n + k − 1 for i yields
                                                      
                                                     n+k−1
                                     cn+k−1 =                  aj tn+k−j−1
                                                       j=0

                                                     
                                                     n−1
                                                 =         aj tn+k−j−1 ,                  (5.6)
                                                     j=0

        since aj = 0 for j > n − 1. Comparing the right hand sides of Eq. (5.5) with
        Eq. (5.6), we see that they are identical. Hence, bk = cn+k−1 .
            Following this, to compute the product T a, we compute a ⊗ t and set
        bk = cn+k−1 . This takes Θ(log n) time using O(n) processors on the PRAM
        and butterﬂy.

        Example 5.7         Consider computing he product T a, where
                               ⎛            ⎞              ⎛ ⎞
                                 4 3 2 1                      1
                               ⎜5 4 3 2 ⎟                  ⎜2⎟
                           T =⎜             ⎟
                               ⎝6 5 4 3⎠ and a = ⎝ 3 ⎠ .
                                                           ⎜ ⎟

                                 7 6 5 4                      4
May 7, 2022   11:14         Parallel Algorithms      9in x 6in           b4591-ch05          page 241




                                         Fast Fourier Transform                        241


        First, the vector t is determined to be [1, 2, 3, 4, 5, 6, 7]T . Computing the
        convolution a ⊗ t yields the vector c, which is equal to [1, 4, 10, 20, 30, 40,
        50, 52, 45, 28]T . Hence, b0 = c4+0−1 = c3 = 20, b1 = c4+1−1 = c4 = 30,
        b2 = c4+2−1 = c5 = 40, and b3 = c4+3−1 = c6 = 50. That is, T a =
        [20, 30, 40, 50]T , as can be veriﬁed by direct multiplication.              


        5.8      Using Modular Arithmetic

        In many applications, the aim is always to perform error-free computations
        of the fast Fourier transform. It turns out that this can be achieved by per-
        forming the FFT computations in modulo arithmetic. Let m be a positive
                           ∗
        integer. The set Zm   is the set of positive integers relatively prime to m. For
        example, Z9∗ = {1, 2, 4, 5, 7, 8}. It is a group under multiplication modulo m.
        An element α is a primitive root of unity for a group if it generates such
        a multiplicative group. For instance, α = 2 generates all elements of the
        multiplicative group Z9∗ under the operation of multiplication modulo 9.
        That is, 20 = 1, 21 = 2, 22 = 4, 23 = 8, 24 = 7, 25 = 5, where all powers are
        computed modulo 9. There are no primitive roots for Z8∗ = {1, 3, 5, 7}.
            Let n = 2j , α = 2k , l = n/2 and m = αl + 1 = 2kl + 1. Then, α is a
        primitive root of unity over the set of integers modulo m. It is not hard
        to see that the fast Fourier transform works correctly by replacing ω by α.
        Figure 5.4 illustrates the n roots of unity for n = 8 (mod 17) generated
        by the primitive root 2. As shown in the ﬁgure, the pairs αj and αj+n/2
        are symmetrically located with respect to the origin. Algebraically, we have
        αj+n/2 = −αj (Property 5.2), and in particular, αn/2 = −1. In this section,


                                                     4
                                              8                  2



                                        16                           1



                                             15                  9
                                                    13

              Fig. 5.4.   The 8 roots of unity mod 17 generated by the primitive root 2.
May 7, 2022   11:14        Parallel Algorithms              9in x 6in             b4591-ch05       page 242




        242                                       Parallel Algorithms

        all arithmetic will be done modulo m; we will simply write x + y to mean
        x + y (mod m).
            Using α as a primitive root of unity, the transformation matrix Fn
        looks like:
                           ⎛                                                              ⎞
                               1        1             1         ...           1
                           ⎜                          α2                    αn−1          ⎟
                           ⎜   1        α                       ...                       ⎟
                           ⎜            α2            α4                    α2n−2         ⎟
                           ⎜   1                                ...                       ⎟
                           ⎜                                                              ⎟
                           ⎜   ..        ..            ..        ..           ..          ⎟
                           ⎜    .         .             .         .            .          ⎟
                           ⎜                                                              ⎟
                           ⎝ 1      αn−2          α2(n−2)       . . . α(n−1)(n−2)         ⎠
                                                                               2
                             1      αn−1          α2(n−1)       ...     α(n−1)

        whose inverse is
                               ⎛                                                               ⎞
                               1               1             1          ...               1
                             ⎜1               α−1           α−2                    α−(n−1)     ⎟
                             ⎜                                          ...                    ⎟
                             ⎜1               α−2           α−4                    α−2(n−1)    ⎟
                             ⎜
                          −1 ⎜
                                                                        ...                    ⎟
                 Fn−1   =n ⎜.                                                                  ⎟
                                               ..            ..          ..           ..       ⎟
                             ⎜ ..               .             .           .            .       ⎟
                             ⎜                                                                 ⎟
                             ⎝1             α−(n−2)     α−2(n−2)        . . . α−(n−2)(n−1)     ⎠
                                                                                       2
                               1            α−(n−1)     α−2(n−1)        ...    α−(n−1)

           It is clear that Fn and Fn−1 are obtained from the usual FFT matrices
        by substituting α for ω.


        Example 5.8          Let n = 8, α = 2 and m = 17. Then,

                                    ⎛                                                     ⎞
                                        1     1     1       1    1      1     1       1
                                ⎜1 2 4 8 16 15 13 9 ⎟
                                ⎜                    ⎟
                                ⎜                    ⎟
                                ⎜1 4 16 13 1 4 16 13⎟
                                ⎜                    ⎟
                                ⎜1 8 13 2 16 9 4 15⎟
                                ⎜                    ⎟
                           F8 = ⎜                    ⎟,
                                ⎜1 16 1 16 1 16 1 16⎟
                                ⎜                    ⎟
                                ⎜1 15 4 9 16 2 13 8 ⎟
                                ⎜                    ⎟
                                ⎜                    ⎟
                                ⎝1 13 16 4 1 13 16 4 ⎠
                                        1     9     13 15 16            8     4       2
May 7, 2022   11:14     Parallel Algorithms       9in x 6in       b4591-ch05                page 243




                                     Fast Fourier Transform                           243

        and
                                ⎛                       ⎞
                                 15 15 15 15 15 15 15 15
                               ⎜15 16 8 4    2 1 9 13⎟
                               ⎜                        ⎟
                               ⎜                        ⎟
                               ⎜15 8 2 9 15 8 2 9 ⎟
                               ⎜                        ⎟
                               ⎜15 4 9 16 2 13 8 1 ⎟
                               ⎜                        ⎟
                       F8−1   =⎜                        ⎟,
                               ⎜15 2 15 2 15 2 15 2 ⎟
                               ⎜                        ⎟
                               ⎜15 1 8 13 2 16 9 4 ⎟
                               ⎜                        ⎟
                               ⎜                        ⎟
                               ⎝15 9 2 8 15 9 2 8 ⎠
                                    15 13     9   1      2    4   8    16
        The second row of F8 contains the powers of α = 2, the third contains the
        powers of α2 = 4, and so on. On the other hand, the second row of F8−1
        contains the powers of α−1 = 2−1 = 9 multiplied by 8−1 = 15 = −2 . For
        example, the second entry in the second row is α−1 8−1 = 9 × (−2) = −18 =
        −1 = 16. The third row contains the powers of α−2 = 2−2 = 13 multiplied
        by 8−1 = 15 = −2, and so on.                                            

        Example 5.9 Let f (x) = 1 + 2x and g(x) = 1 + 3x.                We will compute
        the product h(x) = f (x)g(x) using FFT modulo 17. Let            n = 4, α = 4 and
        m = 17. Then,
                   ⎛               ⎞                ⎛                            ⎞
                     1 1 1 1                          13 13                 13 13
                   ⎜1 4 16 13⎟                      ⎜13 16                   4 1⎟
              F4 = ⎜               ⎟
                   ⎝1 16 1 16⎠ and F4 = ⎝13 4
                                               −1   ⎜                            ⎟.
                                                                            13 4 ⎠
                     1 13 16 4                        13 1                   4 16

        Write f (x) = ax, where a = [1, 2], and x = [1, x]T , and g(x) = bx, where
        b = [1, 3]. Let a = [1, 2, 0, 0]T and b = [1, 3, 0, 0]T . Then,
                                     ⎛                 ⎞⎛ ⎞ ⎛ ⎞
                                        1 1 1 1              1          3
                                     ⎜                 ⎟  ⎜
                                        1 4 16 13⎟ ⎜ 2 ⎟ ⎜ 9 ⎟ ⎟     ⎜
                      c1 = F4 a = ⎜ ⎝1 16 1 16⎠ ⎝ 0 ⎠ = ⎝ 16 ⎠ .
                                                                          ⎟

                                        1 13 16 4            0         10
        Similarly,
                                      ⎛        ⎞⎛ ⎞ ⎛ ⎞
                                     1 1 1 1       1     4
                                   ⎜ 1 4  16 13⎟ ⎜ 3 ⎟ ⎜ 13 ⎟
                      c2 = F4 b = ⎜           ⎟⎜ ⎟ ⎜ ⎟
                                   ⎝1 16 1 16⎠ ⎝ 0 ⎠ = ⎝ 15 ⎠ .
                                     1 13 16 4     0     6
May 7, 2022   11:14          Parallel Algorithms          9in x 6in      b4591-ch05         page 244




        244                                        Parallel Algorithms

        Now, we compute c1                 c2 , which is the componentwise multiplication
        of c1 and c2 .
                                               ⎛ ⎞          ⎛    ⎞ ⎛ ⎞
                                              3               4      12
                                            ⎜ 9 ⎟           ⎜ 13 ⎟ ⎜ 15 ⎟
                                c1     c2 = ⎜    ⎟
                                            ⎝ 16 ⎠
                                                            ⎜ ⎟ = ⎜ ⎟.
                                                            ⎝ 15 ⎠ ⎝ 2 ⎠
                                              10                6        9

        Next, we interpolate.
                                                   ⎛       ⎞⎛ ⎞ ⎛ ⎞
                                                13 13 13 13  12      1
                                              ⎜13 16 4 1 ⎟ ⎜ 15 ⎟ ⎜ 5 ⎟
                      c = F4−1 (c1     c2 ) = ⎜            ⎟⎜ ⎟ ⎜ ⎟
                                              ⎝13 4 13 4 ⎠ ⎝ 2 ⎠ = ⎝ 6 ⎠ .
                                                13 1 4 16    9       0

        Hence, h(x) = 1 + 5x + 6x2 , as can be veriﬁed by direct multiplication. 


        5.9     Bibliographic Notes

        The fast Fourier transform is created by Cooley and Tukey [27]. See also
        Kronsjo [48] and Winograd [98]. Blahut [16] and McClellan [64] cover many
        fast Fourier transform algorithms for computing DFT and convolution. See,
        for example, Borodin and Moenck [17] and Fiduccia [36] for algorithms for
        polynomial evaluation and interpolation using the fast Fourier transform.
        For a good introduction to fast Fourier transform using modular arithmetic,
        see Lakshmivarahan and Dhall [52].


        5.10      Exercises

         5.1. Prove Property 5.1: For even n, if ω is an nth root of unity, then ω 2
              is an (n/2)th root of unity.

         5.2. Prove Property 5.2: For even n, ω k+n/2 = −ωk .

         5.3. Show that if ω is a primitive nth root of unity, then ω−1 is also a
              primitive nth root of unity.

         5.4. Let f (x) = 2 + x and g(x) = 3 + 2x. Compute the product h(x) =
              f (x)g(x) using fast Fourier transform.
May 7, 2022   11:14    Parallel Algorithms      9in x 6in    b4591-ch05               page 245




                                    Fast Fourier Transform                     245


         5.5. Let f1 (x), f2 (x) and f3 (x) be three polynomials of degree n − 1
              each. Apply DFT to ﬁnd their multiplication f1 (x)f2 (x)f3 (x) on
              the PRAM with O(n) processors. What is the running time of your
              algorithm?

         5.6. Generalize Exercise 5.5 to k ≥ 2 polynomials of degree n − 1 each.
              Your algorithm should run in time O(log kn) on the PRAM. How
              many processors are needed?

         5.7. Let f1 (x) = 1 + 2x, f2 (x) = 1 + 3x and f3 (x) = 1 + x. Apply Exer-
              cise 5.6 to compute the product g(x) = f1 (x)f2 (x)f3 (x) using fast
              Fourier transform.

         5.8. Give an eﬃcient algorithm to compute (1 + x)n . What is the running
              time of your algorithm? How many processors are required by your
              algorithm?

         5.9. Carry out the DFT algorithm to ﬁnd the convolution of the two
              vectors [2, 3]T and [4, 1]T .

        5.10. Is the sum of two Toeplitz matrices Toeplitz? Prove your answer.

        5.11. Is the product of two Toeplitz matrices Toeplitz? Prove your answer.

        5.12. How quickly can you multiply two Toeplitz matrices A and B?
              Explain.

        5.13. Let n = 4, α = 2 and m = 5 in the speciﬁcation of FFT in modular
              arithmetic. Compute F4 and F4−1 .

        5.14. Use your answer to Exercise 5.13 to ﬁnd the product f (x)g(x), where
              f (x) = 2 + x and g(x) = 3 + 2x in modular arithmetic.

        5.15. Evaluate f (x) = (1 + x + x2 )2 in modular arithmetic. You may use
              F8 and F8−1 in Example 5.8. Note that α = 2 and m = 17.

        5.16. Let a = [2, 1]T and b = [4, 3]T . Use your answer to Exercise 5.13 to
              ﬁnd the convolution of a and b in modular arithmetic.

        5.17. Is it possible to have n = 6, α = 2 and m = 9 in the speciﬁcation of
              FFT in modular arithmetic? Explain.
May 7, 2022   11:14        Parallel Algorithms          9in x 6in          b4591-ch05           page 246




        246                                      Parallel Algorithms


        5.18. What are the primitive roots of unity of Z5∗ = {1, 2, 3, 4}?
                                                                   ∗
        5.19. How many primitive roots (generators) are there for Zm ?


        5.11      Solutions

         5.1. Prove Property 5.1: For even n, if ω is an nth root of unity, then ω 2
              is an (n/2)th root of unity.
                 (w2 )k = (wk )2 . That is, the powers of ω2 are

                                         ω2 , ω 4 , . . . , ω2(n/2−1) , ω2(n/2) .

                 Moreover, (ω 2 )n/2 = ω n = 1, and (ω 2 )j = ω2j = 1 for 0 < j < n/2.

         5.2. Prove Property 5.2: For even n, ω k+n/2 = −ωk .

                                 ωk+n/2 = ω k × ω n/2 = ωk × (−1) = −ωk .

         5.3. Show that if ω is a primitive nth root of unity, then ω−1 is also a
              primitive nth root of unity.
                 The n powers of ω−1 are ω −1 , (ω −1 )2 , (ω −1 )3 , . . . , (ω −1 )n , or
                 ω−1 , ω−2 , ω −3 , . . . , ω−n . Multiplying by ω n yields the sequence
                 ωn−1 , ωn−2 , ω n−3 , . . . , ω0 . These are precisely the n powers of ω. It
                 follows that ω −1 is a primitive nth roots of unity.

         5.4. Let f (x) = 2 + x and g(x) = 3 + 2x. Compute the product h(x) =
              f (x)g(x) using fast Fourier transform.
                 Similar to Example 5.4.

         5.5. Let f1 (x), f2 (x) and f3 (x) be three polynomials of degree n − 1
              each. Apply DFT to ﬁnd their multiplication f1 (x)f2 (x)f3 (x) on
              the PRAM with O(n) processors. What is the running time of your
              algorithm?
                 First, note that the degree of the product is 3n − 3. Let m be the
                 least power of 2 greater than or equal to 3n − 2. Let a1 , a2 and a3
                 be the vectors of coeﬃcients of f1 (x), f2 (x) and f3 (x), respectively.
                 The steps for the construction of the product g(x) = f1 (x)f2 (x)f3 (x)
                 are shown in Algorithm polynomialmultip1. Steps 1 and 3 take
May 7, 2022    11:14          Parallel Algorithms        9in x 6in         b4591-ch05                      page 247




                                           Fast Fourier Transform                                    247


          Algorithm 5.1 polynomialmultip1
          Input: Three polynomials f1 (x), f2 (x) and f3 (x).
          Output: The product g(x) = f1 (x)f2 (x)f3 (x).
              1. Compute d1 = Fm a1 , d2 = Fm a2 , and d3 = Fm a3 , where a1 , a2 and a3
                 are a1 , a2 and a3 padded with 0s to length m.
              2. Perform the componentwise product c = d1  d2  d3 .
              3. Interpolate by computing the inverse Fourier transform
                       −1
                 e = Fm    c.
              4. Output e = [e0 , e1 , e2 , . . . , em−1 ]T ; e is the vector of coeﬃcients of the
                 product g(x).



                  Θ(log n) parallel time on the PRAM using O(n) processors. Step 2
                  takes Θ(1) parallel time. Hence, the algorithm for computing the
                  product of three polynomials runs in Θ(log n) parallel time on the
                  PRAM with O(n) processors.

         5.6. Generalize Exercise 5.5 to k ≥ 2 polynomials of degree n − 1 each.
              Your algorithm should run in time O(log kn) on the PRAM. How
              many processors are needed?
                  First, note that the degree of the product is kn − k. Let m be the
                  least power of 2 greater than or equal to kn− k + 1. Let a1 , a2 , . . . , ak
                  be the vectors of the coeﬃcients of the k polynomials. The idea
                  is to evaluate the polynomials at m points, multiply them compo-
                  nentwise, and then interpolate by applying the inverse DFT. The
                  steps for the construction of the product g(x) = f1 (x)f2 (x) . . . fk (x)
                  are shown in Algorithm polynomialmultip2. Steps 1 and 3 take

          Algorithm 5.2 polynomialmultip2
          Input: k ≥ 2 polynomials f1 (x), f2 (x), . . . , fk (x) of degree n − 1.
          Output: The product g(x) = f1 (x)f2 (x) . . . fk (x).
              1. Compute d1 = Fm a1 , d2 = Fm a2 , . . . , dk = Fm ak , where a1 , a2 , . . . , ak
                 are a1 , a2 , . . . , ak padded with 0s to length m.
              2. Perform the componentwise product c = d1  d2  . . .  dk .
              3. Interpolate by computing the inverse Fourier transform
                       −1
                 e = Fm    c.
              4. Output e = [e0 , e1 , e2 , . . . , em−1 ]T ; e is the vector of coeﬃcients of the
                 product g(x).
May 7, 2022   11:14       Parallel Algorithms          9in x 6in        b4591-ch05             page 248




        248                                     Parallel Algorithms


                 O(log kn) parallel time on the PRAM using O(kn) processors, since
                 there are O(kn) coeﬃcients in c. Computing the componentwise
                 product in Step 2 can be done recursively in Θ(log n) time using
                 O(kn) processors. It follows that the running time of the algorithm
                 is O(log n + log kn) = O(log kn) on the PRAM with O(kn) proces-
                 sors.

         5.7. Let f1 (x) = 1 + 2x, f2 (x) = 1 + 3x and f3 (x) = 1 + x. Apply Exer-
              cise 5.6 to compute the product g(x) = f1 (x)f2 (x)f3 (x) using fast
              Fourier transform.

                 Write f1 (x) = a1 x, f2 (x) = a2 x and f3 (x) = a3 x, where a1 = [1, 2],
                 a2 = [1, 3], a3 = [1, 1] and x = [1, x]T . Let a1 = [1, 2, 0, 0]T , a2 =
                 [1, 3, 0, 0]T and a3 = [1, 1, 0, 0]T . Then,
                                           ⎡       ⎤⎡ ⎤ ⎡          ⎤
                                         1 1   1 1    1       3
                                       ⎢1 i −1 −i ⎥ ⎢ 2 ⎥ ⎢ 1 + 2i ⎥
                         c1 = F4 a1 = ⎢           ⎥⎢ ⎥ ⎢
                                       ⎣1 −1 1 −1⎦ ⎣ 0 ⎦ = ⎣ −1 ⎦ .
                                                                   ⎥

                                         1 −i −1 i    0     1 − 2i

                 Similarly,
                                           ⎡       ⎤⎡ ⎤ ⎡          ⎤
                                         1 1   1 1    1       4
                                       ⎢1 i −1 −i ⎥ ⎢ 3 ⎥ ⎢ 1 + 3i ⎥
                         c2 = F4 a2 = ⎢           ⎥⎢ ⎥ ⎢
                                       ⎣1 −1 1 −1⎦ ⎣ 0 ⎦ = ⎣ −2 ⎦ ,
                                                                   ⎥

                                         1 −i −1 i    0     1 − 3i

                 and
                                        ⎡                   ⎤⎡ ⎤ ⎡       ⎤
                                         1         1  1  1     1      2
                                        ⎢1          i −1 −i ⎥ ⎢ ⎥ ⎢      ⎥
                          c3 = F4 a3 = ⎢                   ⎥⎢1⎥ = ⎢1 + i⎥.
                                        ⎣1         −1 1 −1⎦ ⎣ 0 ⎦ ⎣ 0 ⎦
                                         1         −i −1 i     0    1−i

                 Now, we compute c = c1 c2                   c3 , which is the componentwise
                 multiplication of c1 , c2 and c3 .
                                ⎡     ⎤           ⎡        ⎤       ⎡       ⎤ ⎡     ⎤
                                 3                    4                2       24
                             ⎢ 1 + 2i ⎥           ⎢ 1 + 3i ⎥       ⎢ 1 + i ⎥ ⎢ −10 ⎥
                           c=⎢
                             ⎣ −1 ⎦
                                      ⎥           ⎢
                                                  ⎣ −2 ⎦
                                                           ⎥       ⎢       ⎥ ⎢
                                                                   ⎣ 0 ⎦ = ⎣ 0 ⎦.
                                                                                   ⎥

                               1 − 2i               1 − 3i           1−i       −10
May 7, 2022   11:14        Parallel Algorithms       9in x 6in    b4591-ch05               page 249




                                         Fast Fourier Transform                     249

                 Finally, we interpolate:

                                     ⎡          ⎤⎡      ⎤   ⎡ ⎤ ⎡ ⎤
                                      1 1  1 1       24       4        1
                                    ⎢           ⎥ ⎢
                                  1 1 −i −1 i ⎥ ⎢ −10 ⎥ 1 ⎢ 24 ⎥ ⎢ 6 ⎥
                                                        ⎥   ⎢    ⎥   ⎢
                      e = F4−1 c = ⎢                      =        =     ⎥.
                                  4 ⎣1 −1 1 −1⎦ ⎣ 0 ⎦ 4 ⎣ 44 ⎦ ⎣ 11 ⎦
                                      1 i −1 −i     −10       24       6


                 Hence, g(x) = 1 + 6x + 11x2 + 6x3 , and we can verify this by direct
                 multiplication.

         5.8. Give an eﬃcient algorithm to compute (1 + x)n . What is the running
              time of your algorithm? How many processors are required by your
              algorithm?

                 We use the fast Fourier transform. This is similar to Exercise 5.6
                 with k replaced by n, and n replaced by 2. The highest degree in
                 the product is n, so let m be the least power of 2 greater than or
                 equal to n + 1. Let a = [1, 1, 0, 0 . . . , 0] (m − 2 0s). Compute the
                 componentwise product c = a a . . . , a (n times). This is equal
                 to [an0 +an1 +. . .+anm−1 ]T . These powers can be computed in Θ(log n)
                 time by assigning each number to one processor, which raises that
                 number to the nth power in sequential Θ(log n) time. Thus, this step
                 can be done in parallel in Θ(log n) time. Finally, apply the inverse
                 DFT on c to obtain the ﬁnal result. Since there are O(n) coeﬃcients
                 in c, applying the inverse DFT requires Θ(log n) time. It follows that
                 the running time of the algorithm is Θ(log n) on the PRAM using
                 O(n) processors.

         5.9. Carry out the DFT algorithm to ﬁnd the convolution of the two
              vectors [2, 3]T and [4, 1]T .

                 Similar to Example 5.5.

        5.10. Is the sum of two Toeplitz matrices Toeplitz? Prove your answer.

                 Yes. Let A + B = C, where A and B are Toeplitz. Then, ci,j = ai,j +
                 bi,j = ai−1,j−1 + bi−1,j−1 = ci−1,j−1 . It follows that C is Toeplitz.
May 7, 2022   11:14        Parallel Algorithms          9in x 6in          b4591-ch05         page 250




        250                                      Parallel Algorithms

        5.11. Is the product of two Toeplitz matrices Toeplitz? Prove your answer.


                 No. Let
                                                      ⎛           ⎞
                                                       1      1 1
                                                  A = ⎝0      1 1⎠ .
                                                       1      0 1
                 Then,
                                                        ⎛            ⎞
                                                        2 2         3
                                                  A2 = ⎝1 1         2⎠ ,
                                                        2 1         2
                 which is not Toeplitz.

        5.12. How quickly can you multiply two Toeplitz matrices A and B?
              Explain.
                 Treat B as a sequence of vectors, and apply the convolution method
                 individually to multiply A by each vector. This results in time com-
                 plexity n×Θ(log n) = Θ(n log n) parallel time using O(n) processors.
                 If the number of processors is O(n2 ), then the running time reduces
                 to Θ(log n), as all matrix by vector multiplications can be carried
                 out in parallel using the convolution method.

        5.13. Let n = 4, α = 2 and m              = 5 in the speciﬁcation of FFT in modular
              arithmetic. Compute F4              and F4−1 .
                           ⎛                        ⎞                ⎛            ⎞
                             1 1 1                1                     4 4 4 4
                           ⎜1 2 4                 3⎟                 ⎜            ⎟
                     F4 = ⎜                         ⎟ and F −1 = ⎜4 2 1 3⎟ .
                           ⎝1 4 1                 4⎠           4     ⎝4 1 4 1 ⎠
                             1 3 4                2                     4 3 1 2

        5.14. Use your answer to Exercise 5.13 to ﬁnd the product f (x)g(x), where
              f (x) = 2 + x and g(x) = 3 + 2x in modular arithmetic.
                 Similar to Example 5.9.

        5.15. Evaluate f (x) = (1 + x + x2 )2 in modular arithmetic. You may use
              F8 and F8−1 in Example 5.8. Note that α = 2 and m = 17.
May 7, 2022   11:14       Parallel Algorithms      9in x 6in    b4591-ch05               page 251




                                       Fast Fourier Transform                     251

                 Similar to Example 5.9.

        5.16. Let a = [2, 1]T and b = [4, 3]T . Use your answer to Exercise 5.13 to
              ﬁnd the convolution of a and b in modular arithmetic.
                 Similar to Example 5.5

        5.17. Is it possible to have n = 6, α = 2 and m = 9 in the speciﬁcation of
              FFT in modular arithmetic? Explain.
                 No, it is impossible since 6 is not invertible modulo 9; 6 and 9 are
                 not relatively prime. 6−1 is needed to compute the inverse.

        5.18. What are the primitive roots of unity of Z5∗ = {1, 2, 3, 4}?
                 There are two of them: 2 and 3.
                                                                   ∗
        5.19. How many primitive roots (generators) are there for Zm ?
                 If there is one generator, then there are φ(φ(m)) generators, where
                 φ(k) is the number of elements less than k and relatively prime to k.
                 For example, for m = 5, there are φ(φ(5)) = φ(4) = 2 generators.
                 Note that these generators generate all elements in the group.
                             B1948   Governing Asia




                      This page intentionally left blank




B1948_1-Aoki.indd 6                                        9/22/2014 4:24:57 PM
May 7, 2022   11:14      Parallel Algorithms       9in x 6in    b4591-ch06                    page 253




                                               Chapter 6


                          Tree-based Networks



        6.1     The Tree Network

        A tree of size n = 2h is an interconnection network constructed from a
        complete binary tree with n processors in the base level P1 , P2 , . . . , Pn , and
        a total of 2n − 1 = 2h+1 − 1 processors. Here h = log n is the height of the
        tree. Each tree has h + 1 levels: 0, 1, . . . , h. The leaf nodes at level h are
        connected by two-way communication links to their parents only, and the
        root is connected to its two children. Every other processor is connected by
        two-way communication links to its parent and its two children. Therefore,
        the tree has degree 3. See Fig. 6.1 for an eight-leaf tree.
            The communication diameter of a tree of size n is only Θ(log n), which
        is very low compared to a linear array of the same size. This is true since
        any two processors in the tree can communicate in O(log n) time. However,
        it may require as much as 2 log n = Ω(log n) time for communication that
        requires an exchange of information between two arbitrary processors. This
        makes the tree ideal for computing problems like semigroup operations, e.g.,
        summation and ﬁnding the maximum, which require O(log n) time. How-
        ever, for problems that demand extensive data movement such as sorting
        and routing data in the base, Ω(n) time may be required, since only Θ(1)
        wires cross the middle of the tree, which means that the bisection width of
        the tree is Θ(1) — and that is very low. For instance, it may be required to
        move data from the n/2 left processors to the n/2 right processors, which
        requires Ω(n) time, since the root serves as a bottleneck.



                                                  253
May 7, 2022   11:14        Parallel Algorithms          9in x 6in         b4591-ch06             page 254




        254                                      Parallel Algorithms




                                        Fig. 6.1.     A tree of size 8.


           An algorithm that runs on the tree is called normal tree algorithm if no
        two processors at diﬀerent levels are active at the same time. That is, at
        any given time, only processors in the same level are participating in the
        computation. A single step of a normal tree algorithm can be simulated in
        one step of the hypercube, given the embedding shown in Fig. 3.12.


        6.1.1         Semigroup operations
        Due to its low communication diameter, the tree is ideal for semigroup
        operations, e.g., addition and ﬁnding the maximum. These operations can
        be performed in Θ(log n) time as follows. Assume that n pieces of data are
        distributed one per base processor. Then, in order to compute a semigroup
        operation ◦ over this set of data, it can be applied to disjoint pairs of partial
        results in parallel as data moves up the tree level by level. After Θ(log n)
        steps, the ﬁnal result will be known to the root processor. Naturally, if all
        processors need to know the ﬁnal result, it can be broadcast from the root
        to all other processors in Θ(log n) time. This means a cost of Θ(n log n) on
        a tree with n base processors, which is a factor of Θ(log n) from optimal.
        Thus, the tree provides a major beneﬁt over the linear array and the mesh
        in terms of combining information.


        6.1.2         Sorting by minimum extraction
        Assume that a tree with n leaves is available for sorting the sequence
        x1 , x2 , . . . , xn  of distinct integers. The n integers are initially loaded into
        the leaf processors. Now, each internal processor determines the smaller
May 7, 2022   11:14         Parallel Algorithms        9in x 6in   b4591-ch06            page 255




                                            Tree-based Networks                   255

        of the two integers held by its children and routes it to its parent. After
        log n + 1 steps, the minimum element exits the machine from the root, and
        is placed in a memory buﬀer for storing the output. If the process is con-
        tinued, the next element in increasing order is obtained at every other step.
        Thus, as mentioned above, the ﬁrst element requires log n + 1 steps to exit
        the root. Each one of the remaining n − 1 elements requires two steps to be
        produced. It follows that a constant multiple of 2n + log n − 1 time units are
        needed to produce the sorted sequence. Hence, the running time is Θ(n),
        and since there are 2n − 1 processors, the cost is Θ(n2 ).


        6.1.3         Sorting by partitioning
        Assume that a tree with k = log n leaves is available for sorting a sequence
        of n = 2k numbers. Each processor at level j, 0 ≤ j ≤ log k, can store n/2j
        elements and can execute a median ﬁnding and sorting algorithm. The n
        numbers are initially loaded into the root processor. First, the root ﬁnds
        the median and splits the sequence into two halves, where the half with
        numbers less than or equal to the median is passed to its left child, and
        the other half with numbers greater than the median is passed to its right
        child. Upon receiving its half, each child ﬁnds the median of its subsequence
        and passes those elements less than or equal to the median to its left child
        and passes those elements larger than the median to its right child. This
        process of ﬁnding the median, partitioning and passing elements continues
        until the leaf nodes are reached. Finally, each leaf node sorts its n/ log n
        elements and places them in the output buﬀer. The algorithm is shown as
        Algorithm treesort
            The running time of the algorithm is computed as follows. Finding the
        median and splitting the sequence at level j takes Θ(n/2j ). The sorting step
        takes Θ((n/ log n) log(n/ log n)) = Θ(n) time. The time needed to ﬁnd the
        median and output the sequences at level j is Θ(n/2j ). Hence, the overall
        running time of the algorithm is expressed as

                                      
                                      k
                            Θ(n) +          Θ(n/2j ) = Θ(n) + Θ(n) = Θ(n).
                                      j=0


        The total cost of the algorithm is log n × Θ(n) = Θ(n log n), which is
        optimal.
May 7, 2022    11:14         Parallel Algorithms          9in x 6in      b4591-ch06        page 256




        256                                        Parallel Algorithms


          Algorithm 6.1 treesort
          Input: A sequence of n numbers, where n = 2k .
          Output: The input sorted in ascending order.
              1. for j ← 0 to log k − 1
              2.     for all processors P at level j
              3.         Processor P ﬁnds the median m and routes all elements ≤ m
                         to the left child and all elements > m to the right child.
              4.     end for
              5. for all leaf processors P
              6.     Processor P sorts the currently held elements and places
                     them in the output buﬀer.
              7. end for




        6.1.4          Selection
        Recall the problem of selection discussed in Section 2.14: Given a
        sequence A = a1 , a2 , . . . , an  of n elements and a positive integer k, 1 ≤
        k ≤ n, ﬁnd the kth smallest element in A. In this section we consider the
        problem of ﬁnding the k’th smallest element in a sequence of n elements
        stored at the leaves of an n-leaf tree of height h, where n = 2h and h = 2m .
        A straightforward solution would be to sort A and return the kth small-
        est element. However, sorting on the tree is expensive, and takes a lot of
        time. The easiest selection problem is k = 1, which amounts to ﬁnding the
        minimum in Θ(log n) time. We observe that if we adopt a modiﬁcation of
        the sorting method of minimum extraction outlined above, then the kth
        smallest element can be found in Θ(log n + k) time, which in the worst case
        is Θ(n), e.g., ﬁnding the median.
            We will simplify discussion by assuming that all the elements are dis-
        tinct. The algorithm is given as Algorithm treeselect (see Fig. 6.2). Ini-
        tially, each item is “active”, and may later become “inactive” when it is
        known that it cannot be the answer.
            Steps 3 to 13 are repeated until the kth smallest element is found. In
                         √                                        √
        each iteration, n recursive calls are executed on n elements each to
                                                              √
        ﬁnd the median of each group. Thus, there are n parallel simultaneous
        calls plus one call to ﬁnd the median of medians med. Let T (n) denote
                                                                √
        the total running time. Then, these calls take 2T ( n). In each iteration,
        at least 1/4 of the elements will be deactivated, and hence the number of
        iterations is at most log4/3 n = ch, where c = 1/ log(4/3) (see Section 2.14
May 7, 2022   11:14     Parallel Algorithms              9in x 6in        b4591-ch06           page 257




                                       Tree-based Networks                               257


                                                   med




                          m1                  m n               mj               m
                                                                                     n
                                                   2




                         T1                   Tn                     Tj          Tn
                                               2

                          n
                                                   n subtrees

                       Fig. 6.2.    Illustration of Algorithm treeselect.


          Algorithm 6.2 treeselect
          Input: A sequence of n numbers, where n = 2h and an integer k, 1 ≤ k ≤ n.
          Output: The kth smallest element in the sequence.
           1. if n ≤ 2 then return the answer.
           2. else repeat Steps 3 to 13
           3. Each processor at level log n/2 computes the median of the active items
              beneath it. It stores this median as its value.
           4. The root computes recursively the median of medians of the items found
              in the previous step, call this med.
           5. The root transmits med to all processors in the base.
           6. Each base processor sends up a 1 if its item is less than or equal to med.
              These 1’s are summed on their way up to the root. Let s be the sum of
              these 1’s.
           7. if k = s then return med
           8. else if k < s then
           9.     Deactivate all items in the base ≥ med
          10. else
          11.     Deactivate all items in the base ≤ med
          12.     Set k ← k − s
          13. end if




        and Exercise 2.17). Broadcasting and summing the 1’s takes O(log n). To see
        how much time the algorithm takes, it is easiest to work with the height of
        the tree (i.e., h = log n) instead of its width. The running time can therefore
May 7, 2022   11:14             Parallel Algorithms          9in x 6in       b4591-ch06       page 258




        258                                           Parallel Algorithms

        be expressed by the recurrence:
                                             
                                              1                             if h = 1
                                 T (h) =
                                              ch × (2T (h/2) + bh)          if h > 1,

       for some constant b. We proceed to solve this recurrence as follows. Rewrite
       the recurrence as
                              
                                1                         if m = 0
                      f (m) =
                                c2m 2f (m − 1) + cb22m if m > 0,

        since h = 2m . Expanding this recurrence yields

               f (m) = c2m+1 f (m − 1) + cb22m
                   ..
                    .
                           = c5 25m−5 f (m − 5) + cb22m
                                                                             
                             × c4 24m−10 + c3 b23m−6 + c2 b22m−3 + cb2m−1 + 1
                      ..
                       .
                                                              
                           = cj 2jm−j(j−3)/2 f (m − j) + cb22m cj 2jm−j(j+1)/2
                                                                                          
                             + · · · + c4 24m−10 + c3 23m−6 + c2 22m−3 + c2m−1 + 1
                      ..
                       .
                                      2
                                          −m(m−3)/2
                                                             
                           = cm 2 m             f (0) + cb22m cm−1 2(m−1)m−(m−1)(m)/2
                                                                                   
                             + · · · + c4 24m−10 + c3 23m−6 + c2 22m−3 + c2m−1 + 1
                                                            
                                                            m−1
                           = cm 2m(m+3)/2 + cb22m                 cj 2jm−j(j+1)/2
                                                            j=0

                           ≤c hm (m+3)/2
                                              + cb2   2m
                                                           × m × cm−1 2(m−1)m−((m−1)(m)/2)
                           = cm h(m+3)/2 + b22m × m × cm 2m(m−1)/2
                           = cm h(m+3)/2 + b × m × cm 2m(m+3)/2
                           = cm h(m+3)/2 + bmcm h(m+3)/2
                           = O(mcm h(m+3)/2 )
                           = O(log log n clog log n (log n)(log log n+3)/2 ).

        Hence, T (n) = o(n ) for any  > 0.
May 7, 2022    11:14            Parallel Algorithms        9in x 6in       b4591-ch06                      page 259




                                                Tree-based Networks                                  259


               (a)                      k=9                  (b)             5         k = 9; s= 5



                                                              4        7           5           9




              4 17 14 3 12 6 15 7 13 5 11 2 1 10 9 16 4 17 14 3 12 6 15 7 13 5 11 2 1 10 9 16


               (c)                 10        k = 4; s= 4


                                                                       activated
               14           7           11            10
                                                                       deactivated


              4 17 14 3 12 6 15 7 13 5 11 2 1 10 9 16

                       Fig. 6.3.    Example 6.1 for the selection algorithm on the tree.

        Example 6.1 Figure 6.3 illustrates the operation of Algorithm treese-
        lect. In this example, we use the algorithm to ﬁnd the 9th smallest element
        of the 16 items: 4, 17, 14, 3, 12, 6, 15, 7,13, 5, 11, 2, 1, 10, 9, 16. Fig. 6.3(a)
        shows the initial input, which is entered at the leaves. In part (b) of the
                √
        ﬁgure, n = 4 calls are executed in parallel, and then one call with the
        medians resulting from these 4 calls as the input. This results in 5 being
        the median of medians. After broadcasting 5, processors with elements 1,
        2, 3, 4 and 5 send a 1 each to the root for a total of s = 5, which is shown
        in Fig. 6.3(b). The deactivated processors are shown as white. More 5 calls
        are executed in Fig. 6.3(c), after which the 9th smallest element, 10, is
        found.                                                                           

        6.1.5          The one-dimensional pyramid
        A one-dimensional pyramid, or simply a 1-pyramid, of size n is an inter-
        connection network obtained from the tree of processors with n leaves by
        adding two-way communication links between adjacent processors at the
        same level. Thus, it forms a linear array at each level. See Fig. 6.4 for an
        eight-leaf 1-pyramid. Like the tree, communication diameter of a 1-pyramid
        of size n is only Θ(log n).
May 7, 2022   11:14          Parallel Algorithms          9in x 6in      b4591-ch06          page 260




        260                                        Parallel Algorithms




                      Fig. 6.4.   A tree with horizontal links (1-pyramid) of size 8.


        6.2     The Pyramid

        A two-dimensional pyramid or simply a pyramid of size n = 4d is an
        interconnection network that can be viewed as a full 4-ary tree of height
        log4 n (see Fig. 6.5). It has log4 n + 1 levels numbered 0, 1, . . . , log4 n. For
        simplicity, we will assume that the base is at level 0, and the root is at
        level log4 n. The base consists of n processors arranged in the form of a
        √      √
          n × n mesh of processors. In general, level k consists of a mesh of n/4k
        processors. In particular, level log4 n consists of one processor referred to
        as the apex. A pyramid of size n has a total of (4n − 1)/3 processors. Each
        processor at level k is connected via bidirectional communication links to
        its nine neighbors (if they exist): four siblings at level k, four children at
        level k − 1 and a parent at level k + 1.
            The pyramid can be projected into a regular pattern in the plane, which
        makes it ideal for VLSI implementation and provides the possibility of con-
        structing pyramids with thousands or millions of processors (see Fig. 6.6).
        A pyramid may be regarded as a combination of a mesh and a tree machine
        architecture.
            One advantage of the pyramid over the mesh is that the communication
        diameter of the pyramid of size n is only Θ(log n). This is true since any two
        processors in the pyramid can communicate through the apex in O(log n)
        time. However, it may require Ω(log n) time for communication that require
        exchange of information between two arbitrary processors. This makes the
        pyramid suitable for problems like semigroup operations, e.g., summation
        and ﬁnding the maximum, which require O(log n) time. However, for prob-
        lems that demand extensive data movement such as sorting and routing all
                             √
        data in the base, Ω( n) time is required (Exercise 6.8).
May 7, 2022   11:14   Parallel Algorithms        9in x 6in     b4591-ch06                   page 261




                                     Tree-based Networks                              261


                                                                            Level 2




                                                                            Level 1




                                                                            Level 0




                               Fig. 6.5.    A pyramid of size 16.




                               Fig. 6.6.    A pyramid of size 16.
May 7, 2022   11:14          Parallel Algorithms          9in x 6in             b4591-ch06             page 262




        262                                        Parallel Algorithms



                        1        4            5      6



                         2       3            8     7             1         2

                                                                                               1

                        15      14            9     10           4          3
                                                                                             Level 2
                                                                      Level 1
                        16     13         12        11


                                    Level 0

        Fig. 6.7.      Proximity indexing scheme for the three levels of a pyramid of size 16.


        6.2.1         Computing parallel prefix on the pyramid
        The parallel preﬁx problem was deﬁned in Section 2.5. In this section, we
        show how to compute it on the pyramid. For simplicity, we will assume
        addition as the binary operation. We will also assume the proximity index-
        ing scheme shown in Fig. 6.7. In this ordering scheme, consecutive elements
        are physically contiguous. Assume that each processor has four registers:
        R1 , R2 , R3 and R4 .
            Initially, the items x1 , x2 , . . . , xn are input to the n = 4d processors at
        level 0. The algorithm consists of two passes: Bottom-up and top-down. It
        is given as Algorithm pyramidparprefix.


        Example 6.2 Consider Fig. 6.8(a) in which is shown the base of a
        2-dimensional pyramid with 16 numbers stored in it. In this ﬁgure, pro-
        cessors are shown by squares (of varying sizes) and registers are shown by
        circles. Initially, each processor P0,j of the base sends its value xj to its
        parent. Each processor of level 1 computes sequentially the four preﬁxes of
        the four values received from its children in level 0, which are then stored
        orderly in its four data registers R1 , R2 , R3 and R4 , as shown in Fig. 6.8(b).
        Notice that there are four processors in this part of the ﬁgure. Each pro-
        cessor of level 1 then sends to its parent in level 2 (the apex) the fourth
        preﬁxes R4 , as shown in Fig. 6.8(c). The apex then computes the preﬁx
        sums of these values as shown in Fig. 6.8(d). In part (e) of the ﬁgure, it
        shifts these preﬁxes by one register so that Ri is stored in register Ri+1 ,
        1 ≤ i ≤ 3, and puts 0 in R1 . It then copies the contents of the four registers
May 7, 2022   11:14       Parallel Algorithms       9in x 6in       b4591-ch06                    page 263




                                         Tree-based Networks                                263


          Algorithm 6.3 pyramidparprefix
          Input: X = x1 , x2 , . . . , xn , a sequences of n numbers, where n = 4d stored
                 at the base of the pyramid.
          Output: S = s1 , s2 , . . . , sn , the preﬁx sums of X.
               (a) Bottom-up phase.
                 (1) Each processor P0,j of the base sends its value xj to its parent.
                 (2) for k = 1, 2, . . . , log4 n do
                     Each processor Pk,j of level k computes sequentially the four preﬁxes
                     of the four values received from its children, which are then stored
                     orderly in the four data registers R1 , R2 , R3 and R4 , then sends to
                     its parent the fourth preﬁx R4 . As k = log4 n, the apex contains in
                     its four registers the four preﬁxes sn/4 , sn/2 , s3n/4 , sn .
               (b) Top-down phase.
                 (1) The apex moves the contents of register Ri into register Ri+1 , 1 ≤
                     i ≤ 3, and puts 0 into R1 . Then, it sends these values orderly to its
                     four children (i.e., R1 goes to the ﬁrst child, R2 to the second, etc.)
                 (2) for k = log4 n − 1, . . . , 1 do
                     Each processor Pk,j adds sequentially the value received from its
                     parent to the values stored in its four data registers. Then, each
                     processor Pk,j moves the contents of its register Ri , 1 ≤ i ≤ 3 into
                     its register Ri+1 , and moves the contents of its register R4 into the
                     register R1 of processor Pk,j+1 (if it exists, processor Pk,1 puts 0 into
                     its register R1 ). Finally, Pk,j sends the values stored in R1 , R2 , R3
                     and R4 orderly to its four children.
                 (3) Each processor P0,j at the base adds the value received from the
                     parent to its xj value. Now, each processor P0,j at the base contains
                     the partial sum sj .



        into the four processors in level 1, as shown in Fig. 6.8(f). Each processor
        P1,j then adds sequentially the value received from its parent to the values
        stored in its four data registers, as shown in Fig. 6.8(g). Then, each proces-
        sor P1,j in level 1 moves the contents of its register Ri , 1 ≤ i ≤ 3, into its
        register Ri+1 , and moves the contents of its register R4 into the register R1
        of processor P1,j+1 (if it exists, processor P1,1 puts 0 into its register R1 ).
        This is shown in Fig. 6.8(h). Next, P1,j sends the values stored in R1 , R2 , R3
        and R4 orderly to its four children in level 0. Finally, each processor P0,j
        at the base adds the value received from its parent to its xj value. Now,
        each processor P0,j at the base contains the partial sum sj . This is shown
        in Fig. 6.8(i).                                                                
May 7, 2022   11:14              Parallel Algorithms              9in x 6in                   b4591-ch06          page 264




        264                                                Parallel Algorithms


                                           Bottom-up                                          Top-down
                                                                                         (e)
                                     (d)                                                           0 10
                                               10 24                                            36 24
                                               45 36                            (f)

                                                                                               0      10

                                                                                               36     24

                                     (c)                                       (g)
                                               10 14                                      2 10 15 21
                                               9 12                                       6        9 24 23
                                                                                         40 38 27 31
                            (b)                                                          45 37 36 34
                                      2 10         5 11
                                                                               (h)
                                      6    9 14 13                                        0     9    10 15
                                      4    2       3   7                                  2        6 23 21
                                      9    1       12 10                                 38 37 24 27
                                                                                         40 36 34 31

                      (a)                                                (i)
                             2        1        5        6                        2             10     15     21

                             4        3        1       2                             6         9      24     23

                            2        1         3       4                         40            38     27     31

                            5        1         2       3                         45            37    36      34



                                                   : Processor                            : Register


                  Fig. 6.8.        Example of computing parallel preﬁx on the pyramid.



        6.3     Mesh of Trees

        A mesh of trees of size n, where we assume for simplicity that n is a perfect
                                                                  √     √
        square, is an interconnection network constructed from a n× n mesh, in
        which the processors of every row and column are the leaves of a complete
May 7, 2022   11:14       Parallel Algorithms       9in x 6in    b4591-ch06                  page 265




                                         Tree-based Networks                           265




                               (a)                              (b)

         Fig. 6.9.    A mesh of trees of size 16. (a) Regular. (b) With base connections.


        binary tree. The base consists of n processors arranged in the form of a
        √     √
          n × n mesh. The base processors are either disjoint or have connections
        as in the regular mesh (see Fig. 6.9(a) and (b)). The mesh of trees of size
                      √                                       √
        n has 3n − 2 n processors. Each row or column has n processors at level
            √
        log n. All row trees are disjoint, and all column trees are disjoint. Every
        row tree has exactly one leaf processor in common with every column tree.
        In each tree, the leaf and the root has degree 2, and every other processor
        has degree 3. Like the pyramid, the communication diameter of the mesh
        of trees of size n is only Θ(log n), which is very low compared to a mesh
        of the same size. This is true since any two processors in the mesh of
        trees can communicate in O(log n) time. However, it may require as much
                √
        as 4 log n = Ω(log n) time for communication that requires exchanging
        of information between two arbitrary processors. This makes the mesh of
        trees suitable for problems like semigroup operations, e.g., summation and
        ﬁnding the maximum, which require O(log n) time. However, for problems
        that demand extensive data movement such as sorting and routing all data
                         √                                       √
        in the base, Ω( n) time may be required, since only n wires cross the
        middle of the mesh of trees.
            The processor connections in the base may be added, but this does
        not improve the computing power of the mesh of trees; it is only useful
        in applications like image processing where direct connections between the
        base processors is desirable. The mesh of trees has a recursive structure. If
                             √
        we remove all the √2 n roots
                                 √     and their incident edges, we will be left with
        four copies of the 2n × 2n mesh of trees. For instance, Fig. 6.10 shows the
May 7, 2022   11:14         Parallel Algorithms          9in x 6in      b4591-ch06                  page 266




        266                                       Parallel Algorithms




                          Fig. 6.10.   Recursive structure of the mesh of trees.


        four copies of the 2 × 2 mesh of trees resulting from removing the roots and
        their incident edges in Fig. 6.9 (a). Henceforth, the processors of the base
                                              √
        will be numbered as Pi,j , 1 ≤ i, j ≤ n.
            The trees in the mesh of trees simplify many computations that can
        be completed in Θ(log n) time. For instance, to broadcast a datum x from
        P1,1 to all other processors in the base, ﬁrst x is broadcast to the ﬁrst
                                                                      √
        row tree (the topmost tree). From the leaves of this tree, n copies are
        passed to all column trees, where x is passed to the leaves of those column
        trees. The semigroup operations like summation and ﬁnding the maximum
        are straightforward. For instance, to ﬁnd the sum of n numbers stored
                      √
        in the base, n row sums are ﬁrst found by row trees and stored in the
        ﬁrst column, followed by summing those totals in the ﬁrst column tree and
        storing the ﬁnal sum in P1,1 .


        6.3.1         Sorting on the mesh of trees
                                                                                       √
        The bisection width of the mesh of trees has a lower bound of Ω( n), which
        means that it is not suitable for sorting data of size in the order of Ω(n)
                                  √
        eﬃciently, since Ω( n) of the data might have to move from one side of the
        base to the opposite side. However, for a smaller amount of data, it may be
                                                                                      √
        possible to sort more eﬃciently. Consider, for instance, sorting n numbers
        a1 , a2 , . . . , a√n stored in processors P1,1 , P1,2 , . . . , P1,√n in the base — that
May 7, 2022   11:14       Parallel Algorithms        9in x 6in        b4591-ch06                    page 267




                                         Tree-based Networks                                  267


        is, in the ﬁrst row. We compute the rank of each element r(ai ), which is the
        number of items less than ai , and store ai in processor number r(ai ) + 1 in
        the ﬁrst row. For simplicity, assume that all items are distinct. First, for 1 ≤
              √
        j ≤ n, we use the column trees to broadcast aj in column j, after which
                                                             √                      √
        processor Pi,j will store a copy of aj , 1 ≤ i ≤ n. Next, for 1 ≤ i ≤ n,
        we broadcast ai from processor Pi,i in row i to all processors in row i. Now,
        every processor Pi,j in the base contains the pair (ai , aj ). Row i will now
        be responsible for ﬁnding r(ai ), the rank of ai ; it achieves this by counting
        the elements aj smaller than ai . Speciﬁcally, if aj < ai , then we store 1 in
        Pi,j , else we store 0 in Pi,j , and so ﬁnding the rank amounts to counting
        the number of 1’s, and storing the sum in all processors Pi,j in row i. The
        sum can easily be found using a row tree, which is then broadcast from the
        root to its leaves. Finally, a column broadcast is used within every column
        to broadcast ai from processor Pi,r(ai )+1 to processor P1,r(ai )+1 (recall that
        all processors in row i contain ai ). It is easy to see that computing the rank
        and broadcasting ai to its ﬁnal destination takes Θ(log n) time. It follows
        that the overall time taken by the algorithm is Θ(log n), which is optimal
        since the diameter is Θ(log n). The cost is Θ(n log n), which is not optimal
                            √
        in view of the Θ( n log n) time sequential algorithm. An outline of the
        above description is given as Algorithm motsort.

          Algorithm
                 √ 6.4 motsort
          Input: n numbers a1 , a2 , . . . , a√n stored in processors P1,1 , P1,2 , . . . , P1,√n
                 in the base.
          Output: Sort the numbers and store them in P1,1 , P1,2 , . . . , P1,√n .
                              √
           1. for j ← 1 to n do in parallel                                             √
           2.     Use column tree j to broadcast aj to processors Pi,j , 1 ≤ i ≤ n.
           3. end for        √
           4. for i ← 1 to n do in parallel
           5.     broadcast ai from processor Pi,i in row i to all processors in row i.
           6. end for        √
           7. for i ← 1 to n √   do in parallel
           8.     for j ← 1 to n do in parallel
           9.         if aj < ai , then store 1 in Pi,j , else store 0 in Pi,j .
          10.     end for
          11.     Compute the sum of 1’s in row i and store it in all processors Pi,j
                  of row i.
          12.     Perform column broadcasts to broadcast ai from processor
                  Pi,r(ai )+1 to processor P1,r(ai )+1 .
          13. end for
May 7, 2022   11:14     Parallel Algorithms           9in x 6in         b4591-ch06        page 268




        268                                   Parallel Algorithms


                        7     4         5      6              7   4         5   6

                                                              7   4         5   6


                                                              7   4         5   6


                                                             7    4         5   6


                                  (a)                                 (b)


                        7     7,4       7,5    7,6            0   1         1   1

                       4,7    4         4,5    4,6            0   0         0   0

                       5,7    5,4       5      5,6            0   1         0   0


                       6,7    6,4       6,5    6             0    1         1   0


                                  (c)                                 (d)


                        3     3         3      3              4   5         6   7

                        0     0         0      0


                        1     1         1      1


                       2      2         2      2


                                  (e)                                 (f)

                             Fig. 6.11.       Sorting on the mesh of trees.



        Example 6.3 Consider Fig. 6.11(a), in which the numbers 7, 4, 5, 6 are
        to be sorted in a mesh of trees of size 16; only the base is shown in the
        ﬁgure. First, for 1 ≤ j ≤ 4, we use the column trees to broadcast aj in
        column j as shown in Fig. 6.11(b), after which processor Pi,j will store a
        copy of aj , 1 ≤ i ≤ 4. Next, for 1 ≤ i ≤ 4, we broadcast ai from processor
        Pi,i in row i to all processors in row i. Now, every processor Pi,j in the base
        contains the pair (ai , aj ) (see Fig. 6.11(c)). Next, we compute the rank of
        ai by counting the number of elements aj smaller than ai . Speciﬁcally, if
May 7, 2022   11:14         Parallel Algorithms       9in x 6in         b4591-ch06                     page 269




                                           Tree-based Networks                                   269


        aj < ai , then we store 1 in Pi,j , else we store 0 in Pi,j (see Fig. 6.11(d)).
        Now, ﬁnding the rank amounts to counting the number of 1’s, and storing
        the sum in all processors Pi,j in row i. The sum can easily be found using
        a row tree, which is then broadcast from the root to its leaves, as shown
        in Fig. 6.11(e). Finally, a column broadcast is used within every column to
        broadcast ai from processor Pi,r(ai )+1 to processor P1,r(ai )+1 , as shown in
        Fig. 6.11(f).                                                                


        6.3.2         Routing in the mesh of trees
                                                                                        √
        The bisection width of the mesh of trees has a lower bound of Ω( n),
        which means that, as in the case of sorting, it is not suitable for routing
                                                               √
        data of size in the order of Ω(n), since Ω( n) of the data might have to
        move from one side of the base to the opposite side. However, for a smaller
        amount of data, it may be possible to route data more eﬃciently. Con-
                                           √
        sider, for instance, routing n packets v1 , v2 , . . . , v√n stored in processors
        P1,1 , P1,2 , . . . , P1,√n in the base, that is, in the ﬁrst row to destination pro-
                                                                                  √
        cessors P√n,δ(v1 ) , P√n,δ(v2 ) , . . . , P√n,δ(v√n ) . First, for 1 ≤ j ≤ n, we use
                                                                                  √
        the column trees to send vj to processor Pj,j . Next, for 1 ≤ j ≤ n, we use
                                                                                      √
        the row trees to send vj to processor Pj,δ(vj ) . Finally, for 1 ≤ j ≤ n, we
        use the column trees to send vj to processor P√n,δ(vj ) . Each of these steps of
        data movements takes Θ(log n) time. It follows that the overall time taken
        by the algorithm is Θ(log n). An outline of the above description is given
        as Algorithm motroute.

          Algorithm 6.5 motroute
                 √
          Input: n packets v1 , v2 , . . . , v√n stored in processors P1,1 , P1,2 , . . . , P1,√n in
                 the base, and destinations δ(v1 ), δ(v2 ), . . . , δ(v√n ).
          Output: Route the packets to destination processors
                    P√n,δ(v1 ) , P√n,δ(v2 ) , . . . , P√n,δ(v√n ) .
                            √
           1. for j ← 2 to n do in parallel
           2.     Use column tree j to send vj to processor Pj,j .
           3. end for       √
           4. for j ← 1 to n do in parallel
           5.     Use row tree j to send vj to processor Pj,δ(vj ) .
           6. end for       √
           7. for j ← 1 to n do in parallel
           8.     Use column tree δ(vj ) to send vj to processor P√n,δ(vj ) .
           9. end for
May 7, 2022   11:14        Parallel Algorithms               9in x 6in             b4591-ch06          page 270




        270                                         Parallel Algorithms


                          v1       v2         v3     v4             v1

                                                                             v2

                                                                                        v3


                                                                                              v4


                        δ( v3 ) δ( v4 ) δ( v1 )    δ( v2 )         δ( v3 ) δ( v4 ) δ( v1 )   δ( v2 )
                                        (a)                                       (b)

                                              v1

                                                     v2


                          v3


                                  v4                                 v3     v4          v1    v2

                        δ( v3 ) δ( v4 ) δ( v1 )    δ( v2 )         δ( v3 ) δ( v4 ) δ( v1 )   δ( v2 )
                                        (c)                                       (d)

                               Fig. 6.12.          Routing on the mesh of trees.


        Example 6.4 Figure 6.12(a) shows an example in which the packets
        v1 , v2 , v3 , v4 , initially in processors P1,1 , P1,2 , P1,3 , P1,4 are to be routed in
        a mesh of trees of size 16 to processors P4,3 , P4,4 , P4,1 , P4,2 in this order.
        Only the base is shown in the ﬁgure. First, for 1 ≤ j ≤ 4, we use the
        column trees to send vj to processor Pj,j as shown in Fig. 6.12(b). Next,
        for 1 ≤ j ≤ 4, we use the row trees to send vj to processor Pj,δ(vj ) as shown
        in Fig. 6.12(c). Finally, for 1 ≤ j ≤ 4, we use the column trees to send vj
        to processor P4,δ(vj ) as shown in Fig. 6.12(d).                                       



        6.4     Computing Parallel Prefix on the Mesh of Trees

        The parallel preﬁx problem was deﬁned in Section 2.5. In this section, we
        show how to compute it on the mesh of trees assuming addition as the
                                                                              √
        binary operation. So, given a sequence of n numbers xi,j | 1 ≤ i, j ≤ n
        stored in the base processors, we consider the problem of ﬁnding their
May 7, 2022   11:14      Parallel Algorithms       9in x 6in    b4591-ch06                   page 271




                                        Tree-based Networks                            271

                                        √
        preﬁx sums si,j | 1 ≤ i, j ≤ n on the mesh of trees. For simplicity, we
        will assume without loss of generality that the mesh of trees has mesh con-
        nections. We will also assume the row major ordering scheme. We will use
        Algorithm bfparprefix used for binary trees in Section 3.14 to compute
        parallel preﬁx on the butterﬂy. We assume there are n registers yi,j associ-
                                                                                 √
        ated with the n processors of the base. Also, we assume there are n regis-
                                                                                    √
        ters zi associated with processor P1,√n , P2,√n , . . . , P√n,√n , and n − 2 n+ 1
                                                                    √             √
        registers li,j associated with processor Pi,j , 2 ≤ i ≤ n, 1 ≤ j ≤ n − 1.
        First, the preﬁx sums of all rows are computed individually in parallel
        using Algorithm bfparprefix for binary trees. This takes Θ(log n) time.
                       √
        For 1 ≤ i ≤ n, let the preﬁx sums of row i be yi,1 , yi,2 , . . . , yi,√n . Note
        that these are not the ﬁnal preﬁx sums, except for row 1. Next, the preﬁx
                           √
        sums of column n are computed, again using Algorithm bfparprefix.
        These are denoted by s1,√n , s2,√n , . . . , s√n,√n , and they are the ﬁnal preﬁx
                           √
        sums for column n. This also takes Θ(log n) time. Next, for all processors
                                           √
        Pi,√n , we set zi ← si,√n , 1 ≤ i ≤ n. This is followed by setting zi ← zi−1 ,
                   √
        2 ≤ i ≤ n (recall that there are mesh connections). Now, for rows i,
                  √
        2 ≤ i ≤ n, we broadcast zi to row tree i, after which zi is copied to all
                                                                      √
        leaves of row tree i and stored in register li,j , 1 ≤ j ≤ n − 1. Finally, for
                  √             √
        2 ≤ i ≤ n, 1 ≤ j ≤ n − 1 we execute the assignment si,j ← yi,j + li,j .


          Algorithm 6.6 motparprefix       √
          Input: X = xi,j | 1 ≤ i, j ≤ n, a sequences of n numbers.
                                              √
          Output: S = si,j | 1 ≤ i, j ≤ n, the preﬁx sums of X.
                            √
           1. for i ← 1 to n do in parallel
           2.     Use Algorithm bfparprefix to compute the preﬁx sums of row i.
                  Let these be yi,1 , yi,2 , . . . , yi,√n .
           3. end for
           4. Use Algorithm√bfparprefix to compute the preﬁx sums
                  of column n. Let these be s1,√n , s2,√n , . . . ,√s√n,√n .
                            √ Pi, n , set zi ← si, n , 1 ≤ i ≤ n.
           5. for all processors      √                      √

           6. for i ← 2 to n do in parallel zi √            ← zi−1
           7. Broadcast zi to row tree i, 2 ≤ i ≤ n, and store zi in the register li,j of
              every leaf. √
           8. for i ← 2 to n √   do in parallel
           9.     for j ← 1 to n − 1 do in parallel
          10.         si,j ← yi,j + li,j
          11.     end for
          12. end for
May 7, 2022   11:14         Parallel Algorithms          9in x 6in             b4591-ch06        page 272




        272                                       Parallel Algorithms


                        1       4          2      3                  1    5          7      10


                        5       3          1      2                  5    8          9      11


                        4       6          3      2              4        10         13     15


                        1       3          4      5              1        4          8      13


                                     (a)                                       (b)


                        1       5          7      10                 1    5          7      10

                        5       8          9      21                 15   18         19     21


                        4       10         13     36             25       31         34     36


                        1       4          9      49             37       39         45     49


                                     (c)                                       (d)

               Fig. 6.13.   Illustration of the operation of Algorithm motparprefix.

        Broadcasting takes Θ(log n) time, and the parallel assignments take con-
        stant time. It follows that the running time of the algorithm is Θ(log n).
        An outline of the above description is given as Algorithm motparprefix.

        Example 6.5 Figure 6.13 shows an illustration of the operation of Algo-
        rithm motparprefix. The input is given in Fig. 6.13(a). Fig. 6.13(b)
        shows the preﬁxes of all rows individually. In part (c), the preﬁx sums
                  √
        of column n are computed, and in part (d) the ﬁnal preﬁx sums are
        shown.                                                                



        6.5     Comparison Between the Mesh of Trees and the Pyramid

        At ﬁrst glance, the structure of the mesh of trees appears to be similar to
        that of the pyramid. They are both constructed from the combination of
                                                                      √
        trees and the mesh. Moreover, both have Θ(n) processors, Θ n bisection
May 7, 2022   11:14     Parallel Algorithms       9in x 6in   b4591-ch06                page 273




                                       Tree-based Networks                       273


        width and Θ(log n) diameter. The diﬀerence between the two is that in
        the case of the pyramid, the apex is a bottleneck, while in the case of the
        mesh of trees, there is no such bottleneck. So, one might expect that the
                                                                                  √
        mesh of trees is more powerful than the pyramid. In fact, due to its Θ n
        bisection width, this is not the case in problems that require extensive data
        movement. It is only in the case of some problems with moderate amounts of
        data movement — that the mesh of trees can solve faster than the pyramid.



        6.6     Bibliographic Notes

        There are a number of books that cover parallel algorithms on the tree,
        pyramid and mesh of trees. These include Akl [4], Akl [5], Akl [6],
        Leighton [57], and Miller and Stout [67]. The algorithm for selection on
        the tree machine is from Stout [89]. The pyramid has long been proposed
        for performing high-speed low-level image processing computations. See, for
        instance, Cantoni and Levialdi [19], and Rosenfeld [78]. Parallel preﬁx com-
        putation on the pyramid computer is from Cinque and Bongiovanni [25].
        Detailed parallel algorithms for many problems on the pyramid can be
        found in Miller and Stout [67]. The mesh of trees was proposed indepen-
        dently by several authors; see, for instance, Leighton [55]. Parallel algo-
        rithms for many problems on the mesh of trees can be found in Leighton [57].
        For more references on parallel algorithms on the tree, pyramid and mesh of
        trees interconnection networks, see, for instance, Leighton [57], and Miller
        and Stout [67].



        6.7     Exercises

         6.1. Design an algorithm to ﬁnd the sum of n numbers on a tree network
              with n leaf processors. The input numbers are stored at the leaves,
              and the output is to be stored in the root processor. What is the
              time complexity of your algorithm?

         6.2. Design an algorithm to ﬁnd the maximum of n numbers on a tree-
              connected computer with O(log n) processors. The input numbers
              are stored at the leaves, and the output is to be stored in the root
              processor. What is the time complexity of your algorithm?
May 7, 2022   11:14     Parallel Algorithms          9in x 6in      b4591-ch06               page 274




        274                                   Parallel Algorithms

         6.3. Give a recursive algorithm for ﬁnding the maximum in the tree
              machine.

         6.4. Illustrate the operation of Algorithm treeselect to ﬁnd the 13th
              smallest element of the 16 items: 7, 10, 15, 13, 2, 9, 5, 12, 3, 8, 11, 4,
              6, 14, 17, 16 on the tree machine with 16 processors.

         6.5. (a) What is the bisection width of the 1-dimensional pyramid?
              (b) Give a lower bound on the problems of sorting and routing on
                  the 1-dimensional pyramid.


         6.6. What is the diameter of the 2-dimensional pyramid?

         6.7. Use Algorithm pyramidparprefix to compute the preﬁx sums of
              the 16 numbers: 2, 1, 1, 3, 2, 1, 2, 4, 3, 5, 1, 4, 2, 1, 3, 1 stored in the
              base of a 2-dimensional pyramid.

         6.8. (a) Compute the bisection width of the 2-dimensional pyramid.
              (b) Give a lower bound on the problems of sorting and routing on
                  the 2-dimensional pyramid.


         6.9. Assume that a digitized black/white picture is initially stored one
              pixel per processor in the base of the pyramid. Give an algorithm to
              ﬁnd the area of the picture, that is, the total number of black pixels
              in the picture, on the pyramid machine.

        6.10. Give an algorithm to determine whether there are more black pixels
              than white pixels in a digitized picture consisting of n pixels stored
              in the base of the pyramid of size n (refer to Exercise 6.9).

        6.11. Give an algorithm to determine the sum of n numbers stored in the
              base of the pyramid.

        6.12. Explain how to compute parallel preﬁx of n numbers on the pyramid
              of size O(n/log n) processors. What is the cost of your algorithm?

        6.13. Assess the pyramid machine in terms of sorting and routing.

        6.14. What is the diameter of the mesh of trees?
May 7, 2022   11:14      Parallel Algorithms       9in x 6in    b4591-ch06                   page 275




                                        Tree-based Networks                           275

        6.15. Give an algorithm for ﬁnding the sum of n numbers on the mesh
              of trees of size n. The numbers are initially loaded into the mesh of
              trees, one element per processor, and their sum is to be stored in the
              topleft processor.

        6.16. (a) Compute the bisection width of the mesh of trees.
              (b) Give a lower bound on the problems of sorting and routing n
                  items on the mesh of trees of size n.

        6.17. Use Algorithm motsort to sort the 4 numbers: 2, 1, 3, 5 on the
              mesh of trees of size 16.

        6.18. Give an assessment of the mesh of trees in terms of sorting and
              routing.

        6.19. Use Algorithm motroute to route the numbers 5, 3, 2, 4, initially
              stored in processors P1,1 , P1,2 , P1,3 , P1,4 in a mesh of trees of size 16
              to processors P4,2 , P4,3 , P4,1 , P4,4 in this order.

        6.20. Let A be an algorithm that runs on the pyramid of size n in time
              t(n). What will be the running time of A when simulated on the
              mesh of trees of the same size? Explain.

        6.21. Generalize Exercise 6.20 to any network. That is, what will be the
              running time of A when simulated on a network of the same size?



        6.8     Solutions

         6.1. Design an algorithm to ﬁnd the sum of n numbers on a tree network
              with n leaf processors. The input numbers are stored at the leaves,
              and the output is to be stored in the root processor. What is the
              time complexity of your algorithm?

                 Similar to ﬁnding the sum on the PRAM using the tree method. The
                 time complexity is Θ(log n).

         6.2. Design an algorithm to ﬁnd the maximum of n numbers on a tree-
              connected computer with O(log n) processors. The input numbers
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch06             page 276




        276                                     Parallel Algorithms

                 are stored at the leaves, and the output is to be stored in the root
                 processor. What is the time complexity of your algorithm?
                 Assign O(n/ log n) elements to each leaf processor. Initially, each
                 leaf processor ﬁnds the maximum of its assigned elements. The rest
                 is as in Exercise 6.1. The running time is O(n/ log n + log log n) =
                 O(n/ log n).

         6.3. Give a recursive algorithm for ﬁnding the maximum in the tree
              machine.
                 Recursively ﬁnd the two individual maxima in the two subtrees of
                 the root, and compute their maximum.

         6.4. Illustrate the operation of Algorithm treeselect to ﬁnd the 13th
              smallest element of the 16 items: 7, 10, 15, 13, 2, 9, 5, 12, 3, 8, 11, 4,
              6, 14, 17, 16 on the tree machine with 16 processors.
                 Similar to Example 6.1.

         6.5. (a) What is the bisection width of the 1-dimensional pyramid?
              (b) Give a lower bound on the problems of sorting and routing on
                  the 1-dimensional pyramid.

                 (a) If we consider a 1-dimensional pyramid of size n, and cut it by
                 a line slightly oﬀ-center, the line will cut log n + 1 links. Hence, the
                 bisection width of the 1-dimensional pyramid is log n + 1.
                 (b) Since all n data items at the leaves of the 1-dimensional
                 pyramid may have to cross from one side to the other, at least
                 n/(log n + 1) = Ω(n/ log n) time is required just to get data across
                 the middle of the 1-dimensional pyramid (see Exercise 6.5(a)). Hence,
                 the lower bound is Ω(n/ log n).

         6.6. What is the diameter of the 2-dimensional pyramid?
                 The diameter of the 2-dimensional pyramid is 2 log4 n.

         6.7. Use Algorithm pyramidparprefix to compute the preﬁx sums of
              the 16 numbers: 2, 1, 1, 3, 2, 1, 2, 4, 3, 5, 1, 4, 2, 1, 3, 1 stored in the
              base of a 2-dimensional pyramid.
                 Similar to Example 6.2.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in   b4591-ch06                page 277




                                         Tree-based Networks                          277


         6.8. (a) Compute the bisection width of the 2-dimensional pyramid.
              (b) Give a lower bound on the problems of sorting and routing on
                  the 2-dimensional pyramid.

                 (a) Consider the number of links crossing the middle of the pyramid
                                                                 √
                 of size n. In the base of the pyramid, there are n links crossing the
                                                                     √
                 middle of the pyramid, in the next level, there are n/2 such links,
                 and so forth. Thus, the total number of links that cross the middle
                 of the pyramid is

                                                
                                            log4 n−1   √
                                                         n    √
                                                         j
                                                           = 2 n − 2.
                                                j=0
                                                       2

                                                                             √
                 Hence, the bisection width of the 2-dimensional pyramid is 2 n−2 =
                   √
                 Θ( n).

                 (b) Since all n data items in the base of the pyramid may have to cross
                                                                            √
                 from one side of the base mesh to the other, at least n/(2 n − 2) =
                    √
                 Ω( n) time is required just to get data across the middle of the
                                                                                 √
                 pyramid (see Exercise 6.8 (a)). That is, the lower bound is Ω( n).

         6.9. Assume that a digitized black/white picture is initially stored one
              pixel per processor in the base of the pyramid. Give an algorithm to
              ﬁnd the area of the picture, that is, the total number of black pixels
              in the picture, on the pyramid machine.

                 The area of the picture can be determined as follows: In stage 1 of
                 the algorithm, every processor in level 1 obtains the values of the
                 pixels stored in its four children in the base processors, computes the
                 number of black pixels, and sends the count to its parent. In general,
                 at stage j, 1 ≤ j ≤ log4 n, of the algorithm, every processor P at
                 level j obtains the values of the pixels stored in its four children at
                 level j − 1, and computes the total number of black pixels in the
                 subpyramid under P . Finally, at the ﬁnal stage, the apex obtains the
                 values of the pixels stored in its four children at level log4 n − 1, and
                 computes the total number of black pixels in the pyramid. The total
                 number of stages is log4 n, and each stage takes Θ(1) time for a total
                 of Θ(log n).
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch06           page 278




        278                                     Parallel Algorithms

        6.10. Give an algorithm to determine whether there are more black pixels
              than white pixels in a digitized picture consisting of n pixels stored
              in the base of the pyramid of size n (refer to Exercise 6.9).
                 Similar to the bit counting problem in Exercise 6.9.

        6.11. Give an algorithm to determine the sum of n numbers stored in the
              base of the pyramid.
                 Similar to the bit counting problem in Exercise 6.9.

        6.12. Explain how to compute parallel preﬁx of n numbers on the pyramid
              of size O(n/log n) processors. What is the cost of your algorithm?
                 Assign O(log n) elements to each leaf processor in the base. First, ﬁnd
                 the preﬁx sums in each group in the base sequentially in O(log n)
                 time. Next, apply the preﬁx sums algorithm for the pyramid on
                 the ﬁnal sums of all groups in time O(log(n/ log n)) = O(log n).
                 Finally, update the preﬁx sums in all groups in the base sequentially
                 in O(log n) time. The running time is O(log n).

        6.13. Assess the pyramid machine in terms of sorting and routing.
                                             √
              By Exercise 6.8(b), it takes Ω( n) time to sort n numbers on the
              pyramid. This shows that the pyramid is a poor choice for problems
              that require intensive data movements such as sorting, routing and
              some problems in computational geometry.

        6.14. What is the diameter of the mesh of trees?
                 The diameter of the mesh of trees is the smallest distance between
                 two processors in opposite corners, which is 4 log n .

        6.15. Give an algorithm for ﬁnding the sum of n numbers on the mesh
              of trees of size n. The numbers are initially loaded into the mesh of
              trees — one element per processor, and their sum is to be stored in
              the topleft processor.
                 First, each row tree ﬁnds the sum of the elements stored at its leaves
                 and stores the sum in its root in Θ(log n) time. Next, the sums in
                 all these roots are routed to the leaves of the leftmost column tree
                 in Θ(log n) time. Finally, the elements at the leaves of this column
May 7, 2022   11:14       Parallel Algorithms       9in x 6in   b4591-ch06                   page 279




                                         Tree-based Networks                          279

                 tree are summed and their sum is routed to the topleft processor in
                 Θ(log n) time. The total running time is Θ(log n).

        6.16. (a) Compute the bisection width of the mesh of trees.
              (b) Give a lower bound on the problems of sorting and routing n
                  items on the mesh of trees of size n.

                 (a) The number of links crossing the middle of the mesh of trees
                                                            √
                 of size n (without the base connections) is n. Thus, the bisection
                                              √        √
                 width of the mesh of trees is n = Θ( n).
                 (b) Since all n data items in the base of the mesh of trees may
                 have to cross from one side of the base mesh to the other, at least
                     √        √
                 n/ n = Ω( n) time is required just to get data across the middle
                 of the mesh of trees (see Exercise 6.16 (a)). That is, the lower bound
                      √
                 is Θ( n).

        6.17. Use Algorithm motsort to sort the 4 numbers: 2, 1, 3, 5 on the
              mesh of trees of size 16.
                 Similar to Example 6.3.

        6.18. Give an assessment of the mesh of trees in terms of sorting and
              routing.
                                                √
              By Exercise 6.16(b), it takes Ω( n) time to sort or route n num-
              bers on the mesh of trees. It follows that the mesh of trees is not a
              good choice for problems that require intensive data movements such
              as sorting, routing and some problems in computational geometry.
              However, unlike the pyramid, the mesh of trees is capable of sorting
              a restricted amount of data in certain conﬁgurations in Θ(log n) time
              (see Section 6.3.1).

        6.19. Use Algorithm motroute to route the numbers 5, 3, 2, 4, initially
              stored in processors P1,1 , P1,2 , P1,3 , P1,4 in a mesh of trees of size 16
              to processors P4,2 , P4,3 , P4,1 , P4,4 in this order.
                 Similar to Example 6.4.

        6.20. Let A be an algorithm that runs on the pyramid of size n in time
              t(n). What will be the running time of A when simulated on the
              mesh of trees of the same size? Explain.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch06         page 280




        280                                     Parallel Algorithms


                 There will be a slow-down by a factor of O(log n) — that is, the
                 running time will be O(t(n) log n). To see this, let Pi and Pj be two
                 adjacent processors on the pyramid machine, and suppose they are
                 mapped to processors Pk and Pl on the mesh of trees. The transfer
                 of data between processors Pi and Pj on the pyramid, which takes
                 constant time, is simulated in O(log n) time between processors Pk
                 and Pl on the mesh of trees.

        6.21. Generalize Exercise 6.20 to any network. That is, what will be the
              running time of A when simulated on a network of the same size?
                 The mesh of trees with n processors can simulate any network of the
                 same size with a slow-down factor of O(log n). The justiﬁcation is
                 the same as that of Exercise 6.20.
May 7, 2022   11:14    Parallel Algorithms       9in x 6in   b4591-ch07                 page 281




                                             Chapter 7


                            The Star Network



        7.1     Introduction

        An eﬃcient interconnection topology usually possesses the following prop-
        erties: small diameter, low degree, high connectivity, regularity, node sym-
        metry, and a simple routing algorithm. The small diameter shortens the
        message routing delay while the low degree of nodes is necessary to limit
        the number of input-output ports to some acceptable value. Regular graphs
        with the property of node symmetry play the most important role in net-
        work design, due to the simplicity of designing routing algorithms. One of
        the most eﬃcient interconnection networks has been the well-known binary
        hypercube; it has been used to design various commercial multiprocessor
        machines and it has been extensively studied. Another regular interconnec-
        tion network was proposed as an attractive alternative to the hypercube,
        called the star. The star, also called d-star, is node and edge symmetric,
        and strongly hierarchical as is the case with the hypercube. Let d be a
        positive integer. The d-dimensional star, denoted by Sd , is deﬁned as fol-
        lows. Consider the n = d! permutations with d symbols, typically 1 to d.
        d! processors are deﬁned, one per permutation, such that two processors
        are connected by a bidirectional link if and only if their corresponding per-
        mutations diﬀer only in the leftmost and any other position. That is, there
        is a connection between processor Pα and processor Pβ if and only if β
        can be obtained from α by interchanging the ﬁrst and the jth symbols of
        α, 2 ≤ j ≤ d (see Fig. 7.1). For example, consider the case when d = 3.



                                                281
May 7, 2022   11:14            Parallel Algorithms           9in x 6in         b4591-ch07          page 282




        282                                          Parallel Algorithms


                               1


                               (a)
                                                            1234                     4231

                      12               21
                                                     3214   2134                     3241   2431

                               (b)                   2314   3124                     2341   3421


                                                            1324                     4321
                              123                           3412                     2413


                  321                  213           4312   1432                    4213    1423
                                                     1342   4132                    1243    4123
                  231                  312
                                                            3142                     2143
                        132
                              (c)                                        (d)

                Fig. 7.1.      d-dimensional star interconnection network; d = 1, 2, 3, 4.



        In this case, there are six processors: P123 , P132 , P213 , P231 , P312 and P321 .
        Figure 7.1(c) shows the connections between these processors.
            The d-dimensional star can also be deﬁned by recursive construc-
        tion, where it is constructed from d copies of (d − 1)-stars, denoted by
        Sd−1 (1), Sd−1 (2), . . . , Sd−1 (d), as follows. Here, the vertices of Sd−1 (i) are
        labeled by the (d − 1)-permutations of the symbols 1, 2, . . . , d except i. We
        add the symbol i at the end of each label of Sd−1 (i). For example, the four
        S3 ’s in Fig. 7.1(d) are constructed from the S3 in Fig. 7.1(c) by appending
        the digit i, 1 ≤ i ≤ 4. Two vertices in two diﬀerent substars are connected if
        and only if one permutation can be obtained from the other by exchanging
        the ﬁrst and last symbols. For instance, in Fig. 7.1(d), processor 1234 is
        connected to processor 4231.
            The d-dimensional star Sd compares with the hypercube favorably in
        several aspects. Its diameter is 3(d − 1)/2 = Θ(d), and its degree is d−1 =
        Θ(d), which are sublogarithmic in term of the number of processors (Notice
        that d < log(d!) = Θ(d log d)). Like the hypercube, the star graph is vertex-
        symmetric in the sense that any two vertices are similar, that is, the graph
        looks the same when viewed from any vertex. Each edge connects an odd
May 7, 2022    11:14            Parallel Algorithms                9in x 6in                    b4591-ch07                  page 283




                                                      The Star Network                                                283

        permutation with an even permutation, and so Sn is bipartite, and contains
        no C4 (the cycle on 4 vertices).


        7.2      Ranking of the Processors

        For some problems, e.g., the problem of sorting, it is imperative to impose
        a linear order on the processors. Let Pα and Pβ be two processors of Sd ,
        where α = a1 a2 . . . ad and β = b1 b2 . . . bd . The ordering ≺ on the processors
        is deﬁned as follows: Pα ≺ Pβ (or α ≺ β) if there exists an i, 1 ≤ i ≤ d, such
        that aj = bj for j > i and ai > bi . For example, 2314 ≺ 3214. To see this, let
        i = 2; then a3 a4 = b3 b4 and a2 > b2 . The rank of a processor Pβ , denoted
        by r(Pβ ) is deﬁned as the number of processors Pα such that Pα ≺ Pβ plus
        one. Table 7.1 shows the ranks of the processors of the 4-dimensional star.
            Figure 7.2 shows the star in Fig. 7.1(d) redrawn with processors’ ranks.
            The labels in Table 7.1 and their corresponding ranks can be generalized
        for any dimension d as follows. We describe the procedure for generating

                       Table 7.1.      The ranks of processors of the 4-dimensional star.

              label     r      label     r        label       r      label          r        label      r    label   r

              1234       1     2134       2       1324         3     3124            4       2314        5   3214     6
              1243       7     2143       8       1423         9     4123           10       2413       11   4213    12
              1342      13     3142      14       1432        15     4132           16       3412       17   4312    18
              2341      19     3241      20       2431        21     4231           22       3421       23   4321    24



                                                      1                         22

                                             6            2                    20           21
                                             5            4                    19           23

                                                      3                         24
                                                      17                        11

                                         18                                    12           9
                                                      15
                                         13           16                       7            10
                                                 14                                     8

          Fig. 7.2.         4-dimensional star interconnection network with processors’ ranks.
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch07       page 284




        284                                    Parallel Algorithms

        the labels in connections with the example 4-dimensional star. There are
        four steps to follow:

        (1) Generate the 4! permutations of {1, 2, 3, 4}.
            1234, 1243, 1324, 1342, 1423, 1432, 2134, 2143, 2314, 2341, 2413, 2431,
            3124, 3142, 3214, 3241, 3412, 3421, 4123, 4132, 4213, 42314312, 4321.
        (2) Revere their order.
            4321, 4312, 4231, 4213, 4132, 4123, 3421, 3412, 3241, 3214, 3142, 3124,
            2431, 2413, 2341, 2314, 2143, 2134, 1432, 1423, 1342, 1324, 1243, 1234.
        (3) Reverse every item in the list.
            1234, 2134, 1324, 3124, 2314, 3214, 1243, 2143, 1423, 4123, 2413, 4213,
            1342, 3142, 1432, 4132, 3412, 4312, 2341, 3241, 2431, 4231, 3421, 4321.
        (4) Partition the list of 24 items into four sublists of six elements each
            corresponding to the four substars.
            1234 ≺ 2134 ≺ 1324 ≺ 3124 ≺ 2314 ≺ 3214 ≺
            1243 ≺ 2143 ≺ 1423 ≺ 4123 ≺ 2413 ≺ 4213 ≺
            1342 ≺ 3142 ≺ 1432 ≺ 4132 ≺ 3412 ≺ 4312 ≺
            2341 ≺ 3241 ≺ 2431 ≺ 4231 ≺ 3421 ≺ 4321.

        Obtaining this set of labels can be achieved in parallel using the routine
        given in Algorithm starlabels.


          Algorithm 7.1 starlabels(d)
          Input: An integer d ≥ 1
          Output: Generate the labels of star Sd .
           1. If d = 1 then return {1}.
           2. Recursively generate all (d − 1)-permutations
              A = α1 , α2 , . . . , α(d−1)! of the symbols {1, 2, . . . , d − 1}:
              A ← starlabels(d − 1).
           3. for j ← 1 to (d − 1)! do in parallel
           4.     β0,j ← Append d to αj
           5. end for
           6. for i ← 1 to d − 1, do
           7.     for j ← 1 to (d − 1)! do in parallel
           8.         βi,j ← Interchange symbols d − i and d − i + 1 in βi−1,j
           9.     end for
          10. end for
          11. return {βi,j | 0 ≤ i ≤ d − 1, 1 ≤ j ≤ (d − 1)!}
May 7, 2022   11:14      Parallel Algorithms      9in x 6in     b4591-ch07                    page 285




                                         The Star Network                              285

              Assume that the labels are arranged into the rectangular table

                             {βi,j | 0 ≤ i ≤ d − 1, 1 ≤ j ≤ (d − 1)!}

        of dimensions d × (d − 1)! such that the entries in row i are the labels
        for substar Sd−1 (d − i), 0 ≤ i ≤ d − 1. We now show how to ﬁll out
        this table by showing how to obtain each row of the table from its pre-
        decessor. The ﬁrst step is to recursively generate all (d − 1)-permutations
        A = α1 , α2 , . . . , α(d−1)! of the symbols {1, 2, . . . , d − 1}. Append the sym-
        bol d to each αj to form the ﬁrst row of the table. This is done in Steps 3
        to 5. Next, interchange the symbols d and d − 1 in each β0,j to form the
        second row of the table. Thus, the symbol d − 1 is the last symbol in every
        label of the second row. This is followed by the row that consists of labels
        in which d − 1 and d − 2 are exchanged, which will make the symbol d − 2
        the last symbol in every label of the third row. This procedure continues
        until the last row is computed, in which 2 and 1 are exchanged. Thus, the
        symbol 1 is the last symbol in every label of the last row. This procedure
        for processing all the remaining d − 1 rows is done in Steps 6 to 10.
            The ranks are computed simply as increasing from left to right and from
        top to bottom. In fact, the rank of processor Pi,j is given by the formula
        r(Pi,j ) = i × (d − 1)! + j. This guarantees that those ranks in the same row
        are localized to one substar.
            The running time of Algorithm starlabels(d) is given by the recur-
        rence T (d) = T (d − 1) + Θ(d), which leads to a running time of T (n) =
        Θ(d2 ).


        7.3     Routing between Substars

        Let Sd−1 (a) and Sd−1 (b) be two diﬀerent substars of a star Sd , and suppose
        we want to transfer data from Sd−1 (a) to Sd−1 (b) such that data from two
        source processors go to diﬀerent destination processors. That is, no two
        sources send to the same destination. There are two types of routes as
        shown in Fig. 7.3: Direct and indirect. In a direct route, there is a direct
        link between the source and destination, as shown in part (a) of the ﬁgure.
        The other type, shown in part (b) of the ﬁgure, is the indirect route, which
May 7, 2022    11:14           Parallel Algorithms          9in x 6in      b4591-ch07           page 286




        286                                          Parallel Algorithms

                       1234                     4231

                                                           3214                 3241




                                                                                        4213
                                                                                        1243



                                   (a)                                       (b)

                   Fig. 7.3.     Routes between two substars: (a) Direct. (b) Indirect.



        consists of a path from the source to the destination. Speciﬁcally, there is
        a path of three links connecting the source to the destination.
           The method of routing between the two substars is accomplished by
        Algorithm starroute. In the algorithm, both α and β consist of d − 2
        symbols. Clearly, the running time of the algorithm is Θ(1).

          Algorithm 7.2 starroute(a, b)
          Input: Two integers a and b, 1 ≤ a < b ≤ d.
          Output: Send data from Sd−1 (a) to Sd−1 (b).
              1. There are (d − 2)! processors Pu with label u = bαa. These processors
                 send their data immediately in one step from Pu in Sd−1 (a) to Pv in
                 Sd−1 (b), where v = aαb.
              2. The remaining (d − 1)! − (d − 2)! processors Pw in Sd−1 (a) with w = cαa,
                 c = a, b, send their data from Sd−1 (a) to Px in Sd−1 (b), where x = cβb in
                 three substeps:
                 (a) First, data is sent from processors Pw in Sd−1 (a) to processors Py in
                      Sd−1 (c), where y = aαc.
                 (b) Next, processors Py send the data they received to Pz in the same
                      substar Sd−1 (c), where z = bβc.
                 (c) Finally, processors Pz send the data they received to Px in Sd−1 (b),
                      where x = cβb, as stated above.
May 7, 2022   11:14       Parallel Algorithms       9in x 6in       b4591-ch07                      page 287




                                          The Star Network                                   287


        Example 7.1 Suppose we want to carry on data transfers from S3 (4) to
        S3 (1) in Fig. 7.1(d). The following steps will take place.

        (1)   1234 → 4231,
        (2)   1324 → 4321,
        (3)   3214 → 4213 → 1243 → 3241,
        (4)   2314 → 4312 → 1342 → 2341,
        (5)   2134 → 4132 → 1432 → 2431, and
        (6)   3124 → 4123 → 1423 → 3421.                                                      


        7.4     Computing Parallel Prefix on the Star

        In this section, assume that the d! processors are numbered by their ranks,
        that is by the integers 1 to d!. It is important that every processor Pα
        knows its label α as well as its rank r(Pα ). This can be done in Θ(d2 )
        time (Exercise 7.1). For convenience, we will assume addition as the binary
        operation. Let a sequence of elements a1 , a2 , . . . , ad!  be given, stored in
        the processors of Sd , one element per processor. Thus ai is initially stored
        in Pi , where the index i is the rank of the processor, as explained above.
        The problem is to ﬁnd the preﬁx sums of a1 , a2 , . . . , ad! , which are a1 , a1 +
        a2 , a1 + a2 + a3 , . . . , a1 + a2 + · · · + ad! . For simplicity, we will assume that d
        is a power of 2; otherwise, it would only make the presentation complex
        (see Example 7.2).
             We divide the d substars into groups. Initially, there are d groups,
        each containing only one substar and the algorithm is applied recur-
        sively to that substar. Next, there are d/2 groups: {Sd−1 (1), Sd−1 (2)},
        {Sd−1 (3), Sd−1 (4)}, . . . , {Sd−1 (d − 1), Sd−1 (d)}. In the next iteration, there
        are d/4 groups: {Sd−1 (1), Sd−1 (2), Sd−1 (3), Sd−1 (4)}, . . . , {Sd−1 (d−3), Sd−1
        (d − 2), Sd−1 (d − 1), Sd−1 (d)}, and so on. Suppose that we have com-
        puted the preﬁx sums for two groups of substars as follows. Group 1:
        Sd−1 (i), Sd−1 (i + 1), . . . , Sd−1 (i + s), and Group 2: Sd−1 (i + s + 1), Sd−1
        (i + s + 2), . . . , Sd−1 (i + 2s + 1). Here, s = 1, 2, 4, . . .. There are two vari-
        ables associated with each processor, x and y, for storing the partial preﬁx
        sum so far and the total sum of values in the group to which it belongs,
        respectively. Let the total sum in Group 1 be y1 and the total sum in
May 7, 2022   11:14           Parallel Algorithms               9in x 6in               b4591-ch07           page 288




        288                                          Parallel Algorithms

        Group 2 be y2 . We ﬁrst use Algorithm starroute in Section 7.3 to send
        y1 to every processor in Group 2 and y2 to every processor in Group 1. The
        preﬁx sums of processors in Group 1, the x1 ’s, remain the same, while the
        preﬁx sum x2 in a processor in Group 2 becomes x2 + y1 . The total sum for
        all processors in both groups becomes y1 + y2 . It is important to note that
        all of these steps require Θ(1) time. Now, combining Group 1 and Group 2
        forms a single group. The steps just described are then used to merge the
        new group with another group formed in the same way. This continues until
        all the preﬁx sums have been computed.
            The above procedure induces a binary tree for the computation of the
        preﬁx sums (see Fig. 7.4 for example). When the recursion terminates, each
        processor in the group holds the variables x and y required at the beginning
        of the merging phase. The groups are now merged in pairs, as described in
        the previous paragraph.
            The merging process is performed as follows. We ﬁrst merge d/2 consec-
        utive pairs of substars to yield d/2 groups of size 2. Next, we merge d/4 pairs
        of consecutive 2-substar groups to yield d/4 groups of size 4. Continuing
        this way, in the jth iteration, we merge d/2j pairs of groups of size 2j−1
        to yield d/2j groups of size 2j . Algorithm starparprefix implements this
        idea. The algorithm maintains the variable s which is the size of groups


                                                           1          4       5    7    10    11
                      2nd iteration                        11        11       11   11   11    11
                                                           P1        P2       P3   P4   P5    P6


                                                                                                         x
                                                                                                         y
                                             1        4         5         7
                      1st iteration          7        7         7         7
                                             P1       P2        P3        P4



                                       1         4                  1         3              3       4
                      Recursive        4         4                  3         3              4       4
                      Processors       P1     P2                    P3        P4             P5      P6
                      Input            1      3                      1        2               3      1

                  Fig. 7.4.       Illustration of computing the preﬁx sums on the star.
May 7, 2022   11:14      Parallel Algorithms      9in x 6in       b4591-ch07                     page 289




                                         The Star Network                                 289


          Algorithm 7.3 starparprefix
          Input: A sequence of d! values a1 , a2 , . . . , ad!  stored in d! processors
                 P1 , P2 , . . . , Pd! , where d is a power of 2.
          Output: The preﬁx sums of the sequence.
           1. Recursively ﬁnd the preﬁx sums in all (d − 1)-substars in parallel. Store
              the preﬁx sums of a (d − 1)-substar in the x registers, and the totals in
              the y registers of all its processors.
           2. t ← 1
           3. while t ≤ d/2
           4.     s ← t;     t ← 2s;     i ← 0;    v ← d/t
           5.     for k ← 0 to v − 1 do in parallel
           6.         i ← kt
           7.         for j ← i + 1 to i + s do in parallel
           8.             Sd−1 (j) sends to Sd−1 (j + s) its y1 register, and
                          Sd−1 (j + s) sends to Sd−1 (j) its y2 register using
                          Algorithm starroute in Section 7.3
           9.             Each processor in Sd−1 (j) adds y2 to its y1 register.
          10.             Each processor in Sd−1 (j + s) adds y1 to its x1 and y1
                          registers.
          11.         end for
          12.     end for
          13. end while



        to be merged as discussed above. Initially, s is set to 1, and is doubled in
        each iteration of the while loop. i + 1, i + s and i + t, where t = 2s, deﬁne
        the boundaries of the two groups to be merged. The while loop is exe-
        cuted log(d/2) = Θ(log d) times. Step 8 takes Θ(1) time (see Section 7.3).
        Hence, the above description of the algorithm leads to a running time of
        T (d) = T (d − 1) + Θ(log d) = Θ(d log d).

        Example 7.2 Let d = 3, so the number of processors is n = d! = 6.
        Note here that d is not a power of 2. Figure 7.4 shows an illustration of
        the ﬂow of Algorithm starparprefix. There are 6 processors numbered
        P1 , P2 , . . . , P6 . The input is shown in the bottom: 1, 3, 1, 2, 3, 1. The results
        of the initial recursive calls are shown in the lowest level of the tree. For
        example, the preﬁx sums computed recursively in processors P1 and P2
        are 1 and 4, shown in the top box, and the sum of values is shown as 4
        in the bottom box. In the next level, processors P1 , P2 are merged with
        processors P3 , P4 , and the preﬁx sums are shown as 1, 4, 5, 7 and the total
        is 7. There is no other group to merge with processors P5 , P6 , so they are
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch07               page 290




        290                                    Parallel Algorithms

        passed to the next iteration. In the last iteration, the group of processors
        P1 , P2 , P3 , P4 are merged with the group of processors P5 , P6 . The resulting
        preﬁx sums are 1, 4, 5, 7, 10, 11 and the total sum is 11.                      


        7.5     Computing the Maximum

        In this section, we show how to compute the maximum of d! numbers stored
        one per processor of a d-dimensional star. The algorithm to be presented is
        a modiﬁcation of the algorithm for computing parallel preﬁx, as discussed
        in Section 7.4. In fact, the following algorithm is a simpliﬁcation of it.
        Instead of discussing the diﬀerences between the two algorithms, we will
        present the maximum ﬁnding algorithm for completeness. Assume that the
        d! processors are numbered by the integers 0 to d! − 1. Let a sequence of
        elements a0 , a1 , . . . , ad!−1  be given, stored in the processors of Sd , one
        element per processor. Thus ai is initially stored in Pi , where the index i is
        the rank of the processor minus 1, as described in Section 7.2. For simplicity,
        we will assume that d is a power of 2; otherwise, it would only make the
        presentation complex (see Example 7.3).
            We divide the d substars into groups. Initially, there are d groups,
        each containing only one substar and the algorithm is applied recursively
        to that substar. Next, there are d/2 groups: {Sd−1 (1), Sd−1 (2)}, {Sd−1(3),
        Sd−1 (4)}, . . . , {Sd−1 (d − 1), Sd−1 (d)}. In the next iteration, there are d/4
        groups: {Sd−1 (1), Sd−1 (2), Sd−1 (3), Sd−1 (4)}, . . . , {Sd−1 (d − 3), Sd−1
        (d − 2), Sd−1 (d − 1), Sd−1 (d)}, and so on. Suppose that we have com-
        puted the maximum for two groups of substars as follows. Group 1:
        Sd−1 (i), Sd−1 (i + 1), . . . , Sd−1 (i + s), and Group 2: Sd−1 (i + s + 1), Sd−1
        (i + s + 2), . . . , Sd−1 (i + 2s + 1), where s = 1, 2, 4, . . .. Suppose also that
        each processor holds the variable x for storing the maximum so far in the
        group to which it belongs. Let the maximum in Group 1 be x1 and the max-
        imum in Group 2 be x2 . We ﬁrst use Algorithm starroute to send x1 to
        every processor in Group 2 and x2 to every processor in Group 1. Then, the
        maximums in processors in Group 1 and in processors in Group 2 become
        max{x1 , x2 }. All of these steps require Θ(1) time since routing takes Θ(1)
        time, as described in Section 7.3. Group 1 and Group 2 now form a single
        group. The steps just described are then used to merge the new group with
        another group formed in the same way. This continues until the maximum
        has been computed.
May 7, 2022   11:14             Parallel Algorithms             9in x 6in               b4591-ch07              page 291




                                                The Star Network                                          291


                        2nd iteration                      7         7        7    7     7    7
                                                           P1        P2       P3   P4    P5   P6




                        1st iteration
                                                7     7     7             7
                                                P1    P2        P3        P4



                        Recursive         3      3                7           7               4      4
                        Processors        P1     P2               P3          P4              P5     P6
                        Input             2      3                   7        5               4      1

                      Fig. 7.5.     Illustration of computing the maximum on the star.

            The above procedure induces a binary tree for the computation of the
        maximum (see Fig. 7.5 for example). When the recursion terminates, each
        processor in the group holds the variable x required at the beginning of
        the merging phase for holding the maximum. The groups are now merged
        in pairs, as described in the previous paragraph. The merging process is
        performed as follows. We ﬁrst merge d/2 consecutive pairs of substars to
        yield d/2 groups of size 2. Next, we merge d/4 pairs of consecutive 2-substar
        groups to yield d/4 groups of size 4. Continuing this way, in the jth iteration,
        we merge d/2j pairs of groups of size 2j−1 to yield d/2j groups of size 2j .
        Algorithm starmax implements this idea. The algorithm maintains the
        variable s which is the size of the groups to be merged as discussed above.
        Initially, s is set to 1, and is doubled in each iteration of the while loop. i+1,
        i + s and i + t, where t = 2s deﬁne the boundaries of the two groups to be
        merged. The while loop is executed log(d/2) = Θ(log d) times. Step 8 takes
        Θ(1) time (see Section 7.3). Hence, the above description of the algorithm
        leads to a running time of T (d) = T (d − 1) + Θ(log d) = Θ(d log d).

        Example 7.3 Let d = 3, so the number of processors is n = d! = 6. Note
        here that d is not a power of 2. Figure 7.5 shows an illustration of the ﬂow
        of Algorithm starmax. There are 6 processors numbered P1 , P2 , . . . , P6 .
        The input is shown in the bottom: 2, 3, 7, 5, 4, 1. The results of the initial
May 7, 2022   11:14      Parallel Algorithms          9in x 6in       b4591-ch07              page 292




        292                                    Parallel Algorithms


          Algorithm 7.4 starmax
          Input: A sequence of d! values a0 , a1 , . . . , ad!−1  stored in d! processors
                 a0 , a1 , . . . , ad!−1 , where d is a power of 2.
          Output: The maximum of all values.
           1. Recursively ﬁnd the maximum in all (d − 1)-substars in parallel. Store
              the maximum of a (d − 1)-substar in the x registers of all its processors.
           2. t ← 1
           3. while t ≤ d/2
           4.     s ← t;     t ← 2s;     i ← 0;  v ← d/t
           5.     for k ← 0 to v − 1 do in parallel
           6.         i ← kt
           7.         for j ← i + 1 to i + s do in parallel
           8.             Sd−1 (j) sends to Sd−1 (j + s) its x1 register, and
                          Sd−1 (j + s) sends to Sd−1 (j) its x2 register using
                          Algorithm starroute in Section 7.3
           9.             Each processor in Sd−1 (j) updates its x register to
                          max{x1 , x2 }.
          10.             Each processor in Sd−1 (j + s) updates its x register to
                          max{x1 , x2 }.
          11.         end for
          12.     end for
          13. end while



        recursive calls is shown in the lowest level of the tree. For example, the max-
        imum computed in processors P1 and P2 is 3, and it is stored in both proces-
        sors. In the next level, processors P1 , P2 are merged with processors P3 , P4 ,
        and the maximum is shown as 7, again stored in all four processors. There
        is no other group to merge with processors P5 , P6 , so they are passed to the
        next iteration. In the last iteration, the group of processors P1 , P2 , P3 , P4
        are merged with the group of processors P5 , P6 . The resulting maximum
        is 7, stored in all processors.                                               


        7.6     Neighborhood Broadcasting and Recursive Doubling

        Assume that the source processor Px , where x = aβc, wants to send a
        message in its substar Sd−1 (c) to the d − 2 processors

                                        bβc,      for all b = a, c.

        The technique of recursive doubling is used to generate the labels of proces-
        sors in substar Sd−1 (c) reachable from the source processor Px eﬃciently.
May 7, 2022    11:14       Parallel Algorithms      9in x 6in   b4591-ch07                   page 293




                                           The Star Network                           293

        It is important to note that the generated labels have distinct starting
        symbols, that is, no two labels have the same starting symbol. Initially,
        the source processor is the only one with the message. In one step, it
        sends the message through a direct link to one of its neighbors. Now
        two processors have the message and they in turn send the message to
        two other processors in such a way that the source sends its message to
        another neighbor in one step and the neighbor which has received the
        message in the previous step sends the message to one of its neighbors
        in one step. The number of processors with the message is now four (the
        source processor and the other three processors) and these four proces-
        sors send the message to four more processors in the same substar in the
        same fashion. The algorithm continues until all d − 2 processors receive the
        message.
            One possible implementation is given in Algorithm starrecdub. Given
        the source processor α = a1 a2 a3 . . . ad , the algorithm simply selects the mid-
        dle symbol and exchanges it with the ﬁrst symbol to obtain the label β. It
        then transmits the message to the processor with the newly generated label,
        and repeats the same procedure recursively on the left half of α and the right
        half of β in parallel. The initial call of the algorithm is starrecdub(α, 1, d),
        where α is the source label. The number of labels generated by the algo-
        rithm is d − 1, that is, d − 2 plus the source processor label. Recall that the
        generated labels have distinct starting symbols. This procedure of repetitive
        doubling leads to a running time of T (d) = T (d/2) + Θ(1) = Θ(log d).


          Algorithm 7.5 starrecdub
          Input: A processor label α = a1 , a2 , . . . , ad , and two integers l and h,
                 1 ≤ l ≤ h ≤ d.
          Output: d − 1 processor labels βj = b1 b2 . . . , bd , where bd = ad , with the
                   property that the ﬁrst symbol b1 is diﬀerent in all labels βj .
              1. if h > l then
              2.     m ← (l + h)/2
              3.     β ← swap a1 and am in α.
              4.     Output β
              5.     do in parallel
              6.         Recursively call starrecdub(α, l, m)
              7.         Recursively call starrecdub(β, m + 1, h)
              8.     end
              9. end if
May 7, 2022   11:14     Parallel Algorithms           9in x 6in         b4591-ch07       page 294




        294                                   Parallel Algorithms

        Example 7.4      Consider applying the algorithm on the label 1234.
        starrecdub(1234, 1, 4) results in 3 labels: 2134, 1234 and 3124. Apply-
        ing the algorithm on the label 12345, starrecdub(12345, 1, 5) results
        in 4 labels: 32145, 21345, 12345 and 42135. Observe that the gen-
        erated labels have distinct starting symbols. If we choose α =
        5234161011987, the call starrecdub(5234161011987, 1, 11) results in 10
        labels: 6234151011987, 3254161011987, 2534161011987, 5234161011987,
        1254361011987,    4253161011987,      9234151011687,     1123415106987,
        1023415611987, 8234151011697.                                        

        Example 7.5 The algorithm can be applied on general labels, not just
        those composed of digits. If we choose α =abcdefghk, the call starrec-
        dub(abcdefghk, 1, 9) results in 8 labels: ebcdafghk, cbadefghk, bacdefghk,
        abcdefghk, dbacefghk, gbcdafehk, f bcdaeghk, hbcdafegk.                 


        7.7     Broadcasting in the Star

        In this section, we discuss broadcasting in the d-dimensional star Sd . In
        the description that follows, note that u, v, x, y and β are permutations, so
        they don’t have repeated symbols. Further, if one of them is combined with
        other symbols, e.g., a, b and c, then it is assumed that they do not contain
        these symbols, and these symbols themselves are diﬀerent. We assume that
        there is a message to be broadcast from processor Px , where x = aβb,
        1 ≤ a, b ≤ d, in substar Sd−1 (b) to all other processors in the star Sd . The
        action of broadcasting can be accomplished in the following three steps.

        (1) Use the algorithm for neighborhood broadcasting discussed in
            Section 7.6 to broadcast the message to the d − 2 processors

                                              cβb,   for all c = a, b

            in the substar Sd−1 (b) containing the source processor.
        (2) In this step, each of the d − 1 processors Pu , where u = cβb, c = b, in
            substar Sd−1 (b), sends the message it received in Step 1 to processor
            Pv , where v = bβc in substar Sd−1 (c).
        (3) Finally, all processors Py , where y = bβc, in substars Sd−1 (c),
            c ∈ {1, 2, . . . , d} − {b}, recursively broadcast the message in substars
            Sd−1 (c). The source processor Px , where x = aβb, recursively broad-
            casts the message in Sd−1 (b).
May 7, 2022    11:14      Parallel Algorithms          9in x 6in          b4591-ch07                 page 295




                                          The Star Network                                     295


         (a)      1234                   4231            (b)       1234                4231


         3214      2134                  3241   2431    3214       2134                3241   2431
         2314      3124                  2341   3421    2314       3124                2341   3421


                   1324                  4321                      1324                4321
                   3412                  2413                      3412                2413

         4312     1432                  4213    1423    4312       1432                4213   1423
        1342                            1243    4123 1342                              1243   4123
                   4132                                            4132


                  3142                   2143                      3142                2143

         (c)      1234                   4231            (d)       1234                4231


         3214      2134                  3241   2431    3214       2134                3241   2431
         2314      3124                  2341   3421    2314       3124                2341   3421


                   1324                  4321                      1324                4321
                   3412                  2413                      3412                2413

         4312     1432                  4213    1423    4312       1432                4213   1423
        1342                            1243    4123 1342                              1243   4123
                   4132                                            4132


                  3142                   2143                      3142                2143

                          Fig. 7.6.    Example of broadcasting in the star.

           The above procedure leads to the following recurrence for the running
        time: T (d) = T (d − 1) + Θ(log d), whose solution is Θ(d log d) = Θ(log d!).

        Example 7.6 The algorithm for broadcasting in the star is illustrated
        in Fig. 7.6. Initially, processor 1234 in the star S4 holds the message to
        be broadcast. By Step 1 and Example 7.4, the message is propagated to
        processors 2134 and 3214, as shown in Fig. 7.6 (b), in which the processors
        that hold the message are shown as dark nodes, and the message transmis-
        sions are shown by thick lines. After Step 2 of the algorithm, the message is
        transmitted to processors 4231, 4132 and 4213, as shown in Fig. 7.6 (c) by
        the dark nodes. Finally, as shown in Fig. 7.6 (d), the message is broadcast
        recursively in all substars.                                               
May 7, 2022   11:14        Parallel Algorithms             9in x 6in            b4591-ch07              page 296




        296                                      Parallel Algorithms

        7.8     The Arrangement Graph

        The family of arrangement graphs is a generalization of the star graph
        topology. It is a family of graphs that contains the star graphs. The (d, k)-
        arrangement graph, denoted by Ad,k , is characterized by the two positive
        integers d and k, where 1 ≤ k < d. Its nodes consist of the (d−k)!       d!
                                                                                      per-
        mutations (arrangements) of d symbols, typically 1, 2, . . . , d, taken k at a
        time. The edges connect nodes that are diﬀerent in exactly one of their
        positions. The arrangement graph addresses a major drawback of the star
        graph, which is scalability; to go from dimension d to dimension d + 1,
        the number of processors in the star graph grows from d! to (d + 1)!. Fur-
        ther, the arrangement graphs are more ﬂexible than the star graphs in
        terms of choosing the main design parameters such as degree and diameter.
        Figure 7.7 shows diﬀerent arrangement graphs.
            The (d, k)-arrangement graph has d!/(d − k)! nodes, and is regular
        of degree k(d − k). Its diameter is 3k/2. As in the hypercube and the
        star, it is vertex-transitive, and has a hierarchical structure. The (d, 1)-
        arrangement graph Ad,1 is Kd , the complete graph on d vertices. The
        (d, d − 1)-arrangement graph Ad,d−1 is isomorphic to the usual star graph.
        In Fig. 7.7, A3,2 is C6 , the cycle on 6 vertices; it is also S3 , the star graph


                           A 2,1                         A 3,1                        A 3,2
                                                     1                          12            13


                      1           2
                                                                           32                      23



                                           3                           2        31            21


                          A 4,1                       A 4,2                          A 4,3




                                      Fig. 7.7.    Arrangement graphs.
May 7, 2022   11:14     Parallel Algorithms      9in x 6in    b4591-ch07                   page 297




                                        The Star Network                            297

        on 6 vertices, and A4,3 is S4 , the star graph on 24 vertices; it is the same
        as the graph shown in Fig. 7.1(d).



        7.9     The (d, k)-Star Graph

        A major practical diﬃculty with the d-star is the restriction on the number
        of nodes: d! for a d-star . The set of values of d! is spread widely over the
        set of integers; so, one may be faced with the choice of either too few or
        too many available nodes. To relax the restriction of the number of nodes
        d! in the d-star, the class of generalized star graphs, called arrangement
        graphs, was discovered. The arrangement graph was discussed in the pre-
        vious section. When designing an interconnection network based on the
        arrangement graph, we can make a more suitable choice for the number of
        nodes by tuning the two parameters d and k. Nevertheless, the degree of
        the resulting network, which is k(d − k), may be very high. This is a very
        signiﬁcant factor from the architectural point of view since the relatively
        high node degree results in additional diﬃculty in interconnection and extra
        complexity in processor design.
            As an alternative to overcome the diﬃculties mentioned above for the
        star graph and the arrangement graph, another generalization of the star
        graph, called the (d, k)-star, was proposed. As in the arrangement graph,
                                                                     d!
        the (d, k)-star graph, denoted by Sd,k , consists of the (d−k)!   permutations
        (arrangements) of d symbols, typically 1, 2, . . . , d, taken k at a time. It is
        regular of degree d − 1, number of nodes d!/(d − k)!, and diameter 2k − 1 for
        k < d/2 and k + (d − 1)/2 for k ≥ d/2 + 1. The (d, k)-star preserves
        many attractive properties of the d-star graph such as node symmetry, low
        degree, small diameter, hierarchical structure, maximal fault tolerance, and
        simple shortest routing. In addition, the (d, d − 1)-star is isomorphic to the
        d-star, and hence, all these properties can be derived for the d-star graph
        as it is a special case of the (d, k)-star graph. A (4, 2)-dimensional and a
        (5, 2)-dimensional star connection network are each shown in Fig 7.8. It is
        important to note that Sd,k can be formed by interconnecting d Sd−1,k−1 ’s.
        Fig 7.8(a) shows that S4,2 can be viewed as an interconnection of four
        S3,1 ’s through 2-edges (see next paragraph), and Fig 7.8(b) shows that S5,2
        can be viewed as an interconnection of ﬁve S4,1 ’s through 2-edges. In fact,
May 7, 2022   11:14                  Parallel Algorithms             9in x 6in              b4591-ch07                     page 298




        298                                                Parallel Algorithms


                                                                                  12                   21
                           12                        21
                                                                          32
                                                                                                                 51
                                                                                           52     31
                                                                                  42                   41
              32                                               41
                                42              31
                                                                    23           13                         25        15
                                13                                               53                    35
              23                           24                  14
                                                                                           24 14
                                                                         43
                                                                                                                 45
                      43                                  34

                                                                                      34               54

                                        (a)                                                     (b)

          Fig. 7.8.        (4, 2)-dimensional and (5, 2)-dimensional star connection networks.

        like the d-star graph Sd , Sd,k can be decomposed into Sd−1,k−1 ’s along
        any dimension i, 2 ≤ i ≤ k. That is, an Sd,k can be decomposed into d
        node-disjoint Sd−1,k−1 ’s diﬀerent ways by ﬁxing the symbol in any position
        i, 2 ≤ i ≤ k. This decomposition can be carried out recursively on each
        Sd−1,k−1 to obtain smaller subgraphs.
             Let d and k be two integers satisfying: 1 ≤ k ≤ d − 1. For simplicity, let
        d = {1, 2, . . . , d} and k = {1, 2, . . . , k}. A (d, k)-star graph is speciﬁed
        by two integers d and k, where 1 ≤ k ≤ d−1. The node set of Sd,k is denoted
        by {a1 a2 . . . ak | ai ∈ d and ai = aj for i = j}. The adjacency is deﬁned as
        follows: a1 a2 . . . ai . . . ak is adjacent to (1) ai a2 . . . a1 . . . ak through an edge
        of dimension i, where 2 ≤ i ≤ k (interchange a1 with ai ), and (2) xa2 . . . ak
        through dimension 1, where x ∈ d − {ai | 1 ≤ i ≤ k}. The edges of type
        (1) are referred to as i-edges (e.g., 2-edges and 3-edges), and the edges of
        type (2) are referred to as 1-edges. Note that the degree of each node is
        d − 1; each node is connected with (d − k) 1-edges, and an i-edge for each
        i, 2 ≤ i ≤ k. Let u be a node, and v a neighbor of u. v is called a 1-neighbor
        of u if they are connected by a 1-edge, and it is called an i-neighbor of u if
        they are connected by an i-edge.

        Example 7.7 Consider the graph shown in Fig 7.9(a) (it is the same as
        the graph shown in Fig. 7.8(a)). d = {1, 2, 3, 4}. The edge (21, 31) is a
        1-edge since 31 is obtained from 21 by replacing 2 in 21 with 3 ∈ d−{2, 1}.
        On the other hand, the edge (21, 12) is a 2-edge since 12 is obtained from
May 7, 2022   11:14                  Parallel Algorithms             9in x 6in               b4591-ch07                     page 299




                                                          The Star Network                                            299


                                                                                431      3-edge       134
                                                                    1-edge          2-edge
                                           1-edge
                                21                       31         231          341                234         314

                      2-edge                                                     241                324
                                                                    321                                         214
                                               41

                                               14                               421                   124

                                 24                  34
                                                                                213                   312
                      12                                       13
                                      42            43
                                                                    123                                         412
                                                                                 413                132

                           32                                 23    423         143                 432         142
                                                                          243                             342
                                            (a)                                          (b)

                  Fig. 7.9.            (4, 2) and (4, 3)-dimensional star connection networks.

        21 by swapping a1 and a2 . Hence, 31 is a 1-neighbor of 21 and 12 is a
        2-neighbor of 21.
            Now, consider Fig 7.9(b). d = {1, 2, 3, 4}. The edge (431, 231) is a
        1-edge since 231 is obtained from 431 by replacing 4 in 431 with 2 ∈ d −
        {4, 3, 1}. On the other hand, the edge (431, 341) is a 2-edge since 341 is
        obtained from 431 by swapping a1 and a2 , and the edge (431, 134) is a
        3-edge since 134 is obtained from 431 by swapping a1 and a3 . Hence, 231
        is a 1-neighbor of 431, 341 is a 2-neighbor of 431 and 134 is a 3-neighbor
        of 431. All dashed lines in the ﬁgure are 1-edges.                       
            In Sd,k , given an arbitrary node u, there exists a cycle between u and
        all u’s 1-neighbors (Exercise 7.14). In Sd,k , given two nodes which are not
        connected by a 1-edge, then cycles formed with these two nodes with their
        1-neighbors are disjoint from each other. It can be shown that Sd,k can be
                              d!
        decomposed into (d−k+1)!    vertex-disjoint cycles of length d − k + 1.

        Theorem 7.1 In Sd,k , for any node v, v and all its 1–neighbors form a
        clique Kd−k+1 of size d − k + 1.
        Proof.    Given any node v = a1 a2 . . . ak and its 1-neighbor set, which is
        denoted by U , we need to prove that any two nodes in U are connected
        with each other by an edge. Suppose x and y are two nodes in U . Let
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch07              page 300




        300                                     Parallel Algorithms

        x = ia2 . . . ak and y = ja2 . . . ak , x = y implies i = j. By deﬁnition of the
        (d, k)-star graph, there is also a 1-edge between x and y, which means every
        two nodes in U are connected to each other. Hence, v and its 1-neighbors
        form a clique with d − k + 1 nodes.                                           

              In fact, there are      d!
                                   (d−k+1)!     cliques each with d − k + 1 nodes in Sd,k .


        Example 7.8 In Fig. 7.9, the dashed subgraphs are cliques. For instance,
        the nodes {21, 31, 41} form a clique of size 3 in part (a) of the ﬁgure, and
        the nodes {431, 231} form a clique of size 2 in part (b) of the ﬁgure.     


        7.10      Sorting in the Sd,k Star

        In this section, we develop a simple sorting algorithm for the (d, k)-star by
        embedding a 2-dimensional mesh into Sd,k . For convenience, we will refer to
        a processor and its label interchangeably. In this embedding, the vertices of
                                                     (d−1)!
        Sd,k are arranged into a 2-dimensional d × (d−k)!   mesh in row-major order.
        The nodes are labeled as described in Section 7.2; we repeat this description
        for S4,2 . There are four steps to follow:

        (1) Generate the 12 2-permutations of {1, 2, 3, 4}.
            12, 13, 14, 21, 23, 24, 31, 32, 34, 41, 42, 43
        (2) Revere their order.
            43, 42, 41, 34, 32, 31, 24, 23, 21, 14, 13, 12.
        (3) Reverse every item in the list.
            34, 24, 14, 43, 23, 13, 42, 32, 12, 41, 31, 21.
        (4) Partition the list of 12 items into four sublists of three elements each
            corresponding to the four substars.
            34 ≺ 24 ≺ 14 → S4,2 (4)
            43 ≺ 23 ≺ 13 → S4,2 (3)
            42 ≺ 32 ≺ 12 → S4,2 (2)
            41 ≺ 31 ≺ 21 → S4,2 (1)

            This ordering suggests the embedding shown in Fig. 7.10. It is important
        to note that in this embedding each column consists of processors with the
        same rank in their respective substar. For instance, processors 24, 23, 32
        and 31 have rank 2 in their substars.
May 7, 2022    11:14        Parallel Algorithms        9in x 6in     b4591-ch07                    page 301




                                            The Star Network                                 301


                                                  34   24    14


                                                  43   23    13


                                                  42   32    12


                                                  41   31    21


              Fig. 7.10.   Example of embedding a mesh into a (4, 2)-dimensional star.


           Now, we apply Algorithm shearsort of Section 4.6, which sorts the
        rows and columns alternately log d times. This is described in Algorithm
        stardksort.

          Algorithm 7.6 stardksort
                                     d!
          Input: A sequence of n = (d−k)! elements a1 , a2 , . . . , an  stored in (d, k)-star
                 Sd,k .
          Output: The sequence sorted in snakelike order.
              1. If k = 1 then sort the elements in Sd−k+1,1 = Kd−k+1 using a straight-
                 forward method.
              2. for i ← 1 to log d do
              3.     for j ← 1 to d do in parallel
              4.         Recursively sort Sd−1,k−1 (j) in forward direction if j is odd
                         and in reverse direction if j is even.
              5.         Sort the columns in upward direction.
              6.     end for
              7. end for



            In Step 1, the star reduces to a clique Sd−k+1,1 , which can be sorted
        using a sorting algorithm for the PRAM in time O(log(d − k + 1)) since
        all processors are connected. The inner loop from Step 3 to Step 6, which
        constitutes one of the log d phases of the algorithm, alternates between
        sorting the rows and sorting the columns. In sorting the rows, the elements
        in Sd−1,k−1 (j), 1 ≤ j ≤ d, are sorted recursively in parallel, where sorting
        is in the forward direction for odd j and in the reverse direction for even j.
            In algorithm shearsort, each column sorting is done in O(d) time.
        Since each edge of the mesh is mapped to a path of length O(d), each
        step of the mesh is simulated by O(d) steps of the star. It follows that the
May 7, 2022   11:14          Parallel Algorithms                9in x 6in               b4591-ch07               page 302




        302                                          Parallel Algorithms

        running time of each iteration of the algorithm is given by the recurrence

        t(d, k) = t(d − 1, k − 1) + O(d)O(d);                     t(d − k + 1, 1) = O(log(d − k + 1)),

        whose solution is

        t(d, k) = O(d2 ) + O((d − 1)2 ) + · · · + O((d − k + 1)2 ) + O(log(d − k + 1))
                  = O(kd2 ),

        since the depth of recursion is min{d, k} = k. Thus, the overall running
        time in all log d iterations of the algorithm is T (d, k) = O(kd2 log d).
            Unraveling recursion leads to a recursion tree similar to that for the
        (4, 3)-star shown in Fig. 7.11.
            As shown in the ﬁgure, sorting a (4, 3)-star induces four sorting instances
        of (3, 2)-stars, which in turn induce 12 sorting instances of (2, 1)-stars, i.e.,
        cliques. These cliques of size 2 are shown in Fig. 7.9 as dashed edges. In
        general, sorting a (d, k)-star induces d sorting instances of (d − 1, k − 1)-
        stars, which in turn induces (d − 1) sorting instances of (d − 2, k − 2)-stars
        and so on. This continues until the base of recursion is reached, in which
        (d−k+1)! instances of (d − k + 1, 1)-stars, i.e., cliques, are generated. Thus
            d!

        the problem of sorting reduces to sorting columns of stars of decreasing
        sizes. Since each step, e.g., element comparison, requires O(d) low-level
        routing steps, each column sorting takes O(d2 ) time (as explained above).

                                           234     134    324     124       314   214
                                           243     143    423     123       413   213
                                           342     142    432     132       412   312
                                           341     241    431     231       421   321



                                 4               3                      2                1


                        234 134            243 143                          342 142            341 241
                        324 124            423 123                          432 132            431 231
                        314 214            413 213                          412 312            421 321




                 234   324      314    243    423        413     342        432   412        341     431   421
                 134   124      214    143    123        213     142        132   312        241     231   321

                       Fig. 7.11.     Recursion tree for sorting on the (d, k)-star.
May 7, 2022   11:14     Parallel Algorithms      9in x 6in   b4591-ch07                  page 303




                                        The Star Network                          303

        This leads to a running time of

          O(d2 ) + O((d − 1)2 ) + · · · + O((d − k)2 ) + O(log(d − k + 1)) = O(kd2 )

        for each iteration, for an overall running time of O(kd2 log d), which matches
        the above derivation. Exercise 7.18 shows how to improve the running time
        to O(kd log d).


        7.11      Bibliographic Notes

        The star network was proposed by Akers, Harel and Krishnamurthy as an
        alternative to the hypercube [1]. A good introduction to the star network
        can be found in the book by Akl [5]. For more on the star graphs and the
        more general Cayley graphs, see for example, Akers and Krishnamurthy [2].
        See also Dietzfelbinger, Madhavapeddy and Sudborough [34]. Algorithms
        for optimal broadcasting in the star graph can be found in Mendia and
        Sarkar [65], and Sheu, Wu and Chen [85]. Arrangement graphs were intro-
        duced by Day and Tripathy [32] as a generalization of the star graphs. For
        routing, broadcasting, preﬁx sums, and sorting algorithms on the arrange-
        ment graph, see Li and Qiu [59]. The (n, k)-star graph was proposed by
        Chiang and Chen as a generalized star graph [23]. Topological properties
        of the (n, k)-star graph can be found in Chiang [22] and He [42]. Many
        algorithms for the star graph (see, e.g., Akl, Qiu and Stojmenovic [9]) may
        adapt to the (n, k)-star graph with slight modiﬁcations. For more references
        on the star network, see Akl [5].


        7.12      Exercises

         7.1. Given a permutation π = k1 k2 . . . kd , show how to compute its rank
              eﬃciently.

         7.2. Analyze the sequential running time of Algorithm starlabels.

         7.3. Analyze the parallel running time of Algorithm starlabels using
              the star graph as a model. That is, given a star graph of processors,
              how long does it take for the processors to know their ranks?
May 7, 2022   11:14     Parallel Algorithms          9in x 6in      b4591-ch07           page 304




        304                                   Parallel Algorithms


         7.4. Show the steps for data transfers from S3 (2) to S3 (3) in the star S4
              such that no two sources send to the same destination (see Fig. 7.3).

         7.5. Illustrate the operation of Algorithm starparprefix for computing
              parallel preﬁx on the star described in Section 7.4 to ﬁnd the preﬁx
              sums of 2, 1, 3, 1, 4, 2. Assume a 3-dimensional star with 6 processors.

         7.6. Illustrate the operation of Algorithm starmax for computing the
              maximum on the star described in Section 7.5 to ﬁnd the maximum
              of 3, 5, 8, 1, 5, 2. Assume a 3-dimensional star with 6 processors.

         7.7. Apply Algorithm starrecdub in Section 7.6 on the label 21435.

         7.8. Show that any neighborhood broadcasting algorithm on a network
              with degree d must require Ω(log d).

         7.9. Illustrate the operation of the algorithm for broadcasting in the star
              discussed in Section 7.7 to broadcast a datum initially stored in pro-
              cessor 2134 of the star S4 .

        7.10. Show that any broadcasting algorithm on a graph with n nodes must
              require time Ω(log n).

        7.11. Show that the arrangement graph A4,2 can be partitioned into cliques
              of size 3, i.e., triangles.

        7.12. Generalize the result of Exercise 7.11 for the arrangement graph Ad,k .
              That is, show that the arrangement graph Ad,k can be partitioned
              into cliques of size d − k + 1.

        7.13. Show that the (d, k)-star Sd,1 is a clique Kd .

        7.14. In Sd,k , given an arbitrary node u, show that there exists a cycle
              between u and all u’s 1-neighbors.

        7.15. Explain how to ﬁnd simple disjoint paths (linear arrays) of length d
              in the (d, k)-star.
May 7, 2022   11:14       Parallel Algorithms         9in x 6in           b4591-ch07         page 305




                                          The Star Network                             305

        7.16. Apply Exercise 7.15 on the embedding of S4,3 shown in Fig. 7.11 to
              obtain 6 disjoint paths (linear arrays) of length 4.

        7.17. Prove the correctness of your solution to Exercise 7.15.

        7.18. Use the result of Exercise 7.15 to improve the running time of the
              sorting algorithm presented in Section 7.10.

        7.19. A dominating set S in a graph G = (V, E) is a subset of V such that
              every element x ∈ V is in S or adjacent to an element y in S. Explain
              how to ﬁnd a dominating set of minimum size in the (d, k)-star.

        7.20. Apply Exercise 7.19 on the embedding of S4,3 shown in Fig. 7.11 to
              obtain 4 dominating sets of minimum size.

        7.21. Prove your answer to Exercise 7.19.


        7.13      Solutions

         7.1. Given a permutation π = k1 k2 . . . kd , show how to compute its rank
              eﬃciently.
                 Let permutation π = k1 k2 . . . kd be given. Then, its rank r(π) is given
                 by
                                                                  
                                       d                 d
                           r(π) = 1 +          |ki − i −       tl | × (i − 1)!,
                                                i=2               l=i+1

                 where ti = 1 if ki > kl , and 0 otherwise. This is shown in pseudocode
                 in Algorithm compstarrank. Its running time is computed as fol-
                 lows. Steps 5 and 7 are executed Θ(d) times each. Hence, the total
                 running time is Θ(d2 ).

         7.2. Analyze the sequential running time of Algorithm starlabels.
                 Each table entry takes constant time to produce. This implies a run-
                 ning time of Θ(d!). So, it is linear in the number of processors.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch07          page 306




        306                                     Parallel Algorithms


          Algorithm 7.7 compstarrank(Sd )
          Input: d-dimensional star Sd
          Output: Generate the ranks of Sd .
           1. for j ← 1 to d! do in parallel
           2.     r← 1
           3.     Let permutation πj = k1 k2 . . . kd
           4.     t← 1
           5.     for i ← 2 to d do
           6.         s ← ki − i
           7.         for l ← i + 1 to d do
           8.             if ki > kl then s ← s − 1
           9.         end for
          10.         t ← t × (i − 1)
          11.         r ← r+ | s | ×t
          12.     end for
          13. end for



         7.3. Analyze the parallel running time of Algorithm starlabels using
              the star graph as a model. That is, given a star graph of processors,
              how long does it take for the processors to know their ranks?
                 All columns of the table can be evaluated in parallel. Each column can
                 be evaluated sequentially in time Θ(d). This implies the running time
                 recurrence T (d) = T (d − 1) + Θ(d), whose solution is T (d) = Θ(d2 ).

         7.4. Show the steps for data transfers from S3 (2) to S3 (3) in the star S4
              such that no two sources send to the same destination (see Fig. 7.3).
                 Similar to Example 7.1.

         7.5. Illustrate the operation of Algorithm starparprefix for computing
              parallel preﬁx on the star described in Section 7.4 to ﬁnd the preﬁx
              sums of 2, 1, 3, 1, 4, 2. Assume a 3-dimensional star with 6 processors.
                 Similar to Example 7.2.

         7.6. Illustrate the operation of Algorithm starmax for computing the
              maximum on the star described in Section 7.5 to ﬁnd the maximum
              of 3, 5, 8, 1, 5, 2. Assume a 3-dimensional star with 6 processors.
                 Similar to Example 7.3.

         7.7. Apply Algorithm starrecdub in Section 7.6 on the label 21435.
                 Similar to Example 7.4.
May 7, 2022   11:14               Parallel Algorithms               9in x 6in          b4591-ch07                           page 307




                                                     The Star Network                                                 307

         7.8. Show that any neighborhood broadcasting algorithm on a network
              with degree d must require Ω(log d).
                 At each time unit, one processor with the messages can only send
                 to one of its neighbors, so after every step, the number of neighbors
                 which have received the information can at most double. The maxi-
                 mum number of neighbors of a node is d, so the least time to solve
                 the neighborhood broadcasting problem must be Ω(log d).

         7.9. Illustrate the operation of the algorithm for broadcasting in the star
              discussed in Section 7.7 to broadcast a datum initially stored in
              processor 2134 of the star S4 .
                 Similar to Example 7.6.

        7.10. Show that any broadcasting algorithm on a graph with n nodes must
              require time Ω(log n).
                 Note that after each time unit the number of processors that have
                 received the information being broadcast can at most double.

        7.11. Show that the arrangement graph A4,2 can be partitioned into cliques
              of size 3, i.e., triangles.
                                                                   |V (A4,2 )|
                 A4,2 can be partitioned into                          3         = 4 triangles in two ways as
                 shown in Fig. 7.12.


                                         42                                                 42


                               12               32                                12               32


                                  14           34        31                           14          34        31
                      13                                                    13
                                    24                                                 24


                43           23                     21        41       43        23                    21        41

                                         (a)                                                (b)

        Fig. 7.12.         Two partitions of the arrangement graph A4,2 into four triangles (dark
        edges).
May 7, 2022   11:14         Parallel Algorithms          9in x 6in          b4591-ch07                        page 308




        308                                       Parallel Algorithms

        7.12. Generalize the result of Exercise 7.11 for the arrangement graph Ad,k .
              That is, show that the arrangement graph Ad,k can be partitioned
              into cliques of size d − k + 1.
                 Let Id = {1, 2, . . . , d}, and for a ﬁxed i, 1 ≤ i ≤ k, let

                  Vi = {a1 . . . ai−1 bi ai+1 . . . ak | bi ∈ Id − {a1 , . . . , ai−1 , ai+1 , . . . ak }.

                 Then, |Vi | = d − k + 1. There are |Pk−1
                                                      d
                                                          | such Vi ’s, where Pk−1
                                                                               d
                                                                                   is the
                 number of permutations of d items taken k at a time. It is easy to
                 see that the subgraph induced by Vi is a complete graph Kd−k+1 . In
                 particular, Kd−k+1 = Kd if k = 1, and Kd−k+1 = K2 if k = d − 1.

        7.13. Show that the (d, k)-star Sd,1 is a clique Kd .
                 By Theorem 7.1 (All edges are 1-edges).

        7.14. In Sd,k , given an arbitrary node u, show that there exists a cycle
              between u and all u’s 1-neighbors.
                 Since Sd,k is node-symmetric, we may assume without loss of gen-
                 erality that u = 123 . . . k. Then, u is connected by a 1-edge to
                 (k+1)23 . . . k, which in turn is connected to (k+2)23 . . . k, and so on.
                 Hence, 123 . . . k, (k + 1)23 . . . k, (k + 2)23 . . . k, . . . , d23 . . . k, 123 . . . k
                 is a cycle in Sd,k .

        7.15. Explain how to ﬁnd simple disjoint paths (linear arrays) of length d
              in the (d, k)-star.
                 If we exchange the 1st symbol with the kth symbol in the
                 2-dimensional embedding, then the columns constitute simple paths
                 of length d. See Table 7.2 for example. For instance, one possible
                 simple path is, 42, 32, 23, 13.

                                                   Table 7.2.

                                          path 1      path 2     path 3

                                            43          42           41
                                            34          32           31
                                            24          23           21
                                            14          13           12
May 7, 2022   11:14           Parallel Algorithms            9in x 6in      b4591-ch07           page 309




                                               The Star Network                            309

                                                     Table 7.3.

                          path 1      path 2        path 3     path 4    path 5   path 6

                            432        431           423         421      413      412
                            342        341           324         321      314      312
                            243        241           234         231      214      213
                            143        142           134         132      124      123




        7.16. Apply Exercise 7.15 on the embedding of S4,3 shown in Fig. 7.11 to
              obtain 6 disjoint paths (linear arrays) of length 4.

                 From the ﬁrst column of Table 7.3, we obtain the path
                 432, 342, 243, 143. The other paths can be found similarly.

        7.17. Prove the correctness of your solution to Exercise 7.15.

                 Any column of the two dimensional embedding looks like the
                 following

                         α1 d
                      α2 (d − 1)
                      α3 (d − 2)
                           ..
                            .
                       αd−1 2
                         αd 1.

                 If we swap the ﬁrst and kth symbol, we obtain

                         dβ1
                      (d − 1)β2
                      (d − 2)β3
                          ..
                           .
                        2βd−1
                         1βd .
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch07              page 310




        310                                     Parallel Algorithms


                 Now, exchanging d in dβ1 with (d-1) yields (d − 1)β2 , exchanging
                 (d − 1) in dβ2 with (d − 2) yields (d − 2)β3 , and so on. Thus, dβ1 ,
                 (d − 1)β2 , . . . , 2βd−1 , 1βd is a simple path of length d − 1. In other
                 words, it represents d processors forming a linear array.

        7.18. Use the result of Exercise 7.15 to improve the running time of the
              sorting algorithm presented in Section 7.10.

                 Do the following steps.
                  (1) Preprocessing step: Before the algorithm starts, copy the con-
                      tents of every processor aαb to processor bαa.
                  (2) Sort as in Algorithm stardksort.
                  (3) Postprocessing step: After the sorting algorithm halts, copy
                      back the contents of every processor bαa to processor aαb.

                 Both the preprocessing and postprocessing steps take Θ(1) time. It
                 follows that performing the above procedure will reduce the time
                 complexity to O(kd log d), that is, it will be faster by a factor of
                 O(d), as the algorithm will work on columns of adjacent elements.

        7.19. A dominating set S in a graph G = (V, E) is a subset of V such that
              every element x ∈ V is in S or adjacent to an element y in S. Explain
              how to ﬁnd a dominating set of minimum size in the (d, k)-star.

                 If we exchange the 1st symbol with the kth symbol in the
                 2-dimensional embedding as in the previous exercises, then the rows
                 constitute dominating sets of minimum size. See Table 7.2. For exam-
                 ple, one possible dominating set of minimum size is, 43, 42, 41.

        7.20. Apply Exercise 7.19 on the embedding of S4,3 shown in Fig. 7.11 to
              obtain 4 dominating sets of minimum size.

                 From the ﬁrst row of Table 7.3, we obtain the dominating set
                 432, 431, 423, 421, 413, 412. The other dominating sets can be found
                 similarly.

        7.21. Prove your answer to Exercise 7.19.
                 First, note that all elements in the same row start with the same
                 symbol. This means that they form an independent set, that is, no
                 one is connected to the other. Next, each row of elements of the form
May 7, 2022   11:14       Parallel Algorithms      9in x 6in      b4591-ch07                   page 311




                                          The Star Network                              311

                 aα1 , aα2 , . . . , aα(d−1)! consists of all permutations of the symbols in
                 {1, 2, . . . , d} − {a} preﬁxed with the symbol a. Hence, if β is any
                 permutation that does not start with a, then it must be a neighbor of
                 one of these (d − 1)! permutations. This implies that this dominating
                 set is of minimum size.
                             B1948   Governing Asia




                      This page intentionally left blank




B1948_1-Aoki.indd 6                                        9/22/2014 4:24:57 PM
May 7, 2022   11:14    Parallel Algorithms       9in x 6in   b4591-ch08                 page 313




                                             Chapter 8


              Optical Transpose Interconnection
                       System (OTIS)



        8.1     Introduction

        When communication distances exceed a few millimeters, optical intercon-
        nects provide speed and power advantages over electronic interconnects.
        Therefore, in the construction of very large multiprocessor machines, it is
        prudent to interconnect physically close processors using electronic inter-
        connects and to use optical interconnects for pairs of processors that are
        distant. This led to the introduction of optical transpose interconnection
        system (OTIS). Speciﬁcally, in OTIS, there are n2 processors organized
        into n groups of n processors each. The intergroup interconnects are opti-
        cal, while the intragroup interconnects are electronic. It can be shown that
        when the number of groups equals the number of processors, the band-
        width and power eﬃciency are maximized, and system area and volume
        are minimized. Each processor is indexed by the pair (g, p), 0 ≤ g, p < n,
        where g is the group index, i.e., the group the processor is in, and p is the
        processor index within each group. Processor p in group g is connected to
        processor g in group p, 0 ≤ p, g < n. Every group can be realized as one of
        the well-studied interconnection networks, e.g., mesh, hypercube, butterﬂy,
        mesh of trees, and so forth. This results in OTIS-Mesh, OTIS-Hypercube,
        OTIS-butterﬂy, and so on.




                                                313
May 7, 2022   11:14     Parallel Algorithms          9in x 6in         b4591-ch08         page 314




        314                                   Parallel Algorithms

           Optical links have much larger bandwidth than electronic links do, and
        transfer times including latency are diﬀerent on optical and electronic links.
        Therefore, we will occasionally count communication along optical and elec-
        tronic interconnects separately. However, we use the simplifying assumption
        that any constant amount of data can be communicated over an optical link
        during an optical communication step, while only a unit amount of data
        can be communicated over an electronic communication step.


        8.2     The OTIS-Mesh

        The OTIS-Mesh consists of n groups of n processors each, where each group
                                √      √
        of processors forms a n × n mesh. Processor p in mesh (group) g is
        connected to processor g in mesh (group) p, 0 ≤ g, p < n. Figure 8.1
        shows an OTIS-Mesh with 4 meshes of 4 processors each for a total of 16
        processors. In this ﬁgure, the optical links are shown in thick lines. As shown
        in the ﬁgure, processor (00, 01) is connected to processor (01, 00), processor
        (01, 10) is connected to processor (10, 01), and so forth. Figure 8.2 shows an
        OTIS-Mesh with 9 meshes of 9 processors each for a total of 81 processors.


                        00                    01             00               01


                               group 00                           group 01


                        10                    11             10               11




                        00                    01             00               01


                               group 10                           group 11



                        10                    11             10               11

                             Fig. 8.1.    OTIS-Mesh with 16 processors.
May 7, 2022   11:14             Parallel Algorithms           9in x 6in        b4591-ch08                             page 315




                          Optical Transpose Interconnection System (OTIS)                                       315

        8.2.1      Data movements in the OTIS-Mesh
                                 √ √ √ √
        Consider embedding a n× n× n× n 4-dimensional mesh on the OTIS-
        Mesh. Corresponding to this embedding, the processors in the OTIS-Mesh
                                                                                √
        can be labeled by the quadruple (i, j, k, l), where 0 ≤ i, j, k, l ≤ n − 1.
                                       √
        Here, the group number is i n + j and the processor number in each group
             √
        is k n + l. Each move in the 4-D mesh can be simulated by at most three
        moves in the OTIS-Mesh as follows. The 4-D mesh moves (i, j, k ± 1, l) and
        (i, j, k, l±1) take one electronic move each, since they are local to the group.
        The 4-D mesh move (i ± 1, j, k, l) can be simulated by one electronic move


               00          01         02              00      01          02     00           01           02



              10          11        12           10          11        12       10           11        12



                   20     21        22           20         21         22      20            21        22

                        group 00                                 group 01                  group 02

                   00      01         02              00      01                00           01            02

                                                                      02

                   10     11        12           10          11        12            10     11        12



                   20     21        22           20         21         22            20     21        22

                        group 10                           group 11                        group 12
                   00      01         02              00      01          02         00      01            02



              10          11        12           10          11        12      10           11        12



              20          21        22           20         21         22      20           21        22

                        group 20                            group 21                      group 22

        Fig. 8.2.       OTIS-Mesh with 81 processors (only some of the optical links are
        shown).
May 7, 2022    11:14          Parallel Algorithms          9in x 6in      b4591-ch08         page 316




        316                                         Parallel Algorithms

        and two optical moves as follows
                                     o              e                  o
                        (i, j, k, l)−→ (k, l, i, j)−→ (k, l, i + 1, j)−→ (i + 1, j, k, l),

        where −→
               e
                   is an electronic move and −→ o
                                                    is an optical move. In some
        data movements, we will use the letters u, v, x and y to refer to the four
        dimensions of the 4-D mesh as well as its embedding on the OTIS-Mesh.
        Thus, each move along the x and y dimensions takes one step, and each
        move along the u and v dimensions takes three steps.

        8.2.2          Broadcasting in the OTIS-Mesh
        Assume the data is initially in the single processor (0, 0) (processor 0 in
        group 0), and it is to be broadcast to all processors in the OTIS-Mesh.
        The algorithm, shown as Algorithm otismeshbroadcast, consists of three
        steps. After Step 1, x is broadcast to all processors in group 0. Following
        Step 2, processor 0 of each group has a copy of x, and following Step 3,
        each processor in the OTIS-Mesh has a copy of x.

          Algorithm 8.1 otismeshbroadcast
          Input: x stored in processor (0, 0).
          Output: Broadcast x to all other processors.
              1. Processor (0, 0) broadcasts x to all processors in its group, group 0.
              2. Perform an OTIS move. That is, all processors in group 0 send their data
                 to processors in other groups using optical links (see Fig. 8.2).
              3. Processor (g, 0) in every group g broadcasts its data to all processors
                 within its group.


                                 √
            Steps 1 and 3 take 2( n−1) electronic moves each, and Step 2 takes one
                                   √
        OTIS move. The total is 4 n − 3 steps. The above discussion assumes that
        the origin of broadcasting is processor 0. Generalizing to other processors
        is straightforward.

        8.2.3          Semigroup operations on the OTIS-Mesh
        Consider performing semigroup operations, e.g., addition, on the OTIS-
        Mesh. Assume the operation of addition and that n numbers are dis-
        tributed one per processor. The 2-tuple index (g, p) of a processor may
        be transformed into a scalar i = gn + p with 0 ≤ i < n2 . Let xi be the
May 7, 2022    11:14       Parallel Algorithms     9in x 6in       b4591-ch08                   page 317




                        Optical Transpose Interconnection System (OTIS)                   317


        data stored in processor i, 0 ≤ i < n2 . Notice that the sum is to be stored
        in all processors of the OTIS-Mesh. The algorithm, shown as Algorithm
        otismeshsum, consists of three steps. After Step 1, the sum of all values
        in every group is computed and stored in all processors of that group. Fol-
        lowing Step 2, for all groups g, 0 ≤ g ≤ n − 1, processor (p, g) contains the
        sum of all elements in group p, and following Step 3, each processor in the
        OTIS-Mesh has a copy of the desired sum x0 + x1 + · · · + xn−1 .

          Algorithm 8.2 otismeshsum
          Input: xi stored in processor i, 0 ≤ i < n2 .
          Output: The sum of values xi stored in all processors.
              1. Each group performs the sum of its local data.
              2. Perform an OTIS move. That is, for all groups g and all processors p,
                 (g, p) sends the local sum in its group computed in Step 1 to processor
                 (p, g) using optical links.
              3. Each group computes the total of its local sums computed in Step 1.


                                √
           Steps 1 and 3 take 4( n − 1) electronic moves each (Exercise 4.3), and
                                                   √
        Step 2 takes one OTIS move. The total is 8( n − 1) electronic moves and
        one OTIS move.

        Example 8.1 Consider running Algorithm otismeshsum for ﬁnding the
        sum on the OTIS-Mesh with 16 processors (see Fig. 8.1). The contents of
        the processors will be represented by a set of four sets, each representing
        a group of processors. Suppose that initially the contents of the processors
        are
                                                                             
                        {1, 2, 1, 3}, {2, 4, 1, 3}, {1, 3, 2, 4}, {2, 4, 1, 2} .

        Following the ﬁrst step, computing the local sums, we obtain
                                                                                
                   {7, 7, 7, 7}, {10, 10, 10, 10}, {10, 10, 10, 10}, {9, 9, 9, 9} .

        After performing the OTIS move in Step 2, the contents become
                                                                                 
                    {7, 10, 10, 9}, {7, 10, 10, 9}, {7, 10, 10, 9}, {7, 10, 10, 9} .

        Finally, after performing the addition in Step 3, we obtain
                                                                                    
               {36, 36, 36, 36}, {36, 36, 36, 36}, {36, 36, 36, 36}, {36, 36, 36, 36} ,
May 7, 2022   11:14         Parallel Algorithms           9in x 6in     b4591-ch08            page 318




        318                                       Parallel Algorithms

        which is the desired sum. Note that all processors contain the ﬁnal
        sum.                                                             


        8.2.4         Parallel prefix in OTIS-Mesh
        The parallel preﬁx problem for the mesh was discussed in Section 4.4. In
        this section, we show how to compute it on the OTIS-Mesh. For simplicity,
        we will assume addition as the binary operation. The 2-tuple index (g, p)
        of a processor may be transformed into a scalar i = gn + p with 0 ≤ i < n2 .
        Let xi be the data stored in processor i. It is required to compute the preﬁx
        sums x0 , x0 +x1 , x0 +x1 +x2 , . . . , x0 +x1 +· · ·+xn−1 . The algorithm is shown
        as Algorithm otismeshparprefix. It consists of six phases. (Recall that
        processor p in group g is denoted by (g, p)). In Phase 1 of the algorithm,
        each group computes its local preﬁx sums. After Phase 2, the partial sum
        computed in processor (g, n − 1) is copied to processor (n − 1, g) for all
        g, 0 ≤ g ≤ n − 1. As a result, group n − 1 will hold the partial sums stored
        in all processors (g, n − 1). Let these sums be s0 , s1 , . . . , sn−1 . Phase 3
        computes the modiﬁed partial sums in group n − 1:

                                             
                                             p−1
                                      tp =         sj ,   0 ≤ p ≤ n − 1.
                                             j=0

        These valves are then copied to processors (g, n − 1) in Phase 4 for all
        g, 0 ≤ g ≤ n − 1. The data in processor (g, n − 1) is then broadcast to all
        processors in group g for all g, 0 ≤ g ≤ n − 1. Finally, in Phase 6, each
        processor (g, p) in the OTIS-Mesh adds the local preﬁx sum computed in
        Phase 1 and the modiﬁed preﬁx sum tg it received in Phase 5.
            The analysis of the algorithm is straightforward. Phases 1 and 3 take
           √                                                √
        Θ( n) steps each. Broadcasting in Phase 5 costs Θ( n) time and addition
        in Phase 6 takes Θ(1) time. Phases 2 and 4 take one OTIS move each. It
                                           √
        follows that the running time is Θ( n).


        Example 8.2 Consider ﬁnding the preﬁx sums in the OTIS-Mesh with 16
        processors (see Fig. 8.1). The contents of the processors will be represented
        by a set of four sets, each representing a group of processors. Initially, the
        contents of the processors are
                                                                              
                         {1, 2, 1, 3}, {2, 4, 1, 3}, {1, 3, 2, 4}, {2, 4, 1, 2} .
May 7, 2022    11:14          Parallel Algorithms          9in x 6in         b4591-ch08                        page 319




                          Optical Transpose Interconnection System (OTIS)                               319


          Algorithm 8.3 otismeshparprefix
          Input: xi stored in processor i, 0 ≤ i < n2 .
          Output: Compute the prefix sums of the xi ’s.
              1. Perform a local prefix sum within each group as discussed in Section 4.4.
              2. Perform an OTIS move of the prefix sums computed in Phase 1 for all
                 processors (g, n − 1). That is, for all groups g, copy the contents of pro-
                 cessor (g, n − 1) to processor (n − 1, g) in group n − 1. Call these sums
                 s0 , s1 , . . . , sn−1 .
              3. Group n − 1 computes a modified prefix sum of the values sj received in
                 Phase2. In this modification, processor (n − 1, p) computes
                 tp = p−1    j=0 sj , 0 ≤ p ≤ n − 1.
              4. Perform an OTIS move of the modified prefix sums computed in Phase 3.
                 That is, for all groups g, copy tg that is computed in the previous phase
                 to processor (g, n − 1) in group g.
              5. Each group g performs a local broadcast of the modified prefix sum tg
                 received by its processor (g, n − 1).
              6. Each processor (g, p) in the OTIS-Mesh adds the local prefix sum com-
                 puted in Phase 1 and the modified prefix sum tg it received in Phase 5.



        After computing the local preﬁx sums in Phase 1, we obtain
                                                                            
                     {1, 3, 4, 7}, {2, 6, 7, 10}, {1, 4, 6, 10}, {2, 6, 7, 9} .

        The contents after Phase 2 change as follows
                                                                                          
             {1, 3, 4, 7}, {2, 6, 7, 10}, {1, 4, 6, 10}, {(2, 7), (6, 10), (7, 10), (9, 9)} .

        Here, the fourth group consists of pairs of values; the ﬁrst value in each
        group-processor pair (3, p) is the local preﬁx sum computed earlier and the
        second is the preﬁx sum sp of group p received in Phase 2. So, in group 3,
        s0 = 7, s1 = 10, s2 = 10 and s3 = 9. Following Phase 3, the contents become
                                                                                           
              {1, 3, 4, 7}, {2, 6, 7, 10}, {1, 4, 6, 10}, {(2, 0), (6, 7), (7, 17), (9, 27)} .
        Here, 0, 7, 17, 27 are the modiﬁed preﬁx sums tp of 7, 10, 10, 9 stored in
        group 3. Following Phase 4, the contents of the processors become
                                                                                                         
            {1, 3, 4, (7, 0)}, {2, 6, 7, (10, 7)}, {1, 4, 6, (10, 17)}, {(2, 0), (6, 7), (7, 17), (9, 27)} ,

        that is, the contents of group 3 are copied to processors (g, 3) in all groups g.
        After broadcasting in Phase 5, the contents are represented by the pairs
                   
                    {(1, 0), (3, 0), (4, 0), (7, 0)}, {(2, 7), (6, 7), (7, 7), (10, 7)},
May 7, 2022   11:14         Parallel Algorithms          9in x 6in      b4591-ch08                  page 320




        320                                       Parallel Algorithms

                                                                                           
                {(1, 17), (4, 17), (6, 17), (10, 17)}, {(2, 27), (6, 27), (7, 27), (9, 27)} .

        Finally, after addition in Phase 6, the contents in all processors become
                                                                                   
                   {1, 3, 4, 7}, {9, 13, 14, 17}, {18, 21, 23, 27}, {29, 33, 34, 36} ,

        which are the desired preﬁx sums, as can be seen by inspection.                         

        8.2.5         Shift operations on the OTIS-Mesh
        In this operation, data in all groups is shifted to the right (or left) along one
                                                      √         √
        of its coordinates by k positions, where − n < k < n. For example, data
        is shifted from (u, v, x, y) to (u + k, v, x, y) along coordinate u, or shifted
        from (u, v, x, y) to (u, v, x+k, y) along coordinate x. Here, we have assumed
        that the processors are labeled by the quadruple (u, v, x, y). Shifting along
        coordinates x and y is straightforward, as it is a standard mesh operation.
        Hence, we will concentrate on shifting along coordinates u and v. Algorithm
        otismeshshift describes how to shift along coordinate u or along coordi-
        nate v. In Step 1, for all groups g, 0 ≤ g ≤ n − 1, processor (g, p) copies
        its element to processor (p, g), and following Step 2, the elements in each
        group are shifted. Shifting is performed along x coordinate if the original
        shifting is by u, and is done along y coordinate if the original shifting is
        by v. Finally, after Step 3, for all groups g, 0 ≤ g ≤ n − 1, processor (g, p)
        copies its element to processor (p, g).
            Steps 1 and 3 take one OTIS move each, and Step 2 takes k electronic
        moves.

          Algorithm 8.4 otismeshshift
                                                                    √       √
          Input: n2 elements stored in OTIS-Mesh and an integer k, − n < k < n.
          Output: The elements are shifted to the right or the bottom by k positions
                  along the u or v coordinates.
              1. Perform an OTIS move. That is, for all groups g and all processors p,
                 (g, p) sends its element to processor (p, g) using optical links.
              2. Each group shifts its local data along coordinate x or y row-wise or
                 column-wise.
              3. Perform an OTIS move as in Step 1.



        Example 8.3       Consider running Algorithm otismeshshift for shifting
        the elements in the OTIS-Mesh with 16 processors one element to the right
May 7, 2022   11:14             Parallel Algorithms           9in x 6in        b4591-ch08           page 321




                           Optical Transpose Interconnection System (OTIS)                    321


        along the u-coordinate (see Fig. 8.1). The contents of the processors will be
        represented by a set of four sets, each representing a group of processors.
        Suppose that initially the contents of the processors are
                                                                                      
                          {1, 2, 3, 4}, {5, 6, 7, 8}, {9, 10, 11, 12}, {13, 14, 15, 16} .

        It is important to note that each set of four numbers constitutes a 2 × 2
        mesh. Following Step 1, we obtain
                                                                                      
                          {1, 5, 9, 13}, {2, 6, 10, 14}, {3, 7, 11, 15}, {4, 8, 12, 16} .

        After Step 2, the contents of the processors become
                                                                                       
                               {0, 1, 0, 9}, {0, 2, 0, 10}, {0, 3, 0, 11}, {0, 4, 0, 12} .

        Finally, following Step 3, we obtain
                                                                                       
                               {0, 0, 0, 0}, {1, 2, 3, 4}, {0, 0, 0, 0}, {9, 10, 11, 12} ,

        which is the desired result.                                                           

        Example 8.4       Consider running Algorithm otismeshshift — for shift-
        ing the elements in the OTIS-Mesh with 16 processors — one element to
        the right along the v-coordinate (see Fig. 8.1). Suppose that initially the
        contents of the processors are
                                                                                      
                          {1, 2, 3, 4}, {5, 6, 7, 8}, {9, 10, 11, 12}, {13, 14, 15, 16} .

        Following Step 1, we obtain
                                                                                      
                          {1, 5, 9, 13}, {2, 6, 10, 14}, {3, 7, 11, 15}, {4, 8, 12, 16} .

        After Step 2, the contents of the processors become
                                                                                        
                                   {0, 0, 1, 5}, {0, 0, 2, 6}, {0, 0, 3, 7}, {0, 0, 4, 8} .

        Finally, following Step 3, we obtain
                                                                                        
                                   {0, 0, 0, 0}, {0, 0, 0, 0}, {1, 2, 3, 4}, {5, 6, 7, 8} ,

        which is the desired result.                                                           
May 7, 2022   11:14         Parallel Algorithms          9in x 6in      b4591-ch08                 page 322




        322                                       Parallel Algorithms

        8.2.6         Permutation routing in OTIS-Mesh
        We consider the problem of permutation routing in the OTIS-Mesh with n2
        processors, in which every processor tries to send to a diﬀerent destination.
        We will denote a processor Pk,l in group (i, j) by the quadruple (i, j, k, l).


        8.2.6.1       Deterministic routing in the OTIS-Mesh
        The greedy algorithm for permutation routing in the OTIS-Mesh is a gen-
        eralization of the greedy algorithm for the 2-dimensional mesh discussed
        in Section 4.9.2. Let π be a packet to be routed from processor (u, v, x, y)
        to processor (u , v  , x , y  ). The greedy algorithm consists of the following
        phases.

        Phase 1: Route π from processor (u, v, x, y) to processor (u, v, u , v  ) as
                 detailed in Section 4.9.2.
        Phase 2: Send π from processor (u, v, u , v  ) to processor (u , v  , u, v) using
                 one optical move.
        Phase 3: Route π from processor (u , v  , u, v) to processor (u , v  , x , y  ) as
                 detailed in Section 4.9.2.
                                                 √
            Phases 1 and 3 take at most 2 n − 2 steps each, and Phase 2 takes
                                             √
        one step for a total of at most 4 n − 3 steps. However, many packets may
        pile up at intermediate processors. Moreover, many packets may pile up at
        processor (u, v, u , v  ) after Phase 1; in the worst case all processors from
        group (u, v) may want to send to all processors in group (u , v  ). In this
        case, Θ(n) packets may accumulate at processor (u, v, u , v  ) after Phase 1.
        This implies that the delay, and hence the total number of steps, is O(n).


        8.2.6.2       Randomized routing in the OTIS-Mesh
        As pointed out in the previous section, the greedy algorithm may result in
        large queue sizes and hence large delays. This renders the greedy algorithm
        impractical. To circumvent this diﬃculty, we use randomization. One pos-
        sibility is to use the randomized algorithm of Section 4.9.3 twice for both
        phases 1 and 3 of the greedy algorithm of the previous section. However,
        this will not prevent the accumulation of packets at intermediate proces-
        sors, which may result from many packets sent from one particular group
        to another group. Hence, we use randomization to ﬁrst send the packets to
May 7, 2022   11:14         Parallel Algorithms         9in x 6in         b4591-ch08                        page 323




                        Optical Transpose Interconnection System (OTIS)                             323

        random locations within the OTIS-Mesh. The proposed randomized algo-
        rithm consists of the following four phases. Recall that the source packet is
        π at (u, v, x, y) and is destined to (u , v  , x , y  ).

        Phase 1: Route π to a random processor (u , v  , x , y  ). It ﬁrst chooses u
                 randomly and moves to (u , v, x, y). It then chooses v  randomly
                 and moves to (u , v  , x, y), then chooses x randomly and moves
                 to (u , v  , x , y) and ﬁnally chooses y  randomly and moves to
                 (u , v  , x , y  ). Traversing in the u, v dimensions can be converted
                 to traversals in the x, y dimensions as follows. First, π moves
                 from the source processor (u, v, x, y) to processor (u, v, u , v)
                 along the x-dimension, then moves along the transpose connec-
                 tion to (u , v, u, v), and ﬁnally in the (u , v) group to (u , v, x, y).
                 Similarly, it then moves from (u , v, x, y) to (u , v  , x, y). Next,
                 it moves in group (u , v  ) to (u , v  , x , y  ) ﬁrst on the x-
                 dimension and then on the y-dimension. It can be shown using
                 an analysis similar to that in Section 4.9.3 that this takes at
                             √              √
                 most 4 n + o( n) steps using queues of size O(log n) with high
                 probability.
        Phase 2: Route π from processor (u , v  , x , y  ) to processor (u , v  , u , v  ).
                 This can be done by traveling ﬁrst along the x-dimension and
                 then along the y-dimension. It can be shown that the delay is
                   √                                                         √
                 o( n). Since the distance traveled is at most n, routing along
                                                             √       √
                 the x-dimension takes at most n + o( n) with high probability.
                                                                                   √           √
                 Similarly, routing along the y-direction takes at most n+o( n).
                 Hence, this data movement, which is local to group (u , v  ), takes
                                  √            √
                 at most 2 n + o( n) steps with high probability.
        Phase 3: Send π from processor (u , v  , u , v  ) to processor (u , v  , u , v  )
                 using one optical move.
        Phase 4: Route π from processor (u , v  , u , v  ) to the destination processor
                 (u , v  , x , y  ). It does this by traveling ﬁrst along the x-dimension
                 and then along the y-dimension. As in Phase 2, this involves rout-
                                                                              √        √
                 ing local to group (u , v  ), and it takes at most 2 n + o( n) steps
                 with high probability.

           In all phases, the farthest-destination-ﬁrst priority scheme is employed.
                                         √       √
        The total number of steps is 8 n + o( n) using queues of size O(log n)
        with high probability.
May 7, 2022   11:14         Parallel Algorithms          9in x 6in        b4591-ch08    page 324




        324                                       Parallel Algorithms

        8.2.7         Sorting on OTIS-Mesh
        Sorting on the OTIS-Mesh can be achieved by simulating an algorithm for
        sorting on the 4-dimensional mesh. It is not diﬃcult to extend the algorithm
        for sorting on the 3-dimensional mesh described in Section 4.17.1 so that it
                                                   √
        runs on the 4-dimensional mesh using O( n) steps. The algorithm for the
        3-dimensional mesh can be generalized to 4-dimensions by replacing planes
                                                     √
        by 3-dimensional meshes with side length n. Sorting in planes becomes
        sorting in 3-dimensional meshes as described in Section 4.17.1. The result
                                        √
        is an algorithm that runs in O( n) steps. (See the Bibliographic notes for
        more discussion of sorting on the 4-dimensional mesh and the OTIS-Mesh).


        8.3     The OTIS-Hypercube

        The OTIS-Hypercube consists of n = 2d groups of n processors each for a
        total of n2 processors, where each group of processors forms a conventional
        hypercube with n = 2d . Processor p in hypercube (group) g is connected
        to processor g in hypercube (group) p, 0 ≤ g, p < n. Figure 8.3 shows
        an OTIS-Hypercube with 8 hypercubes of 8 processors each for a total
        of 64 processors. The thick lines represent optical links and the thin lines
        represent connections within the hypercubes. The number below each group
        is its number. For clarity, only some of the optical connections are shown
        in the ﬁgure.
            As shown in the ﬁgure, processor (000, 010) is connected to processor
        (010, 000), processor (101, 111) is connected to processor (111, 101), and so
        forth. Each processor in the OTIS-Hypercube has degree d + 1; there are d
        connections to other processors in its group as well as one optical link. The
        diameter of the OTIS-Hypercube is 2d + 1; the shortest distance between
        processors (0, 0) and (n − 1, n − 1) is 2d + 1 (Exercise 8.10).


        8.3.1         Simulation of an n2 -processor hypercube
        An n2 -processor OTIS-Hypercube can simulate a regular n2 -processor
        hypercube. A processor (g, p) of the OTIS-Hypercube can be represented
        by the 2d bits

                                    gd−1 gd−2 . . . , g0 pd−1 pd−2 . . . p0 ,
May 7, 2022   11:14               Parallel Algorithms                    9in x 6in              b4591-ch08                              page 325




                            Optical Transpose Interconnection System (OTIS)                                                       325

                                                                   101                                      100             101
                                                      100
                                                             001                                                      001
                                                000                                                 000

                                                                   111                                                      111
                                                      110
                                                                                                                110
                                                010                                           101
                            100                              011                 100                  010             011
                                         101
                                   001                       100           000          001                 101
                      000

                                          111                                                 111
                            110                                                  110

                      010                                                 010
                                   011                                                  011
                                  000                                             001
                                                            100          101
                                                                   001                                    100               101
                                                      000
                                                                                                                      001
                                                                         111                        000
                                                            110
                                                      010                                                 110               111
                                                                   011
                                                                                                    010               011
                            100           101                110                              101
                                                                                 100
                                    001
                                                                                                          111
                      000                                                               001
                                                                           000
                                          111                                                 111
                            110                                                  110
                      010
                                   011                                     010          011
                             010                                                  011
        Fig. 8.3. OTIS-Hypercube with 64 processors. Only some of the optical links
        are shown.



        where gd−1 gd−2 . . . g0 is the group address and pd−1 pd−2 . . . p0 is the local
        processor address. A hypercube move moves data from processor with
        label q to processor q (k) , where q (k) is obtained from the binary represen-
        tation of q by complementing the kth bit. When k is in the lower half, the
        move is done in the hypercube by a local intragroup hypercube move. When
        k is in the upper half, the move is done in the group using the following
        steps.


                                           (gd−1 gd−2 . . . , g0 pd−1 pd−2 . . . p0 )
                                   −→
                                    o
                                      (pd−1 pd−2 . . . , p0 gd−1 gd−2 . . . gj . . . g0 )
                                   −→
                                    e
                                      (pd−1 pd−2 . . . , p0 gd−1 gd−2 . . . gj . . . g0 )
                                   −→
                                    o
                                      (gd−1 gd−2 . . . gj . . . g0 pd−1 pd−2 . . . , p0 ),
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch08       page 326




        326                                      Parallel Algorithms

        Here, −→
               e
                   is an electronic move and −→
                                              o
                                                 is an optical move. The foregoing
        discussion proves the following theorem.


        Theorem 8.1        An n2 -processor OTIS-Hypercube can simulate an
         2
        n -processor hypercube with a slowdown factor of at most 3.

            Although the OTIS-Hypercube has many attractive properties, it suﬀers
        from having limited optical connections between the diﬀerent groups. When
        data is to be transferred between two diﬀerent groups, it creates a conges-
        tion problem to most of the paths that have to pass through this optical link
        because only one optical link connects two diﬀerent groups. However, the
        hardware cost of the OTIS-Hypercube is mush less than that of the hyper-
        cube. To see this, consider comparing an n2 -processor OTIS-Hypercube
        with a hypercube with the same number of processors. A hypercube with n2
        processors has dn2 links, while an n2 -processor OTIS-Hypercube has n2
        hypercubes each with (1/2)dn internal links, and (n/2)(n−1) links between
        groups for a total of (1/2)dn2 + (1/2)(n2 − n) links. This means a reduction
        in the number of links by a factor of almost 2.


        8.3.2         Broadcasting in the OTIS-Hypercube
        The algorithm for broadcasting in the OTIS-Hypercube is similar to that
        for the OTIS-Mesh discussed in Section 8.2.2, and outlined in Algorithm
        otismeshbroadcast. Steps 1 and 3 of Algorithm otismeshbroadcast
        when adapted for the OTIS-Hypercube take log n electronic moves each,
        and Step 2 takes one OTIS move. The total is 2 log n + 1 steps.


        8.3.3         Semigroup operations on the OTIS-Hypercube
        The algorithm for semigroup operations in the OTIS-Hypercube is similar
        to that for the OTIS-Mesh discussed in Section 8.2.3, and the algorithm for
        addition is similar to Algorithm otismeshsum. Steps 1 and 3 of Algorithm
        otismeshsum — when adapted for the OTIS-Hypercube — take log n elec-
        tronic moves each, and Step 2 takes one OTIS move. The total is 2 log n
        electronic moves and one OTIS move.
            Alternatively, we may use the technique of reduction as outlined in
        Algorithm otishcaddition. In this algorithm, the data in each group g is
        ﬁrst added, and the sum is stored in processor (g, 0). The contents of all
        processors (g, 0) are then transferred to group 0 using one optical move.
May 7, 2022    11:14         Parallel Algorithms    9in x 6in      b4591-ch08                   page 327




                          Optical Transpose Interconnection System (OTIS)                 327

        Finally, group 0 computes the sum of the contents in all its processors
        and stores the result in processor (0, 0). Notice that the sum is stored in
        processor (0, 0) only. The analysis of the algorithm is similar to that of the
        previous algorithm, that is, the running time is 2 log n electronic moves and
        one optical move.

          Algorithm 8.5 otishcaddition
          Input: xi stored in processor i, 0 ≤ i < n2 .
          Output: The sum of values xi in all processors.
              1. Each group g performs addition of its local data and stores the sum in
                 processor (g, 0) of group 0.
              2. Each group g moves the content of its processor (g, 0) to processor (0, g).
              3. Group 0 performs addition of the sums computed in Step 1 and stores
                 the total in processor (0, 0).



        Example 8.5 Consider running Algorithm otishcaddition for ﬁnding
        the sum on the OTIS-Hypercube with 16 processors (see Fig. 8.1). The
        contents of the processors will be represented by a set of four sets, each
        representing a group of processors. Suppose that initially the contents of
        the processors are
                                                                             
                        {1, 2, 1, 3}, {2, 4, 1, 3}, {1, 3, 2, 4}, {2, 4, 1, 2} .

        Following the ﬁrst step, computing the local sums, we obtain
                                                                             
                      {7, x, x, x}, {10, x, x, x}, {10, x, x, x}, {9, x, x, x} ,

        where x stands for anything. After performing the OTIS move in Step 3,
        the contents become
                                                                             
                    {7, 10, 10, 9}, {10, x, x, x}, {10, x, x, x}, {9, x, x, x} ,

        Finally, after performing the addition in Step 3, we obtain
                                                                              
                      {36, x, x, x}, {10, x, x, x}, {10, x, x, x}, {9, x, x, x} ,          

        8.3.4          Sorting and routing in the OTIS-Hypercube
        Theorem 8.1 can be employed to simulate sorting and routing on the OTIS-
        Hypercube. For sorting, Algorithm samplesort discussed    in Section
                                                                           3.10
                                                                  n     n
        for the hypercube can be used. The running time will be Θ p log p , where
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch08       page 328




        328                                      Parallel Algorithms

        p is the number of processors and n is the number of elements. Here, we
        have assumed that the number of processors is less than the number of
        elements.
            Alternatively, Algorithm bfoddevenmergesort for odd–even sorting
        on the butterﬂy network discussed in Section 3.9 can also be used, since it
        is a normal butterﬂy algorithm (see deﬁnition of normal butterﬂy algorithm
        in Section 3.2). The running time will be Θ(log2 n).
            The problem of routing in the OTIS-Hypercube can be solved by simu-
        lating the randomized algorithm for routing in the hypercube discussed in
        Section 3.6.2. When adapting this algorithm on an n2 -hypercube, its run-
        ning time becomes at most 8 × 2d = 16d steps with high probability. Hence,
        by Theorem 8.1, the running time on the OTIS-Hypercube will be at most
        3 × 16d = 48d = Θ(log n) steps with high probability.


        8.4     Other OTIS Networks

        8.4.1         The OTIS-Star
        The OTIS-Star consists of n = d! groups of n processors each, where each
        group of processors forms a d-dimensional star. Processor p in star (group) g
        is connected to processor g in star (group) p, 0 ≤ g, p < n. Figure 8.4 shows
        an OTIS-Star with 6 stars of 6 processors each for a total of 36 proces-
        sors. As shown in the ﬁgure, processor (123, 213) is connected to processor
        (213, 123), processor (123, 132) is connected to processor (132, 123), and so
        forth. The diameter of the OTIS-Star is 23(d − 1)/2 + 1 = Θ(d), and its
        degree is d, which are sublogarithmic in terms of the number of processors
        (Notice that d < log(d!)2 = Θ(d log d)).


        8.4.2         The OTIS-MOT
        The OTIS-MOT consists of n groups of n processors each, where each
        group of processors forms a mesh of trees. In this construction, it is more
        convenient to use a slightly diﬀerent model of mesh of trees than the one
        described in Section 6.3; see Fig. 8.5 for an example. In this ﬁgure, each
        node is labeled with the row and column numbers. Here, the processors
        in each row are connected to form a binary tree, and the processors in
        each column are connected to form a binary tree. The roots of these binary
        trees are the processors in the leftmost column and topmost row. The total
May 7, 2022   11:14            Parallel Algorithms               9in x 6in          b4591-ch08               page 329




                            Optical Transpose Interconnection System (OTIS)                            329


                                 312                                                    231


                      132                    213                              321                132
                                 213                                                    321
                                                                123
                      231                    123                              123                312

                                               213                           321
                                 321                                                    213
                                                                123
                                 213           312                           231        321


                      312                    123          132                123                 231
                                312                                                     231
                      132                    321          123                 213                132

                                               213                           321
                                 231                            132                     312


                                               312                           231


                                                                132

        Fig. 8.4.       OTIS-Star with 36 processors (only some of the optical links are
        shown).


                                    00               01                02             03




                                    10               11                12             13




                                    20               21                22             23




                                    30               31                32             33


                                 Fig. 8.5.    Mesh of trees with 16 processors.
May 7, 2022   11:14          Parallel Algorithms           9in x 6in         b4591-ch08            page 330




        330                                        Parallel Algorithms


                      00      01                   00     01            02   00      01       02
                                       02

                      10      11            12     10     11            12   10      11       12



                      20      21        22         20     21        22       20      21       22

                            group 00                           group 01            group 02
                       00       01          02     00     01                 00      01       02
                                                                   02


                      10      11            12     10     11            12   10      11       12



                      20      21        22         20     21        22       20      21       22

                            group 10                    group 11                   group 12
                       00     01            02     00     01        02        00     01       02


                      10      11            12     10     11            12   10      11       12



                      20      21        22         20     21        22       20      21       22

                            group 20                    group 21                   group 22

        Fig. 8.6.      OTIS-MOT with 81 processors (only some of the optical links are
        shown).


        number of processors in this model is n, which is the same as that of the
        √      √
          n × n mesh.
           Processor p in mesh of trees (group) g is connected to processor g in
        mesh of trees (group) p, 0 ≤ g, p < n. Figure 8.6 shows an OTIS-MOT
        with 9 groups of 9 processors each for a total of 81 processors. As shown in
        the ﬁgure, processor (00, 01) is connected to processor (01, 00), processor
        (00, 22) is connected to processor (22, 00), and so forth.


        8.5     Bibliographic Notes

        Optical transpose interconnection system (OTIS) technologies have been
        proposed and investigated in Marsden, Marchand, Harvey and Esener [63],
May 7, 2022   11:14      Parallel Algorithms       9in x 6in       b4591-ch08            page 331




                      Optical Transpose Interconnection System (OTIS)              331


        Szymanski [90] and Szymanski and Hinton [91]. Topological properties of
        OTIS networks were studied in Day and Al-Ayyoub [31]. A number of
        algorithms have been developed for the OTIS networks such as Akhgari,
        Ziaie and Ghodsi [3], Gupta, Singh and Bhati [41], Gupta and Sarkar [40],
        Lucas and Jana [60], Najaf-abadi and Sarbazi-azad [70], Osterloh [73],
        Rajasekaran and Sahni [76], Wang and Sahni ([95], [96], [97]) and Zane,
        Marchand, Pahuri and Esener [103]. Algorithms for basic operations on the
        OTIS-mesh can be found in Wang and Sahni [95]. The randomized routing
        algorithm for the OTIS-mesh in Section 8.2.6.2 is a modiﬁcation of an algo-
                                                                √
        rithm presented in Rajasekaran and Sahni [76]. An O( n) time algorithm
        for sorting can be obtained by simulating an algorithm with similar com-
        plexity for the 4-dimensional mesh as suggested in Section 8.2.7. Another
        possibility is to use Kunde’s sorting algorithm for the 4-dimensional mesh
        in Kunde [47]. The sorting algorithm presented in this paper is for the more
        general r-dimensional meshes, r ≥ 3. There are some complex algorithms
        for sorting on the OTIS-mesh. The deterministic algorithm in Wang and
                                                    √
        Sahni [95] for the OTIS-mesh runs in O( n) steps, and the randomized
                                                                    √
        algorithm in Rajasekaran and Sahni [76] runs in time O( n) with high
        probability. Algorithms for the OTIS-hypercube have been developed in
        Sahni and Wang [80] and Wang and Sahni [94]. A comparative evalua-
        tion of adaptive and deterministic routing in the OTIS-hypercubes appears
        in Najaf-abadi and Sarbazi-azad [69]. Theorem 8.1 can be found in Zane,
        Marchand, Pahuri and Esener [103]. The OTIS-Star was studied by Awwad
        [13], Al-Sadi, Awwad, and AlBdaiwi [10] and Awwad and Al-Sadi [14]. Sort-
        ing and routing on OTIS-Mesh of trees can be found in Lucas and Jana
        [60] and Lucas and Jana [61]. Algorithms for OTIS-Hyper Hexa-Cell can
        be found in Gupta and Sarkar [40] and Gupta, Singh and Bhati [41] (For
        deﬁnitions of the hyper hexa-cell and the OTIS-Hyper Hexa-Cell, see also
        Mahafzah, Hamad, Ahmad and Abu-Kabeer [62]).


        8.6     Exercises

         8.1. Suppose we want to move data from processor p1 in group g1 to
              processor p2 in group g2 . One possibility is to use the sequence of
              moves:
                                          ∗                        ∗
                                         e            o            e
                              (g1 , p1 )−→ (g1 , g2 )−→ (g2 , g1 )−→ (g2 , p2 ),
May 7, 2022   11:14      Parallel Algorithms          9in x 6in      b4591-ch08          page 332




        332                                    Parallel Algorithms

                                                             ∗
                 where −→
                        o
                            is an optical move and −→
                                                    e
                                                       stands for a sequence of zero
                 or more electronic moves. Suggest another sequence of moves.

         8.2. Is Algorithm otismeshbroadcast discussed in Section 8.2.2 for
              broadcasting in the OTIS-Mesh optimal? Justify your answer.

         8.3. In a window broadcast, we start with data in the top left w × w
                                                        √                       √
              submesh of a single group g, where w | n, that is w divides n.
              Following the window broadcast operation, the initial w × w window
              tiles the entire OTIS-Mesh. (See Exercise 4.19). Outline an algorithm
              to implement this operation. What is the running time of your algo-
              rithm?

         8.4. Apply the addition algorithm for the OTIS-Mesh discussed in Sec-
              tion 8.2.3 on the input
                                                                                
                           {1, 3, 2, 4}, {2, 1, 5, 1}, {2, 4, 5, 1}, {2, 1, 3, 2} .

         8.5. Explain how to implement Step 1 in Algorithm otismeshsum of
              Section 8.2.3 eﬃciently.

         8.6. Apply the parallel preﬁx algorithm for the OTIS-Mesh discussed in
              Section 8.2.4 on the input
                                                                                
                           {2, 4, 2, 1}, {1, 3, 1, 4}, {5, 2, 1, 3}, {2, 3, 5, 1} .

         8.7. Illustrate the operation of Algorithm otismeshshift in Section 8.2.5
              given the processor contents
                                                                                   
                        {3, 8, 1, 14}, {2, 13, 5, 11}, {9, 7, 10, 6}, {4, 15, 12, 8} .

                 Assume an OTIS-Mesh with 16 processors, and the elements are to
                 be shifted along the u-coordinate.

         8.8. Repeat Exercise 8.7 with shifting the elements along the v-coordinate
              instead.

         8.9. Modify the algorithm for randomized routing in the OTIS-Mesh dis-
                                                                 √      √
              cussed in Section 8.2.6.2 so that it runs in time 4 n + o( n).

        8.10. Verify that the diameter of the OTIS-Hypercube is 2d + 1.
May 7, 2022   11:14       Parallel Algorithms   9in x 6in     b4591-ch08                 page 333




                       Optical Transpose Interconnection System (OTIS)            333

        8.11. Analyze the running time of the algorithm in Section 8.3.2 for broad-
              casting in the OTIS-Hypercube by simulating an n2 -hypercube.

        8.12. Analyze the running time of the algorithm in Section 8.3.3 for addi-
              tion in the OTIS-Hypercube by simulating an n2 -hypercube.

        8.13. Suppose that each of the n groups in an OTIS-Hypercube has a
              datum in an arbitrary processor. Give an algorithm to collect these n
              data items in a speciﬁed group k so that group k will have one item
              per processor.

        8.14. Illustrate your solution to Exercise 8.13 given the processor contents
                                                                                
                           {x, x, 1, x}, {2, x, x, x}, {x, x, x, 3}, {x, x, 4, x} ,

                 which are to be sent to group 2. Assume an OTIS-Hypercube with
                 four groups. Here, x stands for anything, and the contents of the
                 processors are represented by a set of four sets, each representing a
                 group of processors.

        8.15. Suppose that group k in an OTIS-Hypercube has n data items
              located one per processor. Give an algorithm to replicate these items
              in each of the n groups.

        8.16. Illustrate your solution to Exercise 8.15 for replicating the numbers
              {1, 7, 3, 4} in group 2 in an OTIS-Hypercube with four groups.

        8.17. Outline a deterministic algorithm for routing on the OTIS-
              Hypercube.

        8.18. Discuss the drawbacks of the deterministic algorithm for routing in
              the OTIS-Hypercube in Exercise 8.17.

        8.19. Outline an algorithm for broadcasting a datum in processor (g, p) in
              the OTIS-Star.

        8.20. What is the degree of OTIS-MOT?

        8.21. What is the diameter of OTIS-MOT with n2 processors? Assume the
              mesh of trees depicted in Fig. 8.5.

        8.22. Give two nodes in the OTIS-MOT shown in Fig. 8.6 that realize the
              diameter derived in the solution to Exercise 8.21
May 7, 2022   11:14       Parallel Algorithms          9in x 6in       b4591-ch08           page 334




        334                                     Parallel Algorithms

        8.7     Solutions

         8.1. Suppose we want to move data from processor p1 in group g1 to
              processor p2 in group g2 . One possibility is to use the sequence of
              moves:

                                           ∗                           ∗
                                          e            o            e
                               (g1 , p1 )−→ (g1 , g2 )−→ (g2 , g1 )−→ (g2 , p2 ),

                                                              ∗
                 where −→
                        o
                            is an optical move and −→
                                                    e
                                                       stands for a sequence of zero
                 or more electronic moves. Suggest another sequence of moves.

                 One possibility is the sequence:

                                   ∗                               ∗
                                   e            o            e            o
                        (g1 , p1 )−→ (g1 , p2 )−→ (p2 , g1 )−→ (p2 , g2 )−→ (g2 , p2 ).

         8.2. Is Algorithm otismeshbroadcast discussed in Section 8.2.2 for
              broadcasting in the OTIS-Mesh optimal? Justify your answer.

                 Algorithm otismeshbroadcast for broadcasting in the OTIS-Mesh
                                                                     √
                 is optimal since the diameter of the OTIS-Mesh is 4 n − 3; the
                 distance between processor (0, 0) and processor (n − 1, n − 1) is
                   √
                 4 n − 3.

         8.3. In a window broadcast, we start with data in the top left w × w
                                                        √                       √
              submesh of a single group g, where w | n, that is w divides n.
              Following the window broadcast operation, the initial w × w window
              tiles the entire OTIS-Mesh. (See Exercise 4.19). Outline an algorithm
              to implement this operation. What is the running time of your algo-
              rithm?

                 The algorithm is shown as Algorithm otismeshwinbroadcast. Fol-
                 lowing Step 1, the initial window properly tiles the group g. In Step 2,
                 data d(g, p) from processor (g, p) is moved to (p, g), 0 ≤ p < n. In
                 Step 3, d(g, p) is broadcast to all processors (p, k), 0 ≤ p, k < n.
                 Finally, in Step 4, d(g, p) is moved to (k, p), 0 ≤ k, p < n. By Exer-
                                             √
                 cise 4.19, Step 1 takes 2( n − w) electronic moves. Steps 2 and 4
                                                             √
                 take one OTIS move each. Step 3 takes 2( n − 1) electronic moves.
                               √
                 The total is 4 n − 2w − 2 electronic and two OTIS moves.
May 7, 2022    11:14       Parallel Algorithms    9in x 6in      b4591-ch08                  page 335




                        Optical Transpose Interconnection System (OTIS)                335


          Algorithm 8.6 otismeshwinbroadcast
              1. Do a window broadcast within the initial group g as outlined in the
                 solution of Exercise 4.19.
              2. Perform an OTIS move. That is, all processors in group g send their data
                 to processors in other groups using optical links.
              3. Perform data broadcast from processor g of each group to all processors
                 of that group.
              4. Perform an OTIS move.




         8.4. Apply the addition algorithm for the OTIS-Mesh discussed in Sec-
              tion 8.2.3 on the input
                                                                                
                           {1, 3, 2, 4}, {2, 1, 5, 1}, {2, 4, 5, 1}, {2, 1, 3, 2} .
              Similar to Example 8.1.

         8.5. Explain how to implement Step 1 in Algorithm otismeshsum of
              Section 8.2.3 eﬃciently.
                                                                          √
              If we compute the sum in processor (0, 0), it will take 4( n − 1);
                 √                                                            √
              2( n−1) for transferring the elements to processor (0, 0) and 2( n−
              1) for broadcasting the sum. So, we compute the sum in the middle
                                                                               √
              processor instead. In this case, the number of steps will be 2 n;
              √                                                             √
                n for transferring the elements to the middle processor and n for
              broadcasting the sum.

         8.6. Apply the parallel preﬁx algorithm for the OTIS-Mesh discussed in
              Section 8.2.4 on the input
                                                                                
                           {2, 4, 2, 1}, {1, 3, 1, 4}, {5, 2, 1, 3}, {2, 3, 5, 1} .
                  Similar to Example 8.2.

         8.7. Illustrate the operation of Algorithm otismeshshift in Section 8.2.5
              given the processor contents
                                                                                   
                        {3, 8, 1, 14}, {2, 13, 5, 11}, {9, 7, 10, 6}, {4, 15, 12, 8} .
                  Assume an OTIS-Mesh with 16 processors, and the elements are to
                  be shifted along the u-coordinate.
                  Similar to Example 8.3.
May 7, 2022   11:14        Parallel Algorithms          9in x 6in      b4591-ch08                   page 336




        336                                      Parallel Algorithms

         8.8. Repeat Exercise 8.7 with shifting the elements along the v-coordinate
              instead.
                 Similar to Example 8.4.

         8.9. Modify the algorithm for randomized routing in the OTIS-Mesh dis-
                                                                 √      √
              cussed in Section 8.2.6.2 so that it runs in time 4 n + o( n).
                 One possibility is the following modiﬁcation to Phase 1. Partition
                 the OTIS-Mesh into slices, i.e., 4-D submeshes, of size
                                                √         √     √      √
                                                     n      n     n      n
                                                       ×      ×      ×
                                                   q       q     q      q
                                 √
                 each, 1 ≤ q ≤ n. In the ﬁrst phase, a packet traverses to a random
                 processor (u , v  , x , y  ) in its own slice of origin. The rest of the
                 algorithm is as in Section 8.2.6.2. The maximum distance traveled in
                                                        √
                 the ﬁrst phase becomes ≤ 4 n/q. Thus, the time needed for Phase 1
                     √           √
                 is 4 n/q + o( n), and the total time of the algorithm becomes at
                        √      √                  √
                 most 4 n + 4 n/q + o( n). Choose a suitable q such as q = log n so
                                                                                    √       √
                 that the total time of the algorithm becomes at most 4 n + o( n).

        8.10. Verify that the diameter of the OTIS-Hypercube is 2d + 1.
                 To go from (0, 0) to (n − 1, n − 1), follow the path:
                                  e
                           (0, 0)−→ (0, n − 1)−→
                                               o
                                                 (n − 1, 0)−→
                                                            e
                                                              (n − 1, n − 1),
                 where −→
                        o
                            is an optical move and −→e
                                                         stands for electronic moves.
                 The total number of steps is d + 1 + d = 2d + 1.

        8.11. Analyze the running time of the algorithm in Section 8.3.2 for broad-
              casting in the OTIS-Hypercube by simulating an n2 -hypercube.
                 Since the cost of broadcasting in an n2 hypercube is log n2 = 2 log n
                 steps, by Theorem 8.1, direct simulation of an n2 hypercube costs at
                 most 3 × 2log n = 6 log n steps (electronic and optical).

        8.12. Analyze the running time of the algorithm in Section 8.3.3 for addi-
              tion in the OTIS-Hypercube by simulating an n2 -hypercube.
                 Since the cost of addition in an n2 hypercube is log n2 = 2 log n steps,
                 by Theorem 8.1, direct simulation of an n2 hypercube costs at most
                 3 × 2log n = 6 log n steps (electronic and optical).
May 7, 2022    11:14        Parallel Algorithms        9in x 6in       b4591-ch08                page 337




                         Optical Transpose Interconnection System (OTIS)                   337

        8.13. Suppose that each of the n groups in an OTIS-Hypercube has a
              datum in an arbitrary processor. Give an algorithm to collect these n
              data items in a speciﬁed group k so that group k will have one item
              per processor.
                  See Algorithm otishcproblem1. Step 1 takes no more than log n
                  electronic moves, and Step 2 takes one optical move. (See Exer-
                  cise 8.14).

          Algorithm 8.7 otishcproblem1
              1. For 0 ≤ g ≤ n − 1, group g sends its datum to processor k in that group.
              2. For 0 ≤ g ≤ n − 1, the datum from processor k of group g is sent to
                 processor g of group k using one optical move.


        8.14. Illustrate your solution to Exercise 8.13 given the processor contents
                                                                                
                           {x, x, 1, x}, {2, x, x, x}, {x, x, x, 3}, {x, x, 4, x} ,
                  which are to be sent to group 2. Assume an OTIS-Hypercube with
                  four groups. Here, x stands for anything, and the contents of the
                  processors are represented by a set of four sets, each representing a
                  group of processors.
                  Initially, the contents of the processors are
                                                                                     
                                {x, x, 1, x}, {2, x, x, x}, {x, x, x, 3}, {x, x, 4, x} .
                  In the ﬁrst step, group g, 0 ≤ g ≤ 3, sends its datum to processor 2
                  in that group. Following this step, we obtain
                                                                                   
                              {x, x, 1, x}, {x, x, 2, x}, {x, x, 3, x}, {x, x, 4, x} ,
                  Finally, the datum from processor 2 of group g, 0 ≤ g ≤ 3, is sent to
                  processor g of group 2. After this ﬁnal step, we obtain
                                                                                    
                               {x, x, x, x}, {x, x, x, x}, {1, 2, 3, 4}, {x, x, x, x} .

        8.15. Suppose that group k in an OTIS-Hypercube has n data items located
              one per processor. Give an algorithm to replicate these items in each
              of the n groups.
                  See Algorithm otishcproblem2. After Step 1, each group has one
                  item, and the data is replicated after Step 3. Steps 1 and 3 take
May 7, 2022    11:14        Parallel Algorithms          9in x 6in      b4591-ch08              page 338




        338                                       Parallel Algorithms


                  one optical move each, and Step 2 takes log n electronic moves. (See
                  Exercise 8.16).

          Algorithm 8.8 otishcproblem2
              1. For all p, 0 ≤ p ≤ n−1, processor p of group k sends its item to processor k
                 of group p using one optical move.
              2. Each group replicates its item in all of its processors by broadcasting it
                 to all processors.
              3. For all p, g, 0 ≤ p, g ≤ n − 1, processor p of group g sends its item to
                 processor g of group p using one optical move.


        8.16. Illustrate your solution to Exercise 8.15 for replicating the numbers
              {1, 7, 3, 4} in group 2 in an OTIS-Hypercube with four groups.

                  The contents of the processors will be represented by a set of four
                  sets, each representing a group of processors. Initially, the contents
                  of the processors are

                                                                                      
                                 {x, x, x, x}, {1, 7, 3, 4}, {x, x, x, x}, {x, x, x, x} ,

                  where x stands for anything. In the ﬁrst step, processor p of group 2
                  sends its item to processor 2 of group p. Following this step, we obtain

                                                                                      
                                 {x, 1, x, x}, {x, 7, x, x}, {x, 3, x, x}, {x, 4, x, x} ,

                  Now, each group replicates its item in all of its processors by broad-
                  casting it to all processors in its group. The contents become

                                                                                    
                               {1, 1, 1, 1}, {7, 7, 7, 7}, {3, 3, 3, 3}, {4, 4, 4, 4} ,

                  Finally, each processor p of group g sends its item to processor g of
                  group p. After this ﬁnal step, we obtain

                                                                                    
                               {1, 7, 3, 4}, {1, 7, 3, 4}, {1, 7, 3, 4}, {1, 7, 3, 4} .

        8.17. Outline a deterministic algorithm for routing on the OTIS-
              Hypercube.
May 7, 2022    11:14           Parallel Algorithms       9in x 6in       b4591-ch08                page 339




                         Optical Transpose Interconnection System (OTIS)                     339

                  Use one of the paths shown in the solution of Exercise 8.1 to route
                  the source packet to its destination. For example, use the sequence
                                                ∗                        ∗
                                               e            o            e
                                    (g1 , p1 )−→ (g1 , g2 )−→ (g2 , g1 )−→ (g2 , p2 ).
                           ∗
                  Here, −→
                         e
                             is a sequence of zero or more electronic moves and −→
                                                                                 o
                                                                                   is
                  an optical move. For routing within the hypercubes, use the greedy
                  algorithm of Section 3.6.1.

        8.18. Discuss the drawbacks of the deterministic algorithm for routing in
              the OTIS-Hypercube in Exercise 8.17.
                  The major drawback is that many packets may pile up at inter-
                  mediate processors. In particular, all processors from group g1 may
                  want to send to all processors in group g2 . In this case, Θ(n) packets
                  may accumulate at processor (g1 , g2 ) before transmitting the packets
                  along the optical link between processors (g1 , g2 ) and (g2 , g1 ). This
                  implies that the delay, and hence the total number of steps, is O(n).

          Algorithm 8.9 otisstarbroadcast
          Input: x stored in processor (g, p).
          Output: Broadcast x to all other processors.
              1. Processor (g, p) broadcasts x to all processors in its group, group g.
              2. Perform an OTIS move. That is, all processors in group g send their data
                 to processors in other groups using optical links.
              3. Processor (g  , p ) in every group g  broadcasts its data to all processors
                 within its group.

        8.19. Outline an algorithm for broadcasting a datum in processor (g, p) in
              the OTIS-Star.
                  The algorithm is similar to that in Section 8.2.2 for broadcasting
                  in the mesh. It is shown as Algorithm otisstarbroadcast. The
                  algorithm for broadcasting in the star discussed in Section 7.7 may
                  be used to broadcast in individual stars.

        8.20. What is the degree of OTIS-MOT?
                  The degree of OTIS-MOT is 5, as can be seen from Fig. 8.6. In this
                  ﬁgure, the degree of node 00 in group 10 is 5, and it is maximum.

        8.21. What is the diameter of OTIS-MOT with n2 processors? Assume the
              mesh of trees depicted in Fig. 8.5.
May 7, 2022   11:14           Parallel Algorithms              9in x 6in         b4591-ch08             page 340




        340                                          Parallel Algorithms


                 00     01                          00        01            02   00       01      02
                                  02


                 10      11            12           10        11            12   10       11       12



                20      21          22             20         21        22       20       21      22


                      group 00                                 group 01                group 02

                 00       01           02           00        01                 00       01      02
                                                                       02


                10       11            12           10        11            12   10       11       12



                20      21          22             20         21        22       20       21      22


                      group 10                              group 11                   group 12
                 00     01             02           00        01            02    00      01      02



                10      11             12           10        11            12   10       11      12



                20      21          22             20         21        22       20       21      22


                      group 20                             group 21                    group 22

                                       Fig. 8.7.        Solution to Exercise 8.22.

                                                                             √
                 The underlying mesh is of size n, and each binary tree has n nodes
                                                                               √
                 (see Fig. 8.5). Thus, the height of each binary tree is log n. It
                                                                        √
                 follows that the diameter of the mesh of trees is 4log n, and hence
                                                                             √
                 the diameter of the OTIS-MOT with n2 processors is 8log n+1 =
                 Θ(log n).

        8.22. Give two nodes in the OTIS-MOT shown in Fig. 8.6 that realize the
              diameter derived in the solution to Exercise 8.21

                 The two nodes (11, 12) and (21, 22) realize the √
                                                                 OTIS-MOT diameter.
                 The distance between these two nodes is 8log 9 + 1 = 9. The path
                 between these two nodes is shown in thick lines in Fig. 8.7.
May 7, 2022   11:14    Parallel Algorithms       9in x 6in   b4591-ch09                 page 341




                                             Chapter 9


                        Systolic Computation



        9.1     Introduction

        Systolic computation refers to one in which the processors, usually called
        processing elements (PE’s), are arranged in a very regular way (most often,
        as one or two-dimensional arrays), and so the data moves through them
        in a regular fashion. Processors are usually primitive, and perform very
        simple operations on the data they receive, e.g., computing the maximum
        and minimum of two items. A systolic array is an on-chip multi-processor
        architecture. It was proposed as an architectural solution to the anticipated
        on-chip communication bottleneck of modern, very large-scale integration
        (VLSI) technology. A systolic array features a mesh-connected array of
        identical, simple PE’s. In a systolic system, data ﬂows from the computer
        memory in a rhythmic fashion, passing through many processing elements
        before it returns to memory. A systolic array is often conﬁgured into a
        linear array, a two-dimensional rectangular mesh array, or sometimes, a
        two dimensional hexagonal mesh array. In a systolic array, every PE is
        connected only to its nearest neighboring PEs through a dedicated, buﬀered
        local bus/dedicated, buﬀered local buses. This localized interconnects, and
        regular array conﬁguration allow a systolic array to grow in size without
        incurring excessive on-chip global interconnect delays due to long wires.
        In the rest of this chapter, we will use the terms “PE” and “processor”
        interchangeably.




                                                341
May 7, 2022   11:14      Parallel Algorithms             9in x 6in      b4591-ch09              page 342




        342                                    Parallel Algorithms

                                                                       a 33
                              (a)
                                                                a 32   a 23
                                                      a 31      a 22   a 13
                                                      a 21      a 12
                                                      a 11


                                      y3      y2 y1
                                                      x1         x2    x3



                              (b)      a in


                             y in       w        y out


                         Fig. 9.1.    Systolic matrix-vector multiplication.



        9.2     Matrix-vector Multiplication

        In this section, we present a simple example of systolic arrays. Consider
        performing the multiplication y = Ax, where A is an n × n matrix, and x is
        an n×1 vector. One possible systolic array to solve this problem consists of n
        processors arranged in the form of a linear array. It is assumed that these
        processors are capable of performing scalar addition and multiplication of
        real numbers. Figure 9.1 shows an example of this array and arrangement of
        the input for n = 3. As shown in the ﬁgure, there are 3 processors, and data
        from the matrix A arrives in a systolic fashion, while the xi ’s are preloaded
        to the processors. The initial arrangement and data movements of the ai,j ’s
        is such that column k is delayed by k cycles. The yi ’s are initially set to
        zero, and their values are accumulated in the PEs. Thus, in the ﬁrst cycle,
        y1 is set to a11 x1 , in the next cycle, y1 is set to a11 x1 + a12 x2 and y2 is set
        to a2,1 x1 , and so on.


        9.3     Computing the Convolution of Two Sequences

        Let x1 , x2 , . . . , xn  and w1 , w2 , . . . , wk  be two sequences of real num-
        bers. The convolution of these two sequences is deﬁned as the sequence
May 7, 2022   11:14         Parallel Algorithms                 9in x 6in          b4591-ch09                       page 343




                                           Systolic Computation                                               343


        y1 , y2 , . . . , yn+1−k , where yi = w1 xi + w2 xi+1 + · · · + wk xi+k−1 . The ele-
        ments of the sequence w1 , w2 , . . . , wk  are called weights. The convolution
        of these two sequences can be expressed by the matrix-vector product
              ⎛                                                              ⎞⎛        ⎞   ⎛             ⎞
                      x1        x2              x3           ...     xk           w1              y1
              ⎜                                              . . . xk+1 ⎟ ⎜    ⎟ ⎜                       ⎟
              ⎜       x2        x3              x4                      ⎟ ⎜ w2 ⎟ ⎜                y2     ⎟
              ⎜                                                         ⎟⎜     ⎟ ⎜                       ⎟
              ⎜                                              . . . xk+2 ⎟ ⎜    ⎟ ⎜                       ⎟.
              ⎜       x3        x4              x5                      ⎟ ⎜ w3 ⎟ = ⎜              y3     ⎟
              ⎜        ..        ..              ..                  .. ⎟ ⎜ . ⎟ ⎜                  ..    ⎟
              ⎝         .         .               .          ...      . ⎠ ⎝ .. ⎠ ⎝                  .    ⎠
                  xn+1−k    xn+2−k         xn+3−k            ...     xn           wk            yn+1−k

        In what follows, we present two approaches for systolic computations to
        compute the convolution of two sequences, one semisystolic and the other
        systolic. The basic principles of these designs were previously proposed
        for circuits to implement a pattern matching processor and polynomial
        multiplication. For simplicity, we will assume in the rest of this section that
        k = 3.


        9.3.1         Semisystolic solution
        In this design, a bus is used for global data communication, and this is
        why it is referred to as “semisystolic”. The xi ’s are broadcast, results are
        moved, and weights stay in the PEs. The systolic array and its cell deﬁnition
        are depicted in Fig. 9.2. The weights are preloaded to the cells, one at each
        cell, and stay at the cells throughout the computation. The partial results yi
        move systolically from cell to cell in the left-to-right direction during each
        cycle. At the beginning of a cycle, one xi is broadcast to all the cells, and

                                          x3 x2         x1
                                 (a)

                                           y3 y2 y1
                                                             w1         w2        w3



                                 (b)       x in


                                y in        w           y out


                                      Fig. 9.2.       Semisystolic convolution.
May 7, 2022   11:14           Parallel Algorithms                9in x 6in              b4591-ch09                   page 344




        344                                         Parallel Algorithms

                         Table 9.1.       Convolution using the semisystolic design.

          Cycle                   y1                                 y2                              y3

          1            w1 x 1                           0                                 0
          2            w 1 x 1 + w2 x 2                 w1 x 2                            0
          3            w 1 x 1 + w2 x 2 + w3 x 3        w1 x 2 + w2 x 3                   w1 x 3
          4            Output y1                        w1 x 2 + w2 x 3 + w3 x 4          w1 x 3 + w2 x 4
          5                                             Output y2                         w1 x 3 + w2 x 4 + w3 x 5
          6                                                                               Output y3



                                                                                          w1
                                   (a)     x3         x2    y3           y2        y1
                                                                              x1




                                   (b)
                                         w out                   w in
                                                    y
                                         x in                    x out

                                          Fig. 9.3.      Systolic convolution.

        one yi , which is initialized as zero, enters the leftmost cell. Thus, during the
        ﬁrst cycle, w1 x1 is accumulated to y1 at the leftmost cell, and during the
        second cycle, w1 x2 and w2 x2 are accumulated to y2 and y1 at the leftmost
        and middle cells respectively. Starting from the third cycle, the ﬁnal values
        of y1 , y2 , . . . are output from the rightmost cell, one yi per cycle.

        Example 9.1 Table 9.1 shows the results of the computation of the
        convolution of the two sequences x1 , x2 , x3 , x4 , x5  and w1 , w2 , w3 . 


        9.3.2         Pure systolic solution
        An alternative to the semisystolic design is a pure systolic one in which each
        partial result yi stays at a cell to accumulate its terms, and the xi ’s and wi ’s
        move in opposite directions. The systolic array and its cell deﬁnition are
        depicted in Fig. 9.3. Thus, in this design, the xi ’s and wi ’s move systolically
        in opposite directions such that when an x meets a w at a cell, they are
        multiplied and the resulting product is accumulated to the y staying at
May 7, 2022   11:14          Parallel Algorithms         9in x 6in            b4591-ch09                    page 345




                                           Systolic Computation                                       345

                           Table 9.2.      Convolution using the systolic design.

          Cycle                  y1                          y2                            y3

          1           w1 x 1                       0                            0
          3           w 1 x 1 + w2 x 2             w1 x 2                       0
          5           w 1 x 1 + w2 x 2 + w3 x 3    w1 x 2 + w2 x 3              w1 x 3
          7           Output y1                    w1 x 2 + w2 x 3 + w3 x 4     w1 x 3 + w2 x 4
          9                                        Output y2                    w1 x 3 + w2 x 4 + w3 x 5
          11                                                                    Output y3



        that cell. The diﬃculty with this design is that the xi ’s and wi ’s move
        twice as fast toward each other. The solution is to move data at half the
        speed. Thus, to ensure that each xi is able to meet every wi , consecutive xi ’s
        on the x data stream are separated by two cycle times and so are the wi ’s
        on the w data stream. In this design, a tag bit is associated with the ﬁrst
        weight, w1 , to trigger the output and reset the accumulator contents of
        a cell. It can be easily checked that the yi ’s will output from the systolic
        output path in the natural ordering y1 , y2 , . . .. Speciﬁcally, when wk leaves
        processor Pi (k = 3 in the ﬁgure), the ﬁnal value of yi is computed, and it
        can move out of the array through the data path below the array. Notice,
        however, that in this design only about one half the cells are doing useful
        work at any time.


        Example 9.2 Table 9.2 shows the results of the computation of the
        convolution of the two sequences x1 , x2 , x3 , x4 , x5  and w1 , w2 , w3 . It is
        similar to Table 9.1 except that the cycle numbers are incremented by 2 in
        this design.                                                                       



        9.4     A Zero-time VLSI Sorter

        Basically, this sorter consists of a linear array of n/2 cells, where n is
        assumed to be even. Each cell can store two items of the sequence to be
        sorted. Figure 9.4 depicts the block diagram of the sorter. As shown in the
        ﬁgure, there is only one connection between a cell and each of its upper
        and its lower neighboring cells. There are two phases: The up-down phase
        and the bottom-up phase. In both phases, after comparison, one of the two
May 7, 2022   11:14     Parallel Algorithms             9in x 6in      b4591-ch09          page 346




        346                                   Parallel Algorithms




                                                 Item          Item


                                                 Item          Item   Cell 1


                                                 Item          Item   Cell 2




                                                 Item          Item   Cell n/2



                               : Comparator

                             Fig. 9.4.        Block diagram of the sorter.


        items moves to the next neighboring cell through this connection. Since
        the data ﬂow is the same for all cells at any given time, this removed item
        occupies the newly-created space in the next cell; the removed item at the
        bottom cell is moved out of the array in a downward data ﬂow, while the
        item at the top cell is moved out of the array in an upward data ﬂow.
        The initial sequence to be sorted is entered into the sorter one item at
        each step. After the last item has been entered, the data ﬂow direction is
        reversed, and the sorted sequence is then extracted as output, also serially.
        Each step, executed synchronously and simultaneously by all the cells, has
        two phases:

        (1) Compare: The two items in each and every cell are compared to each
            other.
        (2) Transfer: Subject to the result of the comparison, the desired sorting
            order (ascending or descending), and the sorting state (input or out-
            put), one or the other of the two items is transferred to a neighboring
            cell and the original cell receives an item from the other neighboring cell.

        Example 9.3         Figures 9.5 and 9.6 show an example of sorting the
        sequence 4, 3, 1, 6, 2, 5 in ascending order. Here, ∞ represents the largest
        item possible. At the input stage (Fig. 9.5), the larger of the two items
May 7, 2022   11:14             Parallel Algorithms                       9in x 6in              b4591-ch09                         page 347




                                                         Systolic Computation                                                 347

                            5
                            2                    5                5
                            6                    2                2                 5            5
                            1                    6                6                 2            2                    5
                            3                    1                1                 6            6                    2
                            4                    3                3                 1            1                    6

                                             4               4                 4        3   4        3            1       3
                        8

                                8




                                                 8




                                                                      8
                                                                                                                  4
                        8
                                8


                                         8
                                                 8


                                                             8
                                                                  8




                                                                                            8
                                                                                                  8




                                                                                                                      8
                                                                               8
                                                                                    8
                        8




                                         8
                                8




                                                 8


                                                             8




                                                                                            8




                                                                                                              8
                                                                  8




                                                                                                  8




                                                                                                                      8
                                                                            8

                                                                                    8
                                                 8




                                                                                                                      8
                                                                                   8
                       Compare           Transfer            Compare           Transfer     Compare           Transfer
                      {
                      {
                      {
                            5
                                    Step 1                            Step 2                         Step 3
                            2                    5                5
                            6                    2                2                 5            5

                        1       3            1       6        1       6         1       2    1       2            1       5

                        4                    4       3                3         6       3   6        3            2       3
                            8




                                                              4
                                                                                            4                     4       6
                       8




                                         8




                                                                                                     8
                            8




                                                     8




                                                                                4
                                                             8

                                                                      8




                                                                                        8




                                                                                                                      8
                                                 8




                                                                                    8




                       Compare           Transfer            Compare           Transfer     Compare           Transfer
                      {
                      {
                      {             Step 4                            Step 5                             Step 6

        Fig. 9.5.     Input stage in the zero-time sorter: Larger items are circled and trans-
        ferred.

        in each cell is transferred down, while at the output stage (Figs. 9.6), the
        smaller of the two items is transferred up.                               

            Note that at the end of the input stage (step 6 in the above example),
        the smallest item must be in the top cell and the second smallest must be
        in either the top or the second cell. In general, the kth smallest item must
        be in one of the top k cells. This is why the output sequence is sorted.


        9.5     An On-chip Bubble Sorter

        The basic component of the bubble sorter is the compare/steer unit, which
        is shown in Fig. 9.7. It consists of four interconnected cells: A, B, C and D.
        The sorter consists of a stack of n comparators that work synchronously in
        one of two modes: downward and upward (see Fig. 9.8). In the downward
        mode, cell A in every unit receives its input from the unit above or from
        outside, the content of C is routed to B, and the content of D, which is
May 7, 2022   11:14            Parallel Algorithms                         9in x 6in               b4591-ch09                     page 348




        348                                             Parallel Algorithms

                                                                                                                          1
                                                                                      1                1                  2
                                            1                    1                    2                2                  3

                       1       5        2       5            2       5            3       5        3       5         4        5

                      2        3        4       3        4           3            4       6        4       6                  6




                                                                                                                     8
                      4        6                6                    6




                                                                                          8




                                                                                                           8


                                                                                                                     8

                                                                                                                              8
                                     8




                                                         8




                                                                              8




                                                                                                8
                          8




                                            8




                                                             8




                                                                                      8




                                                                                                       8




                                                                                                                         8
                      Compare           Transfer        Compare                   Transfer      Compare              Transfer
                      {
                      {
                      {
                               Step 7                                    Step 8                             Step 9
                                                                                                                      1
                                                                                      1            1                  2
                                            1                1                        2            2                  3
                           1                2                2                        3            3                  4
                           2                3                3                        4            4                  5
                           3                4                4                        5            5                  6

                      4        5        6       5        6           5        6                6
                                                                                          8




                                                                                                       8


                                                                                                                8
                                                                                                                          8
                               6
                      8




                                      8
                                                8


                                                        8
                                                                 8


                                                                             8
                                                                                          8


                                                                                               8
                                                                                                       8


                                                                                                                8
                                                                                                                          8
                               8


                                     8

                                                8




                                                                 8


                                                                             8

                                                                                          8




                                                                                                       8


                                                                                                                8

                                                                                                                          8
                      8




                                                        8




                                                                                               8
                                            8




                                                             8




                                                                                  8




                                                                                                   8
                          8




                      Compare           Transfer        Compare               Transfer        Compare            Transfer
                      {
                      {
                      {        Step 10                               Step 11                               Step 12

        Fig. 9.6. Output stage in the zero-time sorter: Smaller items are circled and
        transferred.



                                                                 A                C       C = min(A,B)


                                     D = max(A,B)                D                B



                               Fig. 9.7.            A compare/steer unit (comparator).



        the larger of the two numbers, is moved to the next comparator below.
        Next, the contents of A and B are compared, and the minimum and maxi-
        mum are delivered to C and D, respectively. That is, C = min{A, B} and
        D = max{A, B}. In the upward mode, an outside key is loaded from the
        bottom into cell B of the unit, and an inside key previously at D is loaded
        into A. After loading, the comparison is executed, and the minimum is
May 7, 2022   11:14     Parallel Algorithms               9in x 6in   b4591-ch09         page 349




                                      Systolic Computation                         349




                                              Fig. 9.8.     Sorter.


        delivered to C and the maximum to D. Loading and comparing are exe-
        cuted almost simultaneously, so all these operations are performed in all
        comparators in every period, which we will take for convenience as one unit.
            During the downward input phase, n keys are loaded into n units in 2n
        periods. During the upward output phase, each unit delivers the smaller
        key to its upper unit in every period, outputting one item per period from
        the sorted keys. The sorting time is completely absorbed into input/output
        time. So, it takes 2n periods to sort n numbers.


        Example 9.4 Figures 9.9 and 9.10 illustrate the action of the sorter
        during the sorting of an input of six numbers, 4, 3, 1, 6, 2, and 5 in the
        downward and upward phases, respectively. Initially, at time t0 , the contents
        of the buﬀer cells in each comparator are all set to ∞. During the ﬁrst cycle,
        the ﬁrst number 4 is compared to ∞ and routed to the upper right cell.
        During the second cycle, the number 3 is loaded and compared to 4; then
        the number 3 is routed to the upper right cell and 4 is routed to the lower
        left cell of the ﬁrst unit. During the third cycle, as the third number is
        being loaded into the ﬁrst unit, the number 4 is loaded into the second
        unit. In other words, the larger of the two numbers will be pushed out
        of the comparator in which it resides. At the end of time t3 , the upper
        right cell of the ﬁrst unit contains 1 and the lower left cell contains 3; and
May 7, 2022   11:14                 Parallel Algorithms                                          9in x 6in                       b4591-ch09                             page 350




        350                                                                    Parallel Algorithms


                 5
                 2                      5
                 6                      2                         5
                 1                      6                         2                         5
                 3                      1                         6                         2                       5
                 4                      3                         1                         6                       2                  5

                                                 4                                                   1                       1                  1                   1
                       8




                                                                           3
                                                                  4                         3                       6                  2                   5
                8




                                    8




                                                                                                     4                       3                  3                   2
                       8




                                             8




                                                                       8

                                                                                                                    4                  6                   3
                8




                                    8




                                                              8




                                                                                        8
                                                                                                                                                4                   4
                       8




                                             8




                                                                       8




                                                                                                 8




                                                                                                                         8
                                                                                                                                                           6




                                                                                                                                       8
                8




                                    8




                                                              8




                                                                                        8




                                                                                                                8
                      t0                    t1                        t2                        t3                      t4                 t5                  t6

                                                              Fig. 9.9.                     Up-down sorter.

                                                                                                                                                       1
                                                                                                                                   1                   2
                                                                                                                1                  2                   3
                                                                                        1                       2                  3                   4
                                                              1                         2                       3                  4                   5
                                    1                         2                         3                       4                  5                   6

                                    2                         3                         4                       5
                                                                                                                                                      8



                                                                                                                                   6
                           5                         5                         5                         6
                                                                                                                                                8
                                                                                                                             8




                                                                                                                                                      8




                                    3                         4                         6
                                                                                                               8




                                                                                                                                  8




                           4
                                                                                                                             8




                                                                                                                                                8
                                                                           8




                                                                                                     8




                                                     6
                                    6
                                                          8




                                                                                    8




                                                                                                               8




                                                                                                                                  8




                                                                                                                                                      8
                                                                                                                                                8
                           8



                                                 8




                                                                           8




                                                                                                     8




                                                                                                                             8




                               t7                        t8                        t9                        t 10                t11                t 12

                                                          Fig. 9.10.                    Bottom-up sorter.


        the upper right cell of the second unit contains 4 and the lower left cell
        contains ∞. At the end of time t6 , all the six numbers have been loaded
        into the sorter, thus completing the input downward phase. From time t7
        on, the output upward phase begins. Note that in the input phase the ∞’s
        are pushed out of the bottom of the sorter; in the output phase the ∞’s
        are pushed back into the sorter from the bottom. At the end of time t7 , the
May 7, 2022   11:14     Parallel Algorithms      9in x 6in    b4591-ch09                  page 351




                                      Systolic Computation                         351

        smallest number 1 is out and the second smallest 2 is in the upper right cell
        of the ﬁrst unit awaiting to be output. In this output phase, the smaller of
        the two numbers within each comparator is popped up, leaving the unit it
        resides in and entering the unit on top of it. In the case of the top unit, the
        smaller number is delivered as output. Thus, the sorter continues to put out
        the numbers in order. At the end of time t12 all data in the sorter will have
        been output in ascending order as desired. At the same time, the sorter is
        automatically reset to its initial state (all ∞) and is ready to accept the
        next input sequence.                                                         

        Theorem 9.1 The sorter correctly sorts the input numbers.

        Assume the elements to be sorted are distinct, and n is even. Let Ci and
        Di denote the contents stored in cells C and D of the ith comparator.
        First, we show that min{Ci+1 , Di+1 } ≥ min{Ci , Di }. It is the function of
        the ith comparator to push down the larger of its two keys in the input
        phase, and to pop up the smaller of the two keys in the output phase. In
        input phase, the keys Ci+1 and Di+1 are obtained via comparator i. Hence,
        the pushed key Ci+1 or Di+1 must be greater than or equal to the key in
        comparator i against which it was compared. Similarly, in the output phase,
        the popped up key Ci or Di is obtained from comparator i+1, hence it must
        be smaller than or equal to the key in comparator i + 1 against which it was
        compared. In both cases, it follows that min{Ci+1 , Di+1 } ≥ min{Ci , Di }.
        Consequently, the kth smallest element is in one of the top k comparators.
        To see this, assume that the kth smallest element x is not in the ﬁrst k
        comparators, that is, it is in comparator j for some j > k. Then, since
        min{C1 , D1 } ≤ min{C2 , D2 } ≤ · · · ≤ min{Cj , Dj } ≤ x, at least k elements
        are smaller than x, which is a contradiction. It follows that after n keys have
        been read into the sorter, the minimum must be in the top comparator, the
        second smallest must be either in the ﬁrst or second comparator, and so on.
        Thus, the ﬁrst element to be output must be the smallest, followed by the
        second smallest, etc.


        9.6     Bibliographic Notes

        Systolic array (Arnould, Kung, Menzilcioglu and Sarocky [12], Kung [50],
        Kung [51]) is an on-chip multi-processor architecture proposed by
May 7, 2022   11:14     Parallel Algorithms          9in x 6in      b4591-ch09           page 352




        352                                   Parallel Algorithms

        H.T. Kung in late 1970’s. It is proposed as an architectural solution to
        the anticipated on-chip communication bottleneck of modern very large-
        scale integration (VLSI) technology. For more on matrix-vector multipli-
        cation using less than n PEs, see Navarro, Llaberia and Valero [72], and
        Stojanovic, Milovanovic, Stojcev and Milovanovic [88]. Several variants of
        systolic architectures for the convolution problem can be found in Kung [50].
        The zero-time sorter is due to Miranker, Tang and Wong [68]. Bubble sorter
        is from Lee, Hsu and Wong[54].


        9.7     Exercises

         9.1. Design another systolic array for the matrix-vector product such that
              n = 4.

         9.2. How many steps are required to ﬁnish the computation in the systolic
              array for the matrix-vector product discussed in Section 9.2?

         9.3. Design a systolic array for the matrix-vector product in which the
              x-values enter from the left of the array, and the product y-values
              stay. How many steps are required to ﬁnish the computation?

         9.4. Design a two-dimensional systolic array for the problem of multiply-
              ing two 3 × 3 matrices A and B to produce the 3 × 3 matrix C.
              Assume that the products — that is, the ci,j ’s, will stay in the array.

         9.5. How many steps are required to ﬁnish the computation in the systolic
              array for the matrix-matrix product in Exercise 9.4?

         9.6. What is the main drawback of the systolic array design for convolu-
              tion described in Section 9.3.1?

         9.7. Design another semisystolic array for convolution similar to the one
              described in Section 9.3.1 in which the xi ’s are broadcast, the results
              stay and the weights move.

         9.8. What is the main drawback of the systolic array design for convolu-
              tion described in Section 9.3.2?

         9.9. Suggest a simple systolic array for sorting, and explain how it works.
May 7, 2022   11:14       Parallel Algorithms      9in x 6in    b4591-ch09                  page 353




                                        Systolic Computation                         353

        9.10. Illustrate the operation of the zero-time sorter on the input sequence
              3, 6, 2, 1, 3, 5.

        9.11. Explain how the zero-time sorter can sort in descending order.

        9.12. What modiﬁcation should be done to the zero-time sorter algorithm
              if the sequence is entered and extracted from the bottom port?

        9.13. Illustrate the operation of the bubble sorter on the input
              3, 6, 2, 1, 3, 5.

        9.14. Explain how to make the bubble sorter output the numbers in
              descending order.


        9.8     Solutions

         9.1. Design another systolic array for the matrix-vector product such that
              n = 4.

                 Similar to Fig. 9.1.

         9.2. How many steps are required to ﬁnish the computation in the systolic
              array for the matrix-vector product discussed in Section 9.2?

                 It takes 2n − 1 steps to produce the vector product y.

         9.3. Design a systolic array for the matrix-vector product in which the
              x-values enter from the left of the array, and the product y-values
              stay. How many steps are required to ﬁnish the computation?

                 The systolic array is shown in Fig. 9.11. It takes 2n − 1 steps to ﬁnish
                 the computation.

         9.4. Design a two-dimensional systolic array for the problem of multiply-
              ing two 3 × 3 matrices A and B to produce the 3 × 3 matrix C.
              Assume that the products, that is, the ci,j ’s, will stay in the array.

                 Arrange the rows and columns of A and B so that the ith row of A is
                 input to the ith column of the array from the top, and the jth column
                 of B is input to the jth row of the array from the left (see Fig. 9.12).
May 7, 2022   11:14       Parallel Algorithms              9in x 6in          b4591-ch09   page 354




        354                                     Parallel Algorithms


                                                                        a 33
                                                                a 23    a 32
                                                    a 13        a 22    a 31
                                                    a 12        a 21
                                                    a 11


                                    x3     x2 x1
                                                    y1          y2      y3


                         Fig. 9.11.      Systolic matrix-vector multiplication.



                                                                                   a 33
                                                                        a 23       a 32
                                                                 a 13   a 22       a 31
                                                                 a 12   a 21
                                                                 a 11



                                         b 31 b 21 b 11         c11     c21        c31


                                 b 32 b
                                        22 b 12                 c12     c22        c32



                          b 33 b 23 b 13                        c13     c23        c33


                             Fig. 9.12.      Systolic matrix multiplication.




                 The elements of A move downwards while the elements of B move in
                 the left-to-right direction.

         9.5. How many steps are required to ﬁnish the computation in the systolic
              array for the matrix-matrix product in Exercise 9.4?

                 It takes 3n − 2 steps to ﬁnish the matrix-matrix multiplication.
May 7, 2022   11:14         Parallel Algorithms             9in x 6in          b4591-ch09         page 355




                                             Systolic Computation                           355


                                          x3 x2     x1
                                   (a)

                                                    w1        w3         w2
                                                         y1         y2        y3



                                   (b)       x in


                                   w in       y     w out


                           Fig. 9.13.        Systolic convolution for Exercise 9.7.


                                     input
                                    output

                      Fig. 9.14.     Systolic sorting on linear array for Exercise 9.9.



         9.6. What is the main drawback of the systolic array design for convolu-
              tion described in Section 9.3.1?

                 The main drawback is that using the bus may be impractical for
                 implementation. As the number of cells increases, wires become too
                 long for the bus.

         9.7. Design another semisystolic array for convolution similar to the one
              described in Section 9.3.1 in which the xi ’s are broadcast, results stay
              and the weights move.

                 The xi ’s are broadcast, results stay and the weights circulate around
                 the array of cells. See Fig. 9.13. The ﬁrst weight w1 is associated
                 with a tag bit that signals the accumulator to output and reset its
                 contents.

         9.8. What is the main drawback of the systolic array design for convolu-
              tion described in Section 9.3.2?

                 The main drawback is that the computation of the yi ’s takes twice
                 as long, as input moves at half the speed.
May 7, 2022   11:14       Parallel Algorithms          9in x 6in      b4591-ch09           page 356




        356                                     Parallel Algorithms

         9.9. Suggest a simple systolic array for sorting, and explain how it works.
                 A linear array of processors can be used to sort as follows (see
                 Fig. 9.14). Each interior processor is connected by two-directional
                 links to its left and right neighbors. The input stream enters from the
                 leftmost end of the linear array. During the input phase, each proces-
                 sor, upon receiving a new element, compares its content with the cur-
                 rent element, keeps the smaller of the two and passes the larger one to
                 the right. After the input is consumed, the output phase commences,
                 in which the elements exit from the leftmost processor one at a time.

        9.10. Illustrate the operation of the zero-time sorter on the input sequence
              3, 6, 2, 1, 3, 5.
                 Similar to Example 9.3.

        9.11. Explain how the zero-time sorter can sort in descending order.
                 The same principle of sorting in ascending order applies to the
                 descending sort; we only have to replace ∞ by −∞, the smallest
                 item, and interchange larger and smaller.

        9.12. What modiﬁcation should be done to the zero-time sorter algorithm
              if the sequence is entered and extracted from the bottom port?
                 In this case, the larger of the two items is moved up instead, and the
                 smaller is moved down.

        9.13. Illustrate the operation of the bubble sorter on the input
              3, 6, 2, 1, 3, 5.
                 Similar to Example 9.4.

        9.14. Explain how to make the bubble sorter output the numbers in
              descending order.
                 There are two possibilities. The ﬁrst approach is to still use the same
                 sorting mechanism, except that we add a multiplier on top of the
                 sorter, which multiplies each input/output datum by −1. The sec-
                 ond approach is to exchange the input and output ports. That is,
                 to let the input data enter the sorter from the lower right end (i.e.,
                 where the number ∞ enters) and output data then comes out from
                 the lower left end (i.e., where ∞ comes out). In addition to the I/O
                 port exchange, the sorter must be initialized to contain a number
                 known to be smaller than the input data.
May 7, 2022   11:14        Parallel Algorithms       9in x 6in      b4591-app          page 357




                                             Appendix A


                       Mathematical Preliminaries

        A.1      Asymptotic Notations

        A.1.1         The O-notation

        Deﬁnition A.1 Let f (n) and g(n) be two functions from the set of nat-
        ural numbers to the set of nonnegative real numbers. f (n) is said to be
        O(g(n)) if there exists a natural number n0 and a constant c > 0 such that
                                        ∀ n ≥ n0 , f (n) ≤ cg(n).

        Consequently, if limn→∞ f (n)/g(n) exists, then
                                    f (n)
                              lim         = ∞ implies f (n) = O(g(n)).
                             n→∞    g(n)


           Informally, this deﬁnition says that f grows no faster than some constant
        times g. The O-notation can also be used in equations as a simpliﬁcation
        tool. For instance, instead of writing
                                     f (n) = 5n3 + 7n2 − 2n + 13,
        we may write

                                           f (n) = 5n3 + O(n2 ).
        This is helpful if we are not interested in the details of the lower order
        terms.




                                                   357
May 7, 2022   11:14        Parallel Algorithms           9in x 6in     b4591-app      page 358




        358                                      Parallel Algorithms

        A.1.2         The Ω-notation

        Deﬁnition A.2 Let f (n) and g(n) be two functions from the set of nat-
        ural numbers to the set of nonnegative real numbers. f (n) is said to be
        Ω(g(n)) if there exists a natural number n0 and a constant c > 0 such that

                                        ∀ n ≥ n0 , f (n) ≥ cg(n).

        Consequently, if limn→∞ f (n)/g(n) exists, then
                                     f (n)
                               lim         = 0 implies f (n) = Ω(g(n)).
                              n→∞    g(n)


           Informally, this deﬁnition says that f grows at least as fast as some
        constant times g. It is clear from the deﬁnition that

                          f (n) is Ω(g(n)) if and only if g(n) is O(f (n)).




        A.1.3         The Θ-notation

        Deﬁnition A.3 Let f (n) and g(n) be two functions from the set of nat-
        ural numbers to the set of nonnegative real numbers. f (n) is said to be
        Θ(g(n)) if there exists a natural number n0 and two positive constants c1
        and c2 such that

                                 ∀ n ≥ n0 , c1 g(n) ≤ f (n) ≤ c2 g(n).

        Consequently, if limn→∞ f (n)/g(n) exists, then
                                     f (n)
                               lim         = c implies f (n) = Θ(g(n)),
                              n→∞    g(n)
        where c is a constant strictly greater than 0 .

              An important consequence of the above deﬁnition is that

              f (n) = Θ(g(n)) if and only if f (n) = O(g(n)) and f (n) = Ω(g(n)).

        Unlike the previous two notations, the Θ-notation gives an exact picture of
        the rate of growth of the running time of an algorithm.
May 7, 2022   11:14          Parallel Algorithms      9in x 6in     b4591-app                 page 359




                                       Mathematical Preliminaries                       359

        A.1.4         The o-notation
        Deﬁnition A.4 Let f (n) and g(n) be two functions from the set of nat-
        ural numbers to the set of nonnegative real numbers. f (n) is said to be
        o(g(n)) if for every constant c > 0 there exists a positive integer n0 such
        that f (n) < cg(n) for all n ≥ n0 . Consequently, if limn→∞ f (n)/g(n) exists,
        then
                                        f (n)
                                 lim          = 0 implies f (n) = o(g(n)).
                                n→∞     g(n)


            Informally, this deﬁnition says that f (n) becomes insigniﬁcant relative
        to g(n) as n approaches inﬁnity. It follows from the deﬁnition that
              f (n) = o(g(n)) if and only if f (n) = O(g(n)), but g(n) = O(f (n)).
        For example, n log n is o(n2 ) is equivalent to saying that n log n is O(n2 )
        but n2 is not O(n log n).


        A.2      Divide-and-conquer Recurrences

        Lemma A.1 Let a and c be nonnegative integers, b, d and x nonnegative
        constants, and let n = ck , for some nonnegative integer k. Then, the solution
        to the recurrence
                                      
                                         d               if n = 1
                             f (n) =
                                         af (n/c) + bn x
                                                         if n ≥ 2
        is
                      f (n) = bnx logc n + dnx                           if a = cx ,
                                                              
                                      bcx                   bcx
                      f (n) = d +            n logc a
                                                      −            nx    if a = cx .
                                    a − cx                a − cx



        Corollary A.1 Let a and c be nonnegative integers, b, d and x nonneg-
        ative constants, and let n = ck , for some nonnegative integer k. Then, the
        solution to the recurrence
                                    
                                      d                if n = 1
                            f (n) =
                                      af (n/c) + bnx if n ≥ 2
May 7, 2022   11:14     Parallel Algorithms           9in x 6in         b4591-app        page 360




        360                                   Parallel Algorithms

        satisﬁes

                         f (n) = bnx logc n + dnx                   if a = cx ,
                                         
                                    bcx
                         f (n) ≤            nx                      if a < cx ,
                                   cx − a
                                             
                                         bcx
                         f (n) ≤ d +            nlogc a             if a > cx .
                                       a − cx


        Proof. If a < cx , then logc a < x, or nlogc a < nx . If a > cx , then
        logc a > x, or nlogc a > nx . The rest of the proof follows immediately from
        Lemma A.1.                                                                 

        Theorem A.1 Let a and c be nonnegative integers, b, d and x nonnegative
        constants, and let n = ck , for some nonnegative integer k. Then, the solution
        to the recurrence
                                      
                                         d               if n = 1
                             f (n) =
                                         af (n/c) + bnx if n ≥ 2
        is
                                      ⎧
                                      ⎨ Θ(nx )      if a < cx .
                               f (n) = Θ(n log n) if a = cx .
                                           x
                                      ⎩
                                        Θ(nlogc a ) if a > cx .

        In particular, if x = 1, then
                                       ⎧
                                       ⎨ Θ(n)        if a < c.
                                f (n) = Θ(n log n) if a = c.
                                       ⎩
                                         Θ(nlogc a ) if a > c.


        Example A.1        Consider the recurrence
                                     
                                      1                           if n = 1
                             f (n) =            √
                                      f (n/2) + n                 if n ≥ 2.

        By Corollary A.1, since a = b = 1, c = 2, x = 0.5, we have
                                        √
                                           2 √          √
                              f (n) ≤ √         n = Θ( n).                          
                                        2−1
May 7, 2022   11:14         Parallel Algorithms           9in x 6in         b4591-app                   page 361




                                       Mathematical Preliminaries                                361

        Example A.2            Consider the recurrence
                                        
                                          1                            if n = 1
                                f (n) =
                                          f (n/2) + h(n)               if n ≥ 2.
        Then,
                                                        
                                                        n
                                                                  n
                                              f (n) =         h      .
                                                        i=0
                                                                  2i

        If we let h(n) = log n, and n = 2k , then the solution to the recurrence
                                     
                                       1                if n = 1
                             f (n) =
                                       f (n/2) + log n if n ≥ 2
        is
                  
                  k                    
                                       k                      
                                                              k
                                                                                k(k + 1)
        f (n) =                 i
                        log(n/2 ) =          (log n − i) =          (k − i) =            = Θ(log2 n).
                  i=0                  i=0                    i=0
                                                                                   2

                                                                                                   


        A.3      Summations

        The arithmetic series:
                                        
                                        n
                                                   n(n + 1)
                                              j=            = Θ(n2 ).                          (A.1)
                                        j=1
                                                      2

        The sum of squares:
                                 
                                 n
                                               n(n + 1)(2n + 1)
                                       j2 =                     = Θ(n3 ).                      (A.2)
                                 j=1
                                                      6

        The geometric series:
                                 
                                 n
                                              cn+1 − 1
                                       cj =            = Θ(cn ),          c = 1.              (A.3)
                                 j=0
                                                c−1

        If c = 2, we have
                                       
                                       n
                                              2j = 2n+1 − 1 = Θ(2n ).                          (A.4)
                                       j=0
May 7, 2022   11:14           Parallel Algorithms           9in x 6in         b4591-app                   page 362




        362                                         Parallel Algorithms


        If c = 1/2, we have

                                        n
                                            1         1
                                              j
                                                = 2 − n < 2 = Θ(1).                              (A.5)
                                        j=0
                                            2        2

        When | c | < 1 and the sum is inﬁnite, we have the inﬁnite geometric series
                                    ∞
                                                    1
                                           cj =         = Θ(1),         | c | < 1.               (A.6)
                                     j=0
                                                    1−c

        Diﬀerentiating both sides of Eq. (A.3) and multiplying by c yields

          
          n             
                        n
                                      ncn+2 − ncn+1 − cn+1 + c
                jcj =         jcj =                            = Θ(ncn ),                 c = 1. (A.7)
          j=0           j=1
                                              (c − 1)2

        Letting c = 1/2 in Eq. (A.7) yields

                                  n
                                      j     n
                                                j        n+2
                                        j
                                          =       j
                                                    = 2 − n = Θ(1).                              (A.8)
                                  j=0
                                      2     j=1
                                                2         2

        Diﬀerentiating both sides of Eq. (A.6) and multiplying by c yields
                                  ∞
                                                    c
                                        jcj =              = Θ(1),        | c | < 1.             (A.9)
                                  j=0
                                                  (1 − c)2




        A.4      Probability

        A.4.1         Random variables and expectation
        A random variable X is a function from the sample space to the set of real
        numbers. For example, we may let X denote the number of heads appearing
        when throwing 3 coins. Then, the random variable X takes on one of the
        values 0, 1, 2, and 3 with probabilities
            Pr[X = 0] = Pr[{T T T }] = 18 , Pr[X = 1] = Pr[{HT T, T HT, T T H}] =
        3                                                  3
        8 , Pr[X = 2] = Pr[{HHT, HT H, T HH}] = 8 and Pr[X = 3] =
                          1
        Pr[{HHH}] = 8 .
May 7, 2022   11:14        Parallel Algorithms           9in x 6in          b4591-app            page 363




                                     Mathematical Preliminaries                            363


           The expected value of a (discrete) random variable X with range S is
        deﬁned as
                                                   
                                        E[X] =           xPr[X = x].
                                                   x∈S


        For example, if we let X denote the number appearing when throwing a
        die, then the expected value of X is

                          
                          6
                                                   1                          7
                E[X] =          kPr[X = k] =         (1 + 2 + 3 + 4 + 5 + 6) = .        (A.10)
                                                   6                          2
                          k=1


           E[X] represents the mean of the random variable X and is often writ-
        ten as μX or simply μ. An important and useful property is linearity of
        expectation:

                                             
                                             n             
                                                           n
                                        E          Xi =           E[Xi ],
                                             i=1            i=1


        which is always true regardless of independence.


        A.4.2         Bernoulli distribution
        A Bernoulli trial is an experiment with exactly two outcomes, e.g., ﬂipping
        a coin. These two outcomes are often referred to as success and failure with
        probabilities p and q = 1 − p, respectively. Let X be the random variable
        corresponding to the toss of a biased coin with probability of heads 13 and
        probability of tails 23 . If we label the outcome as successful when heads
        appear, then
                                            
                                             1     if the trial succeeds
                                    X =
                                             0     if it fails.

        A random variables that assumes only the numbers 0 and 1 is called an
        indicator random variable. The expected value and variance of an indicator
        random variable with probability of success p are given by

                            E[X] = p        and var[X] = pq = p(1 − p).
May 7, 2022   11:14         Parallel Algorithms           9in x 6in              b4591-app   page 364




        364                                       Parallel Algorithms

        A.4.3         Binomial distribution
                     n
        Let X =      i=1 Xi , where the Xi ’s are indicator random variables corre-
        sponding to n independent Bernoulli trials with parameter p (identically
        distributed). Then, X is said to have the binomial distribution with param-
        eters p and n. The probability that there are exactly k successes is given by
                                                
                                                 n k n−k
                                 Pr[X = k] =        p q    ,
                                                 k

        where q = 1 − p. The expected value and variance of X are given by:

                          E[X] = np         and var[X] = npq = np(1 − p).

        The ﬁrst equality follows from the linearity of expectations, and the second
        follows from the fact that all Xi  s are pairwise independent.
            For example, the probabilities of getting k heads, 0 ≤ k ≤ 4, when
        tossing a fair coin 4 times are
                                             1 1 3 1 1
                                               , , , ,   .
                                             16 4 8 4 16
        E[X] = 4 × (1/2) = 2, and var[X] = 4 × (1/2) × (1/2) = 1.


        A.4.4         Chernoﬀ bounds
        Let X1 , X2 , . . . , Xn be a collection of n independent indicator random
        variables representing Bernoulli trials such that each Xi has probability
        Pr[Xi = 1] = pi . We are interested in bounding the probability that their
                      n
        sum X = i=1 Xi will deviate from the mean μ = E[X] by a multiple of μ.

        A.4.4.1       Lower tail

        Theorem A.2 Let δ be some constant in the interval (0,1). Then,
                                                      μ
                                               e−δ
                    Pr[X < (1 − δ)μ] <                    ,
                                          (1 − δ)(1−δ)
        which can be simpliﬁed to
                                                                        2
                                      Pr[X < (1 − δ)μ] < e−μδ               /2
May 7, 2022   11:14         Parallel Algorithms         9in x 6in             b4591-app         page 365




                                      Mathematical Preliminaries                          365

        A.4.4.2       Upper tail

        Theorem A.3            Let δ > 0 Then,
                                                                              μ
                                                                   eδ
                               Pr[X > (1 + δ)μ] <                                   ,
                                                              (1 + δ)(1+δ)
        which can be simpliﬁed to
                                                              2
                            Pr[X > (1 + δ)μ] < e−μδ               /4
                                                                        if δ < 2e − 1,
        and
                             Pr[X > (1 + δ)μ] < 2−δμ                   if δ > 2e − 1.



        Example A.3        We seek the probability that the number of heads in a
        sequence of n ﬂips of a fair coin is at least 2n/3.
           Let μ = E[X] = n/2. Solving for δ,
                                                                  2n
                                                  (1 + δ)μ =
                                                                   3
        gives δ = 13 . We apply Chernoﬀ bound of Theorem A.3. Since δ < 2e − 1,
        we have
                                         
                                       2n         2
                               Pr X ≥       < e−μδ /4
                                        3
                                                        = e−(n/2)(1/9)/4
                                                        = e−n/72 .
        So, we see that there is an exponential fall oﬀ.                                   
                             B1948   Governing Asia




                      This page intentionally left blank




B1948_1-Aoki.indd 6                                        9/22/2014 4:24:57 PM
May 7, 2022    11:15       Parallel Algorithms    9in x 6in     b4591-bib                  page 367




                                       Bibliography



              [1] Akers, S. B., Harel, D. and Krishnamurthy, B., “The star graph: An
                  attractive alternative to the n-cube”, Proceeding of the International
                  Conference on Parallel Processing, 393–400, 1987.
              [2] Akers, S. B. and Krishnamurthy, B., “A group-theoretic model for
                  symmetric interconnection networks”, IEEE Transactions on Com-
                  puters, 38(4), 555–566, 1989.
              [3] Akhgari, E., Ziaie, A. and Ghodsi, M., “Sorting on OTIS-Networks”,
                  In: Sarbazi-Azad, H., Parhami, B., Miremadi SG. and Hessabi, S.
                  (eds.) Advances in Computer Science and Engineering. CSICC 2008.
                  Communications in Computer and Information Science, Vol. 6.
                  Springer, Berlin, Heidelberg, 871–875, 2008.
              [4] Akl, S. G., The Design and Analysis of Parallel Algorithms, Prentice
                  Hall, Englewood Cliﬀs, New Jersey, 1989.
              [5] Akl, S. G., Parallel Computation: Models and Methods, Prentice Hall,
                  Upper Saddle River, Florida, 1997.
              [6] Akl, S. G., Parallel Sorting Algorithms, Academic Press, Englewood
                  Cliﬀs, NJ, 1985.
              [7] Akl, S. G., “An optimal algorithm for parallel selection”, Information
                  Processing Letters, 19, 47–50, 1984.
              [8] Akl, S. G. and Lyons, K. A., Parallel Computational Geometry, Pren-
                  tice Hall, Englewood Cliﬀs, New Jersey, 1993.
              [9] Akl, S. G., Qiu, K. and Stojmenovic, I., “Fundamental algorithms for
                  the star and pancake interconnection networks with applications to
                  computational geometry”, Networks, Special Issue: Interconnection
                  Networks and Algorithms, 23, 215–226, 1993.


                                                 367
May 7, 2022   11:15    Parallel Algorithms           9in x 6in     b4591-bib          page 368




        368                                  Parallel Algorithms


         [10] Al-Sadi, J., Awwad, A. M. and AlBdaiwi,“Eﬃcient routing algorithms
              on OTIS-Star network”, Proceedings of the IASTED International
              Conference on Advances in Computer Science and Technology, Virgin
              Islands, U.S.A., ACTA Press, 157–162, 2004.
         [11] Alsuwaiyel, M.H., “An eﬃcient and adaptive algorithm for multi-
              selection on the PRAM”, Proceeding of the International Confer-
              ence on Software Engineering, Artiﬁcial Intelligence, Networking and
              Parallel/Distributed Computing (SNPD01) Nagoya, Japan, 140–143,
              2001.
         [12] Arnould, E., Kung, H., Menzilcioglu, O. and Sarocky, K., “A systolic
              array computer”, Proc. IEEE International Conference on Acoustics,
              Speech, and Signal Processing, 10, 232–235, 1985.
         [13] Awwad, A. M.,“OTIS-Star: An attractive alternative network”, Pro-
              ceedings of the 4th WSEAS International Conference on Software
              Engineering, Parallel & Distributed Systems, 37–41, 2005.
         [14] Awwad, A. M. and Al-Sadi, J., “Investigating the distributed load
              balancing approach for OTIS-Star topology”, International Journal
              of Computer Science and Information Security (IJCSIS), 14, no. 3,
              163–171, 2016.
         [15] Batcher, K., “Sorting networks and their applications”, AFIPS Spring
              Joint Computing Conference, Atlantic City, NJ, 307–314, 1968.
         [16] Blahut, R. E., Fast Algorithms for Digital Signal Processing, Addison
              Wesley, Reading, MA, 1985.
         [17] Borodin, A. and Moenck, R., “Fast modular transforms”, Journal of
              Computer and System Sciences, 8, 366–386, 1974.
         [18] L. E. Cannon, “A cellular computer to implement the Kalman ﬁlter
              algorithm”, Ph.D. Thesis, Montana State University, 1969.
         [19] Cantoni, V. and Levialdi, S., Eds. Pyramidal Systems for Computer
              Vision, Springer, Berlin, 1986.
         [20] Chandran, S. and Rosenfeld, A., “Order statistics on a hypercube”,
              Information Processing Letters, 27, 129–132, 1988.
         [21] Chaudhuri, P., Parallel Algorithms: Design and Analysis, Prentice
              Hall, Sydney, Australia, 1992.
         [22] Chiang, W. K., “Topological properties of the (n, k)-star graph”,
              International Journal of Foundations of Computer Science, 9(2),
              235–248, 1998.
         [23] Chiang, W. K. and Chen, R. J., “The (n, k)-star graph: A generalized
              star graph”, Information Processing Letters, 56, 259–264, 1995.
         [24] Christopher, T., “An implementation of Warshalls algorithm for tran-
              sitive closure on a cellular computer, Technical Report 36, Institute
              for Computer Research, University of Chicago, Chicago, IL, 1973.
May 7, 2022   11:15    Parallel Algorithms           9in x 6in   b4591-bib            page 369




                                             Bibliography                      369


         [25] Cinque, L. and Bongiovanni, G., “Parallel preﬁx computation on a
              pyramid computer”, Pattern Recognition Letters, 16, 19–22, 1995.
         [26] Cole, R., “Parallel merge sort”, SIAM Journal on Computing, 17(4),
              770–785, 1988.
         [27] Cooley, J. M. and Tukey, J. W., “An algorithm for machine calcu-
              lation of complex Fourier series”, Mathematics of Computation, 19,
              297–301, 1965.
         [28] Cook, S. A., “A taxonomy of problems with fast parallel algorithms”,
              Information and Control , 64, 2–22, 1985.
         [29] Cosnard, M. and Trystram, D., Parallel Algorithms and Architectures,
              International Thomson Computer Press, London, 1995.
         [30] Cypher, R. and Plaxton, G., “Deterministic sorting in nearly log-
              arithmic time on the hypercube and related computers”, In Pro-
              ceeding of the 22nd ACM Symp. Theory of Computing, ACM Press,
              1990.
         [31] Day, K. and Al-Ayyoub, A., “Topological properties of OTIS-
              networks”, IEEE Transactions on Parallel and Distributed Systems,
              13(4), 359–366, 2002.
         [32] Day, K. and Tripathy, A., “Arrangement graphs: A class of generalized
              star graph”, Information Processing Letters, 42, 235–241, 1992.
         [33] Dekel, E., Nassimi, D. and Sahni, S.,“Parallel matrix and graph algo-
              rithms”, SIAM Journal on Computing, 10(4), pp. 307–315, 1981.
         [34] Dietzfelbinger, M., Madhavapeddy, S. and Sudborough, I. H., “Three
              disjoint path paradigms in star networks”, Proceedings of the Third
              IEEE Symposium on Parallel and Distributed Processing, 400–406,
              1991.
         [35] Durad, M. H., Akhtar, M. N. and Irfan-ul-Haq, “Performance anal-
              ysis of parallel sorting algorithms Using MPI”, 2014 12th Interna-
              tional Conference on Frontiers of Information Technology, Islamabad,
              pp. 202–207, 2014.
         [36] Fiduccia, C. M., “Polynomial evaluation via the division algo-
              rithm:The fast Fourier transform revisited”, In Proceeding of the
              fourth ACM Symposium on Theory of Computing, Denver, CO, 88–
              93, 1972.
         [37] Gibbons, A. and Rytter, W., Eﬃcient Parallel Algorithms,
              Cambridge University Press, London, 1990.
         [38] Greenlaw, R., Hoover, J. and Ruzzo, W., Limits to Parallel Compu-
              tation: P-completeness Theory, Oxford University Press, New York,
              1995.
May 7, 2022   11:15    Parallel Algorithms           9in x 6in     b4591-bib         page 370




        370                                  Parallel Algorithms


         [39] Grama, A., Gupta, A., Karypis, G. and Kumar, V., Introduction to
              Parallel Computing, Addison-Wesley, New York, 2003.
         [40] Gupta, A. and Sarkar, B. K., “Shortest path routing on OTIS hyper
              hexa-cell”, 2017 8th International Conference on Computing, Com-
              munication and Networking Technologies (ICCCNT), pp. 1–6, 2017.
         [41] Gupta, A., Singh, H. and Bhati, A., “Eﬃcient parallel algorithm for
              mapping LaGrange’s interpolation on OTIS and BSN hyper hexa-
              cell”, 2020 International Conference on Emerging Smart Computing
              and Informatics (ESCI), AISSMS Institute of Information Technol-
              ogy, Pune, India, pp. 82–87, 2020.
         [42] He, L., “Properties and Algorithms of the (n, k)-Star Graphs”,
              Ms Thesis, Faculty of Mathematics and Science, Brock University,
              St. Catharines, Ontario, Canada, 2008.
         [43] Horowitz, E., Sahni, S. and Rajasekaran, S., Computer Algorithms,
              Computer Science Press, Rockville, MD, 1998.
         [44] JáJá, J., An Introduction to Parallel Algorithms, Addison-Wesley,
              Reading, MA, 1992.
         [45] Jan, G. E. and Huang, Y. S., “A simple algorithm for optimal load
              balancing on hypercube multiprocessors, Proceedings of 2001 Interna-
              tional Conference on Parallel and Distributed Processing Techniques
              and Applications, Las Vegas, Nevada, USA, 17, 2001.
         [46] JáJá, J. and Ryu, K. W., “Load balancing on the hypercube and
              related networks”, The Proceeding of the 1990 International Confer-
              ence on Parallel Processing, pp. I203–I210, 1990.
         [47] Kunde, M., “Ruting and sorting on mesh-connected arrays”, Pro-
              ceeding of Third Agean Workshop on Computing: VLSI Algorithms
              and Architectures, Vol. 319 of Lecture notes in computer science,
              pp. 423–433, Springer-Verlag, 1988.
         [48] Kronsjo, L. Algorithms: Their Complexity and Eﬃciency, Wiley,
              New York, NY, 1987.
         [49] Kruscal, C., “Searching, merging and sorting in parallel computa-
              tion”, IEEE Transactions on Computers, C-32(10), 942–946, 1983.
         [50] Kung, H.T., “Why systolic architectures?”, IEEE Computers, 15,
              37–46, 1982.
         [51] Kung, S.Y., VLSI Array Processors. Prentice Hall, Englewood Cliﬀs,
              NJ, 1988.
         [52] Lakshmivarahan, S. and Dhall, S. K., Analysis and Design of Par-
              allel Algorithms: Arithmetic and Matrix Problems, McGraw-Hill,
              New York, 1990.
May 7, 2022   11:15    Parallel Algorithms           9in x 6in   b4591-bib             page 371




                                             Bibliography                       371


         [53] Lakshmivarahan, S. and Dhall, S. K., Parallel Computing Using the
              Preﬁx Problem, Oxford University Press, New York, 1994.
         [54] Lee, D.T., Hsu, C. and Wong, C. K., “An on-chip compare/steer
              bubble sorter”, IEEE Transactions on Computers, c-30(6), 396–404,
              1981.
         [55] Leighton, F.T., Complexity Issues in VLSI , MIT Press, Cambridge,
              MA, 1983.
         [56] Leighton, F.T., “Tight bounds on the complexity of parallel sorting”,
              IEEE Transactions on Computers, c-34(4), 344–354, 1985.
         [57] Leighton, F.T., Introduction to Parallel Algorithms and Architec-
              tures: Arrays, Trees and Hypercubes, Morgan Kaufmann Publishers,
              San Mateo, CA, 1992.
         [58] Leighton, F.T., Makedon, F. and Tollis, I., “A 2n − 2 step algorithm
              for routing in an n × n mesh”, Proceeding of the ACM Symposium on
              Parallel Algorithms and Architectures, 328–335, 1989.
         [59] Li, Y. and Qiu, K.,“Routing, broadcasting, preﬁx sums, and sort-
              ing algorithms on the arrangement graph”, 2009 15th International
              Conference on Parallel and Distributed Systems, 324–331, 2009.
         [60] Lucas, K. T. and Jana, P. K., “An eﬃcient parallel sorting algorithm
              on OTIS mesh of trees”, 2009 IEEE International Advance Comput-
              ing Conference (IACC 2009), 175–180, 2009.
         [61] Lucas, K. T. and Jana, P. K., “Sorting and routing on OTIS mesh of
              trees”, Parallel Processing Letters, 20, no. 2, 145–154, 2010.
         [62] Mahafzah, B. A., Sleit, A., Hamad, N. A., Ahmad, E. F. and
              Abu-Kabeer, T. M., “The OTIS hyper hexa-cell optoelectronic archi-
              tecture”, J. Computing, 94(5), 411–432, 2012.
         [63] Marsden, G., Marchand, P., Harvey, P. and Esener, S., “Optional
              transpose interconnection system architecture”, Optics Letters,
              18(13), 1083–1085, 1993.
         [64] McClellan, J. H. and Rader, C. M. Number Theory in Digital Signal
              Processing, Prentice Hall, Englewood Cliﬀs, NJ, 1979.
         [65] Mendia, V. E. and Sarkar, D., “Optimal broadcasting on the star
              graph”, IEEE Transactions on Parallel and Distributed Systems, 3(4),
              389–396, 1992.
         [66] Miller, R. and Boxer, L., Algorithms Sequential & Parallel , Prentice-
              Hall, Englewood Cliﬀs, NJ, 2000.
         [67] Miller, R. and Stout, Q. F., Parallel Algorithms for Regular Architec-
              tures: Meshes and Pyramids, MIT Press, Cambridge, MA, 1996.
         [68] Miranker, G., Tang, L. and Wong, C. K., “A zero-time VLSI sorter”,
              IBM J. Res. Develop., 27(2), 140–147, 1983.
May 7, 2022   11:15    Parallel Algorithms           9in x 6in     b4591-bib           page 372




        372                                  Parallel Algorithms


         [69] Najaf-abadi, H. H. and Sarbazi-azad, H.,“Comparative evaluation
              of adaptive and deterministic routing in the OTIS-hypercube”,
              Proceeding of the 9th Asia-Paciﬁc Computer Systems Architecture
              Conference (ACSAC), in LNCS 3189, pp. 349–362, 2004.
         [70] Najaf-abadi, H. H. and Sarbazi-azad, H.,“An empirical comparison
              of OTIS-mesh and OTIS-hypercube multicomputer systems under
              deterministic routing”, Proceeding of the 14th IEEE International
              Parallel and Distributed Processing Symposium, p. 262-a, IEEE Press,
              New York, 2005.
         [71] Nassimi, D. and Sahni, S., “Parallel permutation and sorting algo-
              rithms and a generalized interconnection network”, Journal of the
              ACM , 29(3), 642–667, 1982.
         [72] Navarro, J., Llaberia, J. and Valero, M., “Partitioning: An essential
              step in mapping algorithms into systolic array processors”, Computer ,
              20(7), 77–89, July 1987.
         [73] Osterloh, A.,“Sorting on the OTIS-mesh”, 19th IEEE Interna-
              tional Parallel and Distributed Processing Symposium (IPDPS 2000),
              269–274, 2000.
         [74] Plaxton, C. G. “Load balancing, selection and sorting on the hyper-
              cube”, Proceeding of the 1989 ACM Symposium on Parallel Algo-
              rithms and Architectures, 64–73, 1989.
         [75] Rajasekaran, S., “Sorting and selection on interconnection networks”,
              DIMACS Series in Discrete Mathematics and Theoretical Computer
              Science, 21, 275–296, 1995.
         [76] Rajasekaran, S. and Sahni, S., “Randomized routing, selection, and
              sorting on the OTIS-mesh”, IEEE Transactions on Parallel and
              Distributed Systems, 9(9), 833–840, 1998.
         [77] Roosta, S. H., Parallel Processing and Parallel Algorithms: Theory
              and Computation, Springer-Verlag, New York, 2000.
         [78] Rosenfeld, A., Ed. Multiresolution Image Processing and Analysis,
              Springer, Berlin, 1984.
         [79] Sado, K. and Igarashi, Y., “Some parallel sorts on a mesh-connected
              processor array and their time eﬃciency”, Journal of Parallel and
              Distributed Computing, 3, 398–410, 1986.
         [80] Sahni, S. and Wang, C-F., “BPC permutations on the OTIS-
              hypercube optoelectronic computer”, Informatica, 22, 263–269, 1998.
         [81] Scherson, I., Sen, S. and Shamir, A., “Shear-sort: A true two-
              dimensional sorting technique for VLSI networks”, Proceeding of the
              International Conference on Parallel Processing, 903–908, 1986.
         [82] Shamos, M. I., “Computational Geometry”, PhD Thesis, Department
              of Computer Science, Yale University, New Haven, CT, 1978.
May 7, 2022   11:15     Parallel Algorithms           9in x 6in   b4591-bib             page 373




                                              Bibliography                       373


         [83] Shen, H., “Eﬃcient parallel multiselection on hypercubes”, Proceeding
              of the 1997 International Symp. on Parallel Architectures, Algorithms
              and Networks (I-SPAN), IEEE CS Press, 338–342, 1997.
         [84] Shen, H., “Optimal multiselection in hypercubes”, Parallel Algo-
              rithms and Applications, 14, 203–212, 2000.
         [85] Sheu, J. P., Wu, C. T. and Chen, T. S., “An optimal broadcasting
              algorithm without message redundancy in star graphs”, IEEE Trans-
              actions on Parallel and Distributed Systems, 6(6), 653–658, 1995.
         [86] Shi, H. and Schaeﬀer, J., “Parallel sorting by regular sampling”, Jour-
              nal of Parallel and Distributed Computing, 14, 361–372, 1990.
         [87] Shiloach, Y. and Vishkin, U., “Finding the maximum, merging and
              sorting in a parallel computation model”, Journal of Algorithms, 2(1),
              88–102, 1981.
         [88] Stojanovic, N. M. ,Milovanovic, I. Z.,Stojcev, M. K. and Milovanovic,
              E. I., “Matrix-vector Multiplication on a Fixed Size Unidirectional
              Systolic Array”, 2007 8th International Conference on Telecom-
              munications in Modern Satellite, Cable and Broadcasting Services,
              457–460, 2007.
         [89] Stout, Q., “Sorting, merging, selecting, and ﬁltering on tree and pyra-
              mid machines”, Proceeding of the 1983 International Conference on
              Parallel Processing, 214–221, 1983.
         [90] Szymanski, T., “Hypermesh optical interconnection networks for par-
              allel computing ”, Journal of Parallel and Distributed Computing, 26,
              1–23, 1995.
         [91] Szymanski, T. and Hinton, H., “Architecture of a terabit free-space
              intelligent optical backplane”, Journal of Parallel and Distributed
              Computing, 55(1), 1–31, 1998.
         [92] Thompson, C. and Kung, H., “Sorting on a mesh-connected parallel
              computer”, Communication of the ACM , 20(4), 263–271, 1977.
         [93] Wagar, B., “Hyperquicksort: A fast sorting algorithm for hyper-
              cubes”, in Hypercube Multiprocessors, M.T. Health, ed., SIAM, 292–
              299, 1987.
         [94] Wang, C-F., “Algorithms for the OTIS optoelectronic computer”,
              PhD thesis, Dept. of Computer Science, Univ. of Florida, 1998.
         [95] Wang, C-F. and Sahni, S., “Basic operations on the OTIS-mesh opto-
              electronic computer”, IEEE Transactions on Parallel and Distributed
              Systems, 9(12), 1226–1236, 1998.
         [96] Wang, C-F. and Sahni, S., “Image processing on the OTIS-mesh opto-
              electronic computer”, IEEE Transactions on Parallel and Distributed
              Systems, 11(2), 97–109, 2000.
May 7, 2022   11:15    Parallel Algorithms           9in x 6in     b4591-bib          page 374




        374                                  Parallel Algorithms


         [97] Wang, C-F. and Sahni, S., “Matrix multiplication on the OTIS-mesh
              optoelectronic computer”, IEEE Transactions on Computers, 50(7),
              635–646, 2001.
         [98] Winograd, S. Arithmetic Complexity of Computation, SIAM Pub-
              lishers, 1980.
         [99] Woo, J. and Sahni, S., “Load balancing on a hypercube”, 1991 Pro-
              ceedings of the Fifth International Parallel Processing Symposium,
              525–530, 1991.
        [100] Valiant, L. G., “A scheme for fast parallel communication”, SIAM
              Journal on Computing, 11, 350–361, 1982.
        [101] Valiant, L. G. and Brebner, G. L., “Universal schemes for parallel
              communication”, Proceedings of the 13th ACM Symposium on Theory
              of Computing, Milwaukee, WI, 263–277, 1–9, 1981.
        [102] Vishkin, U., “An optimal parallel algorithm for selection”, Advances
              in Computing Research, JAI Press Inc., Greenwich, CT, 1987.
        [103] Zane, F., Marchand, P., Pahuri, R. and Esener, S., “Scalable net-
              work architectures using the optical transpose interconnection system
              (OTIS)”, Proceeding of the Second International Conference Mas-
              sively Parallel Processing Using Optical Interconnections (MPPOI’
              96), 114–121, 1996.
        [104] Xavier, C. and Iyengar, S. S., Introduction to Parallel Algorithms,
              John Wiley, New York, 1998.
May 7, 2022   11:16     Parallel Algorithms       9in x 6in       b4591-index                 page 375




                                              Index



        Θ(1) time, 11                                  bit ﬁxing, 106–107, 110, 144
                                                       bitonic merging, 39–40
        A                                              bitonic merging and sorting, 35, 43
        acyclic graph, 210, 222                        bitonic sequence, 35, 37
        ARBITRARY, 8                                   bitonic sort network, 71, 80
        area of a picture, 274, 277                    bitonic sorting, 40, 75, 92
        arrangement graph, 296, 304, 307–308           bitonicmerge algorithm, 39–42
        array packing, 16–17, 77                       bitonicsort algorithm, 40–41
                                                       bottom-up merge sorting, 31
        B                                              breadth-ﬁrst spanning tree, 210, 222
        balanced tree method, 8                        Brent theorem, 10, 74, 90
        Bernoulli distribution, 363                    broadcasting, 70, 78, 104, 138, 144,
        Bernoulli trial, 363                             162, 207, 212, 294–295, 304, 307,
        bfoddevenmerge algorithm,                        316, 326, 332–334, 336, 339
          128–129, 132, 142, 146, 154                  broadcasting in OTIS-Mesh, 316, 332,
        bfoddevenmergesort algorithm,                    334
          130, 132, 328                                broadcasting in the hypercube, 104,
        bfparprefix algorithm, 126–127, 271              138, 144
        binomial distribution, 364                     broadcasting in the mesh, 162
        bipartite graph, 75, 92                        broadcasting in the OTIS-Hypercube,
        bisection width, 95, 141, 151, 207,              326, 333, 336
          213, 274–277, 279                            broadcasting in the OTIS-Star, 333,
        bisection width of a network, 4                  339
        bisection width of the butterﬂy, 141,          broadcasting in the ring, 207, 212
          151                                          broadcasting in the star network,
        bisection width of the hypercube, 95             294–295, 304, 307
        bisection width of the mesh, 207, 213          bucketsort, 139–140, 147–148
        bisection width of the torus, 207, 213         butterﬂy, 96, 98, 110

                                                 375
May 7, 2022    11:16       Parallel Algorithms          9in x 6in      b4591-index                  page 376




        376                                      Parallel Algorithms

              bisection width of, 141, 151                   simulating a hypercube on, 144,
              odd–even merging and sorting on,                  158
                 127, 130                                    sum on, 143, 156
              odd–even merging on, 127, 130,               cyclic graph, 210, 222
                 142, 154
              parallel preﬁx on, 126, 143, 155             D
              permutation routing in, 110
                                                           1-dimensional pyramid, 259
              preﬁx sum on, 126, 143, 155
                                                              bisection width of, 274, 276
                                                              lower bound on routing, 274,
        C
                                                                 276
        CCC, see cube-connected cycles, 143,                  lower bound on sorting, 274, 276
          364–365                                          2-dimensional pyramid, 260
           lower tail, 364                                    bisection width of, 274, 277
           upper tail, 365                                    diameter of, 274, 276
        columnsort, 196–198, 211, 223–225                     lower bound on routing, 274, 277
        columnsort algorithm, 196–202,
                                                              lower bound on sorting, 274, 277
          211, 223–224
                                                           3-dimensional mesh, 202
        columnsort2 algorithm, 197–199,
                                                              sorting on, 202, 204, 212, 225, 226
          211, 223
                                                           data movements in OTIS-Mesh, 315
        COMMON, 7
                                                           degree of OTIS-mesh of trees, 333,
        component labeling, 191–192, 212,
                                                              339
          226
                                                           degree of a network, 4
        concurrent read concurrent write,
          7                                                depth-ﬁrst numbering in a tree, 24
        concurrent read exclusive write, 7                 deterministic routing on
        congestion, 161                                       OTIS-Hypercube, 333, 339
        connected components, 184                          deterministic routing on OTIS-Mesh,
        convex hull, 185, 187, 189                            322
           on the mesh, 185, 187, 189, 191                 deterministic routing on the mesh,
           parallel algorithm, 63, 68                         173
        convolution, 238–239, 245, 249,                    DFT, see discrete Fourier transform,
          342–345, 352, 355                                   228
           computing, 238–239, 245, 249                    diameter, 274, 276, 278
           systolic array for, 342–345, 352, 355              of OTIS-mesh of trees, 333,
        CRCW, 7–8                                                339–340
        CREW, 7                                            diameter of a network, 4
        cube-connected cycles, 143, 156                    dilation, 161
           bisection width of, 144, 156                    dimension k edge, 95
           degree of, 144, 156                             directing a tree, 25–26, 73, 87
           diameter of, 144, 156                           directingtree algorithm, 25
           embedding of hypercube into, 144,               discrete Fourier transform, 228
              157                                          divide and conquer recurrence, 359
           parallel preﬁx on, 144, 157                     dominating set, 305, 310
           preﬁx sum on, 144, 157                          dotproduct algorithm, 56–57
May 7, 2022   11:16     Parallel Algorithms      9in x 6in        b4591-index                     page 377




                                              Index                                         377

        E                                             Fourier transform, see fast Fourier
        embedding, 140, 144, 150, 157, 161,             transform, 227
          207, 212
                                                      G
          linear array into the mesh, 161,
             207, 212                                 Gray codes, 100
          mesh into linear array, 161, 207,           greedy path in butterﬂy, 98
             212
        embedding of a binary tree into               H
          hypercube, 103, 140, 150                    hcbroadcast algorithm, 104–105,
        embedding of a linear array into                144
          hypercube, 101                              hchyperquicksort algorithm,
        embedding of a mesh into hypercube,             113–114
          102                                         hcloadbalance algorithm, 126
        embedding of cube-connected cycles            hcmultiselect algorithm, 120–121,
          into hypercube, 144, 157                      140, 149
        embeddings of the hypercube, 99               hcparprefix algorithm, 112, 142,
        enumeration sort, 11                            153
        ERCW, 7                                       hcselect algorithm, 118, 120–122,
        EREW, 7                                         140, 148–149
        Euler circuit, 22                             hcsum algorithm, 105, 134
        Euler tour, 22, 24, 73, 87                    Horner’s rule, 74, 88
        Eulerian graph, 22                            hypercube, 95, 99
        exclusive read concurrent write, 7              bisection width of, 95
        exclusive read exclusive write, 7               broadcasting in, 104, 138, 144
        expectation, 362                                computing maximum on, 141, 152
        expected value, 363                             computing parallel preﬁx on, 139,
                                                           145–146
        F                                               computing preﬁx sum on, 139,
                                                           145–146
        fast Fourier Transform, 227                     computing sum on, 138, 141, 144,
        fast Fourier transform, 227, 244–247,              151
           249                                          embedding of a binary tree into,
           convolution, 238–239, 245, 249                  103
           implementation on the butterﬂy,              embedding of a linear array into,
              231                                          101
           inverse, 234                                 embedding of a mesh into, 102
           iterative on the butterﬂy, 231               load balancing on, 122, 138, 142,
           modular arithmetic, 241, 243,                   153–154
              245–246, 250–251                          matrix multiplication on, 132, 134,
           product of polynomials, 235, 237,               137, 142, 155
              243–249                                   multiselection on, 140, 149
           Toeplitz matrix, 239–240, 245,               odd–even merging on, 139, 146
              249–250                                   parallel preﬁx on, 112, 142, 153
        FFT, see fast Fourier transform, 227            parallel quicksort on, 139, 146
May 7, 2022   11:16     Parallel Algorithms          9in x 6in      b4591-index                   page 378




        378                                   Parallel Algorithms

          permutation routing in, 105–107,              maximum, 69, 72, 77, 82–83, 141,
             140, 148                                    152, 207, 214, 273–276, 290–291,
          preﬁx sum on, 112, 142, 153                    304, 306
          routing in, 140, 148                          mean, 363
          routing on, 141–142, 152                      merge-split sort, 208–209, 217–218
          sorting on, 113, 115, 139, 147                mergesort, 76, 92–93
        hyperquicksort, 113                               pipelined mergesort, 43, 49
                                                        mergesort algorithm, 40
        I                                               merging, 27, 30, 33, 35, 39, 70–71, 74,
        indicator random variable, 363                   79–80, 89–90, 169–170, 211,
        interconnection networks, 3                      224–225
        interpolation, 235, 236                           odd–even, 33, 70–71, 79–80, 211,
                                                             224–225
        L                                               merging by ranking, 27, 30
                                                        mesh
        labeling connected components,                    deterministic routing on, 173
           191–192, 212, 226                              one-to-one routing on, 209, 218
        laparprefix algorithm, 163–164                    permutation routing on, 172–174,
        linear array, 159                                    209, 218
           broadcasting in, 162                           randomized routing on, 174
           merge-split sort on, 208–209,                mesh network, 159
             217–218                                      3-dimensional, 202
        linearity of expectation, 363                     bisection width of, 207, 213
        load balancing, 122, 138, 142, 153–154            broadcasting in, 162
        lower bound, 274–277, 279                         component labeling, 191–192, 212,
                                                             226
        M                                                 computing maximum on, 207, 214
        many-to-many routing on the                       computing transpose of a matrix
         hypercube, 141–142, 152                             on, 208, 215
        mathematical notations                            odd–even merging and sorting on,
          O-notation, see O-notation, 357                    169–171, 209, 218
          Ω-notation, see Ω-notation, 358                 odd–even merging on, 209, 218
        matrix multiplication, 56, 132, 134,              odd–even transposition sort on,
         137, 142, 155, 177–178, 210, 222                    164–165, 208, 217
        matrix multiplication on the                      parallel preﬁx on, 163, 208–209,
         hypercube, 132, 134, 137                            214, 219
        matrix multiplication on the mesh,                permutation routing on, 207, 213
         177–178, 210, 222                                preﬁx sum on, 163, 208–209, 214,
        matrix multiplication on the PRAM,                   219
         56, 71, 82, 177–178                              routing on, 207, 213
        matrix–matrix multiplication,                     searching in, 210, 221
         352–354                                          sorting on, 202, 204, 207, 210,
        matrix-vector multiplication, 342,                   212–213, 219–220, 225–226
         352–353                                          window broadcast in, 209, 219
May 7, 2022   11:16      Parallel Algorithms      9in x 6in        b4591-index                  page 379




                                               Index                                      379

          with wraparound connections, 159             oddevenmerge algorithm, 33–35,
        mesh of trees, 264, 275, 278                     70, 74, 79, 90, 196–197, 211, 224
          bisection width of, 275, 279                 oddevenmergesort algorithm,
          comparison with pyramid, 272                   34–35, 130, 171
          computing sum on, 275, 278                   on-chip bubble sorter, 347, 349, 353,
          diameter of, 274, 278                          356
          lower bound on routing, 275, 279             Optical transpose interconnection
          lower bound on sorting, 275, 279               system, 313
          parallel preﬁx on, 270, 272                  OR computing logical, 71, 82
          preﬁx sum on, 270, 272                       OTIS, 313
          routing on, 269–270, 275, 279                  data movement in, 331, 334
          sorting on, 266, 268, 275, 279               OTIS-Hypercube, 324
        meshoddevenmerge algorithm,                      broadcasting in, 326, 333, 336
          169–170, 172                                   computing sum on, 326, 333, 336
        meshoddevensort algorithm,                       deterministic routing on, 333, 339
          171–172                                        permutation routing on, 327, 333,
        meshparprefix algorithm, 163–164                    338–339
        meshsortrec algorithm, 167–168                   routing on, 327, 333, 338–339
        minimum spanning tree, 59, 61–62,                semigroup operations on, 326
          75, 90–92                                      simulation of hypercube on, 324
        modparsearch algorithm, 27–29                    sorting on, 327
        multiselection, 52, 71, 81, 119, 140,          OTIS-Mesh, 314
          149                                            broadcasting in, 316, 332, 334
        multiselection on the hypercube, 119,            computing sum on, 316, 332, 335
          140, 149                                       data movements in, 315
                                                         deterministic routing on, 322
        N                                                parallel preﬁx on, 318, 332, 335
                                                         permutation routing on, 322, 332,
        Θ-notation, 358
                                                            336
        normal butterﬂy algorithm, 98                    preﬁx sum on, 318, 332, 335
        normal tree algorithm, 254                       randomized routing on, 322, 332,
        nth root of unity, 227, 244, 246                    336
                                                         routing on, 322, 332, 336
        O                                                semigroup operations on, 316
        O-notation, 357–359                              shift operations on, 320–321, 332,
        Ω-notation, 358                                     335–336
        oblivious sorting algorithm, 32                  sorting on, 324
        odd–even merging, 33, 70–71, 74,                 window broadcast in, 332, 334
          79–80, 90, 127, 130, 139, 142, 146,          OTIS-Mesh of trees, 328
          154, 209, 211, 218, 224–225                    degree of, 333, 339
        odd–even merging and sorting, 127,               diameter of, 333, 339–340
          130, 169–171, 209, 218                       OTIS-Star, 328
        odd–even transposition sort, 164–165,            broadcasting in, 333, 339
          208, 217                                     otishcaddition algorithm, 326–327
May 7, 2022   11:16      Parallel Algorithms          9in x 6in      b4591-index                  page 380




        380                                    Parallel Algorithms

        otismeshbroadcast algorithm, 316,                parmerge algorithm, 30–31, 74, 90
          326, 332, 334                                  parmultiselect1 algorithm, 52, 56,
        otismeshparprefix algorithm,                       71, 81
          318–319                                        parmultiselect2 algorithm, 53–54,
        otismeshshift algorithm, 320–321,                  71, 81–82
          332, 335                                       parprefix algorithm, 14–15, 77
        otismeshsum algorithm, 317, 326,                 parprefixrec algorithm, 15–16, 85
          332, 335                                       parquicksort algorithm, 18, 70, 74,
                                                           78, 89
        P                                                parrank algorithm, 28–30, 70, 78
        paraddition algorithm, 8–9, 11, 56,              parsearch algorithm, 20–21, 27, 70,
          77                                               78
        parallel architectures                           parselect algorithm, 50, 52, 71,
          classiﬁcations of, 1, 5                          80–81, 140, 148
        parallel merging, 74, 89–90                      partial permutation routing on the
        parallel multiselection, 56                        hypercube, 141, 152
        parallel preﬁx, 14, 72, 84, 112, 126,            permutation routing on OTIS-Mesh,
          139, 142–146, 153, 155, 157, 163,                322
          208–209, 214, 219, 262, 270, 272,              permutation routing on the butterﬂy,
          274, 276, 278, 287, 289, 304, 306,               110
          318, 332, 335                                  permutation routing on the
        parallel preﬁx on the butterﬂy, 126,               hypercube, 105–107, 140, 148
          143, 155                                       permutation routing on the mesh,
        parallel preﬁx on the cube-connected               207, 213
          cycles, 144, 157                               permutation routing on the
        parallel preﬁx on the hypercube, 112,              OTIS-Hypercube, 327, 333,
          142, 153                                         338–339
        parallel preﬁx on the mesh, 163,                 permutation routing on the
          208–209, 214, 219                                OTIS-Mesh, 322, 332, 336
        parallel preﬁx on the mesh of trees,             picture
          270, 272                                          area of, 274, 277
        parallel preﬁx on the OTIS-Mesh,                 pipelined mergesort, 43, 49, 76, 92–93
          318, 332, 335                                  pjumping algorithm, 21–22
        parallel preﬁx on the pyramid, 262,              point-value representation, 235
          274, 276, 278                                  pointer jumping, 21, 73, 87
        parallel preﬁx on the star network,              polygon, 71, 82
          287, 289, 304, 306                             polynomial evaluation, 142, 153, 209,
        parallel quicksort, 18, 70, 74, 78, 89,            219
          139, 146                                       postorder numbering in a tree, 73,
        parallel search, 18, 21, 70, 78                    87–88
        parallel sorting, 74, 89                         PRAM, 2, 7–8
        parbottomupsort algorithm, 31, 43                   array packing on, 77
        parconvexhull algorithm, 68, 186                    broadcasting in, 70, 78
        parmatrixmult algorithm, 57                         computing logical OR on, 71, 82
May 7, 2022   11:16     Parallel Algorithms      9in x 6in        b4591-index                  page 381




                                              Index                                      381

          computing maximum on, 69, 72,               processing element, 3
             77, 82–83                                Product of polynomials, 235, 237,
          computing parallel preﬁx on, 72, 84           243, 245, 248
          computing preﬁx minima on, 72, 85           pyramid network, 260, 274, 278
          computing ranks on, 70, 72, 78, 83            bisection width of, 274, 277
          computing suﬃx minima on, 73, 85              comparison with mesh of trees, 272
          matrix multiplication on, 71, 82              computing sum on, 274, 278
          merging on, 74, 89–90                         diameter of, 274, 276
          multiselection on, 71, 81                     lower bound on routing, 274, 277
          odd-even merging on, 70, 79                   lower bound on sorting, 274, 277
          parallel quicksort on, 70, 78, 139,           parallel preﬁx on, 262, 274, 276,
             146                                           278
          parallel search on, 70, 78                    preﬁx sum on, 262, 274, 276, 278
          selection on, 72, 76, 84, 89, 93              routing on, 274, 278
          simulating on a hypercube,                    sorting on, 274, 278
             140–141, 148, 150                        pyramidparprefix algorithm,
          sorting on, 74, 89                            262–274, 276
          suﬃx computation on, 73, 85–86
        prammst algorithm, 59–60                      Q
        preﬁx minima, 72, 85
                                                      quicksort, 70, 74, 78, 89, 139, 146
        preﬁx sum, 112, 126, 139, 142–146,
                                                      quicksort on the hypercube, 139, 146
          153, 155, 157, 163, 208–209, 214,
          219, 262, 270, 272, 274, 276, 278,
                                                      R
          287, 289, 304, 306, 318, 332, 335
        preﬁx sum on the butterﬂy, 126, 143,          random variables, 362
          155                                         randomized routing on OTIS-Mesh,
        preﬁx sum on the cube-connected                  322, 332, 336
          cycles, 144, 157                            randomized routing on the mesh, 174
        preﬁx sum on the hypercube, 112,              rank, 11, 70, 72, 78, 83, 303, 305
          142, 153                                       computing, 27, 30
        preﬁx sum on the mesh, 163,                   recurrence relation
          208–209, 214, 219                              divide and conquer, see divide and
        preﬁx sum on the mesh of trees, 270,                conquer recurrence, 359
          272                                         recursive doubling, 292, 294, 304, 306
        preﬁx sum on the OTIS-Mesh, 318,              reduction, 105, 151
          332, 335                                    ring, 159
        preﬁx sum on the pyramid, 262, 274,              broadcasting in, 207, 212
          276, 278                                    roots of unity, 227
        preﬁx sum on the star network, 287,           routing, 105–107, 110, 140–142, 148,
          289, 304, 306                                  152, 172–174, 207, 209, 213, 218,
        preﬁx sums, 14, 72, 84                           269–270, 274–275, 278–279, 322,
        primitive nth root of unity, 227                 327, 332–333, 336, 338–339
        PRIORITY, 8                                      many-to-many routing on the
        probability, 362                                    hypercube, 141–142, 152
May 7, 2022   11:16      Parallel Algorithms          9in x 6in      b4591-index                  page 382




        382                                    Parallel Algorithms

          partial permutation routing on the             simulating a network on mesh of
             hypercube, 141, 152                           trees, 275, 280
        routing on mesh of trees, 275, 279               simulating a pyramid on mesh of
        routing on OTIS-Mesh, 322                          trees, 275, 279
        routing on pyramid network, 274, 278             simulation, 275, 279–280
        routing on the butterﬂy, 110                        of hypercube on OTIS-Hypercube,
        routing on the hypercube, 105–107                      324
        routing on the linear array, 172                 smoothing a picture, 207, 213
        routing on the mesh, 172–173, 209,               sorter
          218                                               on-chip bubble sorter, 347, 349,
                                                               353, 356
        S                                                   zero-time sorter, 345–346, 353, 356
                                                         sorting, 11, 18, 35, 43, 74, 89, 113,
        (d, k)-Star, 297–298, 300, 304–305,                115, 139, 147, 164–171, 196–198,
           308–310                                         202, 204, 207–213, 217–220, 223,
           dominating set in, 305, 310                     225–226, 254–255, 266, 268,
        sample sort, 115, 139, 147                         274–275, 278–279, 300, 305, 310,
        samplesort algorithm, 116–117, 139,                324, 327, 352, 356
           147, 327                                         bitonic sorting, 40
        searching, 5, 18, 21, 70, 78, 210, 221              bucketsort, 139–140, 147–148
        select algorithm, 76                                hyperquicksort, 113
        selection, 50, 71–72, 76–77, 80, 84, 89,            odd-even, 33
           93, 118, 140, 148–149, 256, 259,                 on linear array, 352, 356
           274, 276                                         parallel bottom-up, 31
           on the hypercube, 118, 140,                      parallel quicksort, 18
              148–149                                       pipelined mergesort, 43, 49
           sequential, 76                                   sample sort, 115, 139, 147
        semigroup operations                             sorting by minimum extraction, 254
           in the hypercube, 105                         sorting by partitioning, 255
        semigroup operations on                          sorting on (d, k)-Star, 300, 305, 310
           OTIS-Hypercube, 326                           sorting on 3-dimensional mesh, 202,
        semigroup operations on the                        204, 212, 225–226
           OTIS-Mesh, 316                                sorting on CRCW PRAM, 11
        semigroup operations on tree                     sorting on mesh of trees, 266, 268,
           network, 254                                    275, 279
        semisystolic, 343–344, 352, 355                  sorting on OTIS-Hypercube, 327
        shared-memory computers, 2, 7                    sorting on OTIS-Mesh, 324
        shearsort, 165–166                               sorting on pyramid network, 274, 278
        shearsort algorithm, 165–167, 169                sorting on the mesh
        shift, 320–321, 332, 335–336                        odd–even mergesort, 169, 171
        shift operations on the OTIS-Mesh,                  recursive algorithm, 167–168
           320–321, 332, 335–336                            shearsort, 165–166
        shortest paths, 58, 143, 155, 185, 211,          sortingcrcw algorithm, 11–14, 69,
           223                                             77
May 7, 2022   11:16      Parallel Algorithms       9in x 6in        b4591-index                  page 383




                                               Index                                       383

        spanning tree, 59, 61–62, 75, 90–92,           summation, 361
          210, 222                                       formulas, 362
        star network, 281                              systolic computation, 341
           (d, k)-Star, 297–298, 300, 304–305,
              308–310                                  T
           broadcasting in, 294–295, 304, 307          tail bounds, 365
           computing maximum on, 290–291,              Toeplitz matrix, 239–240, 245,
              304, 306                                    249–250
           computing ranks, 303, 305                      computing, 239–240, 245, 249–250
           labels, 303, 305–306                        topology of a network, 4
           neighborhood broadcasting in, 292,          torus, 159
              294, 304, 306–307                           bisection width of, 207, 213
           parallel preﬁx on, 287, 289, 304,           transitive closure, 58, 142, 155, 180,
              306                                         210, 223
           preﬁx sum on, 287, 289, 304, 306            transpose of a matrix, 208, 215
           ranking of the processors in, 283           tree network, 253, 273–276
           routing between substars, 285, 287             computing maximum on, 273–276
           sorting in (d, k)-Star, 300, 305, 310          computing sum on, 273, 275
        stardksort algorithm, 301, 310                    selection on, 256, 259, 274, 276
        starlabels algorithm, 284–285, 303,               semigroup operations on, 254
          305–306                                         sorting on, 254–255
        starmax algorithm, 291–292, 304,               treelevels algorithm, 26
          306
        starparprefix algorithm, 288–289,              V
          304, 306
                                                       vertex level in a tree, 26–27, 73, 87
        starrecdub algorithm, 293–294,
          304, 306
                                                       W
        starroute algorithm, 286, 288–290,
          292                                          window broadcast, 209, 219, 332, 334
        suﬃx, 73, 85
        suﬃx minima, 73, 85–86                         Z
        sum, 5, 138, 141, 143–144, 151, 156,           zero-one principle, 32
          273–275, 278, 316, 326, 332–333,             zero-time VLSI sorter, 345–346, 353,
          335–336                                        356
